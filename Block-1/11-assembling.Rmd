# Assembling functions {#fun-assembling}

<div style="float:right;">[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/Block-1/11-assembling.Rmd)</div>

When we need a new function for some purpose, we practically always build it out of existing functions. For instance, a  parameterized function like $$A \sin\left(\frac{2 \pi}{P}x\right) + C$$ is built by assempling together a straight-line input scaling, a pattern-book $\sin()$ function, and another straight-line function for scaling the output from $\sin()$.  `r mark(1300)`

In this chapter, we'll introduce three general frameworks for combining functions: linear combination, composition, and multiplication.

## Linear combination 

One of the most widely used sorts of combination is called a ***linear combination***. The mathematics of linear combination is, it happens, at the core of the use of math in applications, whether that be constructing a Google-like search engine or analyzing medical data to see if a treatment has a positive effect.

You've worked for many years with one kind of linear combination: polynomials. No doubt you've seen functions^[It's likely that you saw polynomials as things to be factored, rather than as functions taking an input and producing an output. So they were written as *equations*: $e x^2 + 5x - 2 = 0$] like $$f(x) \equiv 3 x^2 + 5 x - 2$$

There are three pattern-book functions in this polynomial. In this case, as in polynomials generally, they are all power-law functions: $g_0(x) \equiv 1$, $g_1(x) \equiv x$, and $g_2(x) \equiv x^2$. With these functions defined, we can write the polynomial $f(x)$ as $$f(x) \equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)$$
Each of the functions is being scaled by a quantity---3, 5, and -2 in this example---and the scaled functions are added up. That's a linear combination; scale and add. (Later, we'll see that the ***scalars*** generally come with units. So we might well have a metric polynomial and an equivalent traditional-unit polynomial. Just wait.)


There are other places where you have seen linear combinations: 

- The parameterized ***sinusoid***  $$A \sin\left(\frac{2 \pi}{P}t\right) + C$$ is a linear combination of the functions $h_1(t) \equiv \sin\left(\frac{2 \pi}{P} t\right)$ and $g_0(t) \equiv 1$. The scalars are $A$ and $C$.
- The parameterized ***exponential*** $$A e^{kt} + C$$ The functions being combined are $e^{kt}$ and $1$. The scalars are, again, $A$ and $C$.
- The straight line function $a x + b$. The functions being combined are $x$ and $1$, the scalars are $a$ and $b$.  `r mark(1310)`

Note that neither the parameterized exponential or the parameterized sinusoid is a polynomial.

There are a few reasons for us to be introducing linear combinations here.

1. You will see linear combinations everywhere once you know to look for them.
2. There is a highly refined mathematical theory of linear combinations that gives us powerful ways to think about them as well as computer software that can quickly find the best scalars to use to match input-output data.
3. The concept of linear combination generalizes the simple idea that we have been calling "scaling the output." From now on, we'll use the linear-combination terminology and avoid the narrower idea of "scaling the output."
4. Many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule or a helicopter in flight or a building shaken by an earthquake are described in terms of simple "modes" which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones.
5. Many modeling tasks can be put into the framework of choosing an appropriate set of simple functions to combine and then figuring out the best scalars to use in the combination. (Generally, the computer does the figuring.)


## Function composition {#function-composition}

To ***compose*** two functions, $f(x)$ and $g(x)$, means to apply one of the functions to the output of the other. "$f()$ composed with $g()$" means $f(g(x))$. This is generally very different from "$g()$ composed with $f()$" which means $g(f(x))$.

For instance, suppose you have recorded the outdoor temperature over the course of a day and packaged this into a function $\text{AirTemp}(t)$: temperature as a function of time $t$. Your digital thermometer uses degrees Celsius, but you want the output units to be degrees Kelvin. The conversion function is $$\text{CtoK}(C) \equiv C + 273.15$$
Notice that CtoK() takes temperature as input. With this, we can write the "Kelvin as a function of time" as $$\text{CtoK}\left(\text{AirTemp}(t)\right)$$
It's important to distinguish the above time $\rightarrow$ Kelvin function from something that looks very much the same but is utterly different: $\text{AirTemp}\left(\text{CtoK}(C)\right)$. In the first, the input is time. In the second, it is temperature in celsius.


Here is a simple, approximate formula for the length of day (in hours) as a function of latitude $L$ and the declination angle $\delta$ of the sun. 

$$\text{day_length}(L, \delta) \equiv \frac{2}{15} \arccos\left(-\tan(L)*\tan(\delta)\right)$$
The declination angle is the latitude of the point on the earth's surface pierced by an imagined line connecting the centers of the earth and the sun. On the summer solstice, the longest day of the year, it is $23.44^\circ$. 

A computer implementation must look different, since $L$ and $\delta$ are typically provided in degrees while the `tan()` and other trigonometric functions in most computer languages expect units of radians. The conversion is easy: $\text{deg2rad}(d) \equiv \frac{\pi}{180} d$. The conversion the other way is $\text{rad2deg}(r) \equiv \frac{180}{\pi} r$.

In order to get the day-length formula to work in a computer, we can compose the $\tan()$ function with `deg2rad()`. The output of `acos()` is in radians, so we have to convert it back to degrees. Like this:

```{r}
day_length <- makeFun(
  (2/15)*rad2deg(
    acos(
      -tan(deg2rad(L))*tan(deg2rad(d))
    )
  ) ~ L & d)
```


Now to make a plot of day length as a function of day of the year. Of course, `day_length(L, d)` does not take day of the year into account. What's missing is to know the declination of the sun as a function of calendar day.

The input is a number $n$ that runs from 0 at the start of January 1st to 365 at the end of December 31. In terms of this input, the declination of the sun is known to be approximately
```{r}
delta_sun <- makeFun(-23.44*cos((2*pi/365)*(n+10) ) ~ n)
```

Composing `day_length()` with `delta_sun()` (on the `d` argument only), and setting the latitude to be, say, $39^\circ$N,  we get a function of day of year `n`:
```{r}
slice_plot(
  day_length(39, delta_sun(n)) ~ n, 
  domain(n=c(0,365))
  )
```



::: {.intheworld  data-latex=""}
Income inequality is a matter of perennial political debate. In the US, most people support Social Security, which is an income re-distribution programming dating back almost a century. But other re-distribution policies are controversial. Some believe they are essential to a healthy society, others that the "cure" is worse than the "disease."  `r mark(1330)`

Whatever one's views, it's helpful to have a way to quantify inequality. There are many ways that this might be done. A mathematically sophisticated one is called the ***Gini coefficient***.

Imagine that society was divided statistically into income groups, from poorest to richest. Each of these income groups consists of a fraction of the population and has, in aggregate, a fraction of the national income. Poor people tend to be many in number but to have a very small fraction of income. Wealthy people are few in number, but have a large fraction of income. The table shows data for US households in 2009:^[These data, as well as the general idea for the topic come from La Haye and Zizler (2021), "The Lorenz Curve in the Classroom", *The American Statistician*, 75(2):217-225]

group label  | population | aggregate income | cumulative income | cumulative pop.
:------------|-----------:|-----------------:|:-------
poorest      | 20%   | 3.4%  | 3.4%         | 20%
low-middle   | 20%   | 8.6%  | 12.0%        | 40%
middle       | 20%   | 14.6% | 26.6%        | 60%
high-middle  | 20%   | 23.2% | 47.8%        | 80%
richest      | 20%   | 50.2% | 100.0%       | 100%


The ***cumulative*** income shows the fraction of income of all the people in that group or poorer.  The cumulative population adds up the population fraction in that row and previous rows. So, a cumulative population of 60% means "the poorest 60% of the population" which, as the table shows, earn as a group 14.6% of the total income for the whole population.

A function that relates the cumulative population to the cumulative income is called a ***Lorenz function***. The data are graphed in Figure \@ref(fig:lorenz-data) and available as the `US_income` data frame in the `r sandbox_link()`. Later, in Figure \@ref(fig:lorenz-1-fun), we'll fit parameterized functions to the data.

```{r lorenz-data, echo=FALSE, fig.cap="Data on household incomes in the US in 2009."}
Income <- tibble::tribble(
  ~ income, ~ pop,
  0,     0,
  3.4,  20,
  12.0, 40, 
  26.6, 60,
  47.8, 80,
  100, 100
) %>%
  mutate(income=income/100, pop=pop/100)
P <- gf_point(income ~ pop, data = Income) %>%
  gf_labs(x="Population percentile (p)", y="Cumulative income percentile (L)") %>%
  gf_refine(coord_fixed())
P
```
Lorenz curves must:

- Be concave up, which amounts to saying that the curve gets steeper and steeper as the population percentile increases. (Why? Because at any point, poorer people are to the left and richer to the right.) 
- Connect (0,0) to (1, 1). 

Calling the income percentile $L$ a function of the population percentile $p$, a Lorenz function is $L(p)$ that satisfies the requirements in the previous paragraph.
Here are some functions that meet the requirements:

- $L_b(p) \equiv p^b$ where $1 \leq b$.
- $L_q(p) \equiv 1 - (1-p)^q$ where $0 < q \leq 1$

Notice that each of these functions has just one parameter. It seems implausible that the workings of a complex society can be summarized with just one number. We can use the curve-polishing techniques that will be introduced in Section \@ref(modeling-cycle) to find the "best" parameter value to match the data.  `r mark(1340)`

```{r}
Lb <- fitModel(income ~ pop^b, data = Income, start=list(b=1.5))
Lq <- fitModel(income ~ 1 - (1-pop)^q, data = Income, start=list(q=0.5))
```

Figure \@ref(fig:lorenz-1-fun) compares the polished functions to the data.
```{r lorenz-1-fun, echo=FALSE, fig.cap="Lorenz curves $L_b(p)$ (blue) and $L_q(p)$ (magenta) fitted to the household income data."}
P %>%
  slice_plot(Lb(pop) ~ pop, color="dodgerblue", npts=500) %>%
  slice_plot(Lq(pop) ~ pop, color="orange3", npts=500) %>%
  slice_plot(.55*Lb(pop) + .45*Lq(pop) ~ pop, color="magenta")
```
Neither form $L_b(p)$ or $L_q(p)$ gives a compelling description of the data. Where should we go from here?

We can provide more parameters by constructing more complicated Lorenz functions. Here are two ways to build a new Lorenz function out of an existing one:

- The product of any two Lorenz functions, $L_1(p) L_2(p)$ is itself a Lorenz function.
- A linear combination of any two Lorenz functions, $a L_1(p) + (1-a) L_2(p)$, so long as the scalars add up to 1, is itself a Lorenz function. For instance, the magenta curve in Figure \@ref(fig:lorenz-1-fun) is the linear combination of 0.45 times the tan curve plus 0.55 times the blue curve.

Question: Is the composition of two Lorenz functions a Lorenz function? That is, does the composition meet the two requirements for being a Lorenz function?

To get started, figure out whether or not $L_1(L_2(0)) = 0$ and $L_1(L_2(1)) = 1$. If the answer is yes, then we need to find a way to compute the concavity of a Lorenz function to determine if the composition will always be concave up. We'll need additional tools for this. We'll introduce these in Block 2.

:::




## The modeling polynomial {#modeling-polynomial-1}

Sometimes, in order to model some simple relationship you need to build a function whose graph has a simple, curving shape.  `r mark(1350)`

A simple but surprisingly powerful approach is to use a ***low-order polynomial***. The ***order of a polynomial*** is the highest exponent on the input. For example:

- A ***straight-line function***, $g_1(x) \equiv a_0 + a_1 x$, is a **first-order** polynomial. 
- A ***quadratic***, $g_2(x) \equiv b_0 + b_1 x + b_2 x^2$ is a **second-order** polynomial.

Many modelers are tempted to extend the technique to third-, fourth-, fifth-order and even higher. This is only rarely worthwhile since all second-, fourth-, sixth- and higher-even-order ***monomials*** have basically the same U-shape, like a referee signalling a touch-down. Similarly, first-, third-, fifth- and higher odd-order monomial have  similar ![](www/cubic.png) shapes.  

An ofttimes better approach is to compose the polynomial with a curved but monotonic function, such as a logarithm.

::: {.why  data-latex=""}
Notice that in writing low-order polynomials like $$g_1(x) \equiv a_0 + a_1 x$$ or  $$g_2(x) \equiv b_0 + b_1 x + b_2 x^2$$ we are using a specific naming convention for the scalars in the linear combinations. For each different function, we use a different start-of-the-alphabet name, like $a$ and $b$. That same name is used for all the scalars in the function, and a subscript is used to make the distinction between the different functions being combined. Thus, we have $a_1$ for the $x$ function in $g_1()$ and $b_2$ for the $x^2$ function in $g_2()$.

In high-school mathematics, polynomials are often written without subscript, for instance $a x^2 + b x + c$. This can be fine when working with only one polynomial at a time, but in modeling we often need to compare multiple, related polynomials.
:::


## Function multiplication

The third in our repertoire of methods for making new function out of old is plain old multiplication. With two functions $f(x)$ and $g(x)$, the product is simply $f(x)g(x)$.

It's essential to distinguish between function multiplication and function composition:

$$\underbrace{f(x) g(x)}_\text{multiplication}\ \ \ \ \underbrace{f(g(x)) \ \ \text{or}\ \ \ g(f(x))}_\text{composition}$$

In function composition, only one of the functions---the ***interior function*** is applied to the overall input, $x$ in the above example. The ***exterior function*** is fed its input from the output of the interior function. 

In multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.

In function composition, the order of the functions matters: $f(g(x))$ and $g(f(x))$ are in general completely different functions.

In function multiplication, the order doesn't matter because multiplication is ***commutative***, that is, if $a$ and $b$ are each quantities, $a \times b = b \times a$.  `r mark(1360)`



***Transient vibration***

A guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string. 

After plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.

Function multiplication is used so often in modeling that you'll see it in many modeling situations. Here's one example that is important in physics and communication: the ***wave packet***. Overall, the wave packet is a localized oscillation as in Figure \@ref(fig:wave-packet). 

```{r wave-packet, echo=FALSE, fig.cap="A *wave packet* constructed by multiplying a sinusoid and a gaussian function."}
wave_packet <- function(x, A=1, center = 0, width = 5, P=1, phase = 0) {
  A*sin(2*pi*x/P)*dnorm((x - center)/width)
}
slice_plot(wave_packet(x, A=2, center=3, width  = 4, P  = 10/5 ) ~ x, 
           domain(x = c(-15, 15)), npts = 500)
```
This is the product of two simple functions: a gaussian times a sinusoid.

```{r making-wave-packet, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="The two components of the wave packet in Figure \\@ref(fig:wave-packet)"}
sinusoid <- makeFun(sin(2*pi*x/P) ~ x, P=10/5)
envelope <- makeFun(2*dnorm((x - center)/width) ~ x, center=3, width=4)
slice_plot(envelope(x) ~ x, domain(x = c(-15, 15)), color="dodgerblue") %>%
  gf_labs(title="The \"envelope\" of the wave packet")
slice_plot(sinusoid(x) ~ x, domain(x = c(-15, 15)),
           color="orange3", npts=500) %>%
  gf_labs(title="The oscillation in the wave packet. ")
```

::: {.intheworld  data-latex=""}
Each simple function such as a gaussian, a sigmoid, a straight-line function, or a sinusoid can be likened to a character in a story. For instance, a sinusoid with a period of 10 seconds and an amplitude of 5 feet might be sufficient for the purpose of describing the shaking encountered during an earthquake. A sigmoid might be a good description of the uptake of a successful social media platform such as Facebook. 

But not every social media firm succeeds and you may need two or more characters to express the drama: the rising young firm and the rapid fall in popularity when competition provides a better alternative.

For instance, the initial rise in popularity of the social media platform [Yik Yak](https://en.wikipedia.org/wiki/Yik_Yak) was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.

On way to model this pattern is by multiplying a sigmoid by an exponential.(See Figure \@ref(fig:yikyak).)

```{r yikyak, fig.cap="Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017."}
yikyak <- makeFun(pnorm(year, mean=2014.5, sd=0.7) * exp(- (year-2014)) ~ year)
slice_plot(yikyak(year) ~ year, domain(year=c(2010,2018)))
```


:::



## All together now!

Two or all three of the techniques for combining functions---linear combinations, function composition, and function multiplication---can be used in the same function.  `r mark(1370)`

Consider the function for the length of the day
$$\text{day_length}(L, \delta) \equiv \frac{2}{15} \arccos\left(-\tan(L)*\tan(\delta)\right)$$
The 2/15 is scaling the output of $\arccos()$. The $\arccos()$ is being composed with an interior function that is itself a scaled product of two functions. 

## Exercises

`r insert_calcZ_exercise(11.1, "JWUVA", "Exercises/combining-hump-funs.Rmd")`

`r insert_calcZ_exercise(11.2, "AVNOW", "Exercises/bumpy-road-1.Rmd")`

`r insert_calcZ_exercise(11.3, "FISHU", "Exercises/two-gaussians.Rmd")`

`r insert_calcZ_exercise("11.5", "fmwTFj", "Exercises/horse-read-sofa.Rmd")`

`r insert_calcZ_exercise("11.6", "LPhyTb", "Exercises/maple-pitch-jacket.Rmd")`

`r insert_calcZ_exercise(11.7, "LDKKVA", "Exercises/arith-with-funs.Rmd")`

`r insert_calcZ_exercise(11.8, "MWDKVA", "Exercises/multiplying-functions.Rmd")`

`r insert_calcZ_exercise(11.9, "AMBXI", "Exercises/compose-1.Rmd")`


<!--

::: {.todo}
Use the `mosaicData::Births_CDC` data and find a segment with a linear secular trend, the yearly trend, and (as always) a weekly trend. Or maybe first just a short segment of a couple of months showing the weekly trend and a secular trend (as part of the yearly trend). Ask for the decomposition. Then back out on the data to show the yearly trend repeating.
:::

-->


