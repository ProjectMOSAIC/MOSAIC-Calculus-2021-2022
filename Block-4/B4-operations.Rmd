# Operations on functions

```{r include=FALSE}
library(Zcalc)

solve_step <- function(tilde, v, x0) {
  f <- makeFun(tilde)
  df <- D(tilde)
  xstar <- x0 + (v-f(x0))/df(x0)
  
  return(xstar)
}
```

In Block 1, we introduced the idea of ***mathematical modeling***, creating a representation of some aspect of the world out of mathematical "stuff." As you know, for us the relevant "stuff" includes the concept of a ***function*** with its ***inputs*** and output, units and dimensions, frameworks such as the ***basic modeling functions*** and ways of ***combining*** functions via linear combination, composition, and multiplication.

When we construct functions, the symbol $\equiv$ is appropriately used to mean "is defined to be." We have used the tradition equal sign ($=$) but always to mean "is" or "amounts to" or "happens to equal." For instance, we write $\sin(\pi/2) = 1$.

In high-school algebra, much more emphasis was placed on ***equations***. For instance, you might well see an equation like $${\mathbf{\text{equation:}}}\ \ \ x^2 - 6 x - 3 = 0$$ in a beginning algebra textbook. It pays to think a little about what such an equation means and what information it is intended to convey. In particular, how is it different from a function like $${\mathbf{\text{function:}}}\ \ \ p(x) \equiv x^2 - 6 x - 3 \ .$$

A simple equation like $3 + 4 = 7$ is a statement of fact: three plus four is indeed exactly the same as seven. But $x^2 - 6 x - 3 = 0$ is not a fact. The equality might be true or false, depending on what $x$ happens to be. In an algebra course, is really meant to be an instruction to a person: 
$$x^2 - 6 x - 3 = 0, \ \ \text{Instruction: Find x.}$$

What's meant by "find $x$" is to determine which numerical values (if any) when substituted for $x$ in the equation will produce a true statement. This is called ***solving for $\mathbf x$*** and algebra courses offer a variety of techniques that are effective for different types of problem statements.

"Solving for $x$" is an example of a ***mathematical task***. We undertake such tasks in order to extract useful information from a mathematical object. For instance, in textbook "word problems," you translate a verbal description of a situation---typically involving canoes paddling across a flowing river---into a matching mathematical form and then, having constructed the mathematical form, you apply some mathematical task to the form in order to reveal the answer you seek.

In this chapter, we're going to look at the mathematical tasks that are commonly performed on functions, that is, ***operations on functions***. We'll introduce you to algorithms that enable you to construct the task result without having to apply huge mental work or solve puzzles. Importantly, we'll give a name to each task. That way, confronted with a mathematical problem, you will be able to look down the short menu of common tasks to decide which one is applicable to your circumstance. Even better, once the task has a name, you can tell a computer to do it for you.

Here are some common mathematical tasks that you've already learned about:

1. Given a function and specific values for the inputs, ***apply*** the function to the inputs to produce an output. Another name for this is to ***evaluate a function*** on inputs.
2. Given a function and the name of a with-respect-to input, construct a new function that is the derivative of the given function. The name for this task is to ***differentiate the function***.
3. Like (2), given a function and the name of a with respect to input, ***anti-differentiate the function***.
4. Given a function and an interval of the domain of that function, ***accumulate*** the function on that interval. This is named to ***integrate the function*** on the interval. (You may recognize that you can perform this task by breaking it down into task (3) and then applying task (1) to the result. That is, $\int_a^b f(t) dt = F(b) - F(a)$.)

We'll focus on these new operations on functions that you may not yet have mastered. 

5. Given a function and an output value from the function, find values for an input (or inputs) which will generate that output value. This is the ***solving*** task. A closely related task is ***zero-finding***, which is to find an input that will cause the function to produce the output zero.
6. Given a function and an interval of the domain, find an input value that will produce an output value that's higher than would be produced for nearby inputs. As you might recognize, this is called finding an ***argmax***. The problem of finding an ***argmin*** is exactly the same kind of problem, and can be solved by finding the argmax of the negative of the function.
7. Given a function and an input value, ***iterate*** the function to produce a new input that will be better than the original for some purpose. 

These seven tasks allow you to perform a huge variety of the important mathematical work of extracting useful information from a model you've constructed of a situation of interest. Human judgement and creativity is needed to construct the model. And judgement and experience is needed to figure out which tasks to perform and in what order. But carrying out the tasks does not require judgement, experience, or creativity. Performing the tasks requires only an algorithm and the tools to step through the algorithm. Computers are excellent for this; you just have to give them the function and whatever additional input is required (e.g. the name of a with-respect-to-variable), and then tell the computer which task it is to perform.

## Task: Solve

Starting materials: a function $f(x)$, a known output value $v$, and a candidate for a suitable input value $\color{brown}{x_0}$.

Ideal result from the algorithm: A new candidate $\color{magenta}{x^\star}$ such that $f(\color{magenta}{x^\star}) = v$ or, equivalently, that $$\left|\strut f(\color{magenta}{x^\star}) - v \right| = 0\ .$$

Realistic result from the algorithm: The new candidate $\color{magenta}{x^\star}$ will be better than $x_0$, that is, 
$$ \left|\strut f(\color{magenta}{x^\star}) - v\right|\ \  {\mathbf <}\ \  \left|\strut f(\color{brown}{x_0}) - v\right|$$
Easy case: When $f(x)$ is a straight-line function, that is $f(x) \equiv \line(x) = a x + b$, the solution can be found by simple arithmetic:

$$a x^\star + b - v = 0 \ \ \implies \ \ \ x^\star = \frac{b-v}{a}$$
Approximation strategy: If $f(x)$ is not a straight-line function, approximate $f()$ by a straight-line function. The approximate function will be $\hat{f}(x) = f(x_0) + f'(x_0) \left[\strut x - x_0 \right]$.^[We're writing $f'(x_0)$ to stand for the more wordy version: $\partial_x f(x=x_0)$/] Then find the easy-case result for $\hat{f}$.

The easy case is ... well, easy. We'll plug in the desired answer, $x^\star$ (which we don't yet know), and solve.
$$f(x_0) + f'(x_0) \left[\strut x^\star - x_0 \right] = v$$
implying 
$$\left[\strut x^\star - x_0 \right] = \frac{v - f(x_0)}{f'(x_0)}$$
That is,
$$x^\star = x_0 + \frac{v-f(x_0)}{f'(x_0)}$$

```{r newton-step, echo=FALSE, fig.cap="Calculation of $x^\\star$ seen graphically. The brown function is approximated as a straight-line function at the initial point $x_0$. The resulting $x^\\star$ is where that straight line crosses the value $v$ on the output scale. Here, $x^\\star$ is a little to the left of the actual place where $f()$ crosses $v$.", warning=FALSE}
raw <- rfun(~ x, seed = 1933)
x0=3.0
ticks <- latex2exp::TeX(c(r'(x^*)', "x_0"))
f <- makeFun(raw(x) ~ x)
df <- D(f(x) ~ x)(x0)
fhat <- makeFun(f(x0) + df*(x-x0) ~ x)
xstar <- solve_step(f(x) ~ x, v=10, x0=x0)
slice_plot(f(x) ~ x, domain(x=-2:5), color="brown", label_text="f(x)", label_vjust=-0.5) %>%
  slice_plot(fhat(x) ~ x, color="blue", size=2, alpha=0.3) %>%
  gf_hline(yintercept=~ 10, color="magenta") %>%
  gf_text(10 ~ 4, label="v=10", color="magenta", vjust = -0.5) %>%
  gf_text(1 ~ 2, label="Straight-line approx. at x0", color="blue", angle=-35) %>%
  gf_text(0 ~ xstar, label="Result", angle=90) %>%
  gf_refine(scale_x_continuous(breaks = c(xstar, x0), labels = ticks, minor_breaks = seq(-2, 5, by=0.5)), coord_fixed(ratio=1/5)) +
  theme(axis.text.x = element_text(size = 15))
```




::: {.example data-latex=""}
For the function $$f(x) \equiv x^2 - x$$ find $x^\star$ such that $f(x^\star) = 4$, that is, $v=4$. We'll start with a guess: $x_0 = 3$. Note that the guess is not the answer: $f(3) = 9 - 3 = 6 \neq 4$.

We can compute $f'(x)$ in the standard way:
$$f'(x) \equiv \partial_x f(x) = 2 x - 1$$, giving $f'(x_0) = f'(3) = 6 - 1 = 5$. Now plug the components $x_0 = 3$ (our guess), $f(x_0) = 6$ (the value of $f()$ at our guess), $v=4$, and $f'(x_0) = 5$ into the formula for $x^\star$. 
$$x^\star = 3 + (4-6)/5 = 3 - 2/5 = 2.6\ .$$
We have our answer! We can confirm that $x^\star$ is the answer by plugging it in to $f()$:
$$f(2.6) = 2.6^2 - 2.6 = 4.16\ .$$
Wait a minute! The idea was to find $x^\star$ such that $f(x^\star) = 4$. We didn't quite do that. Our proposed candidate, $x^\star = 2.6$ is *almost* right, but not exactly right.

The traditional math instructor would say, "$x^\star = 2.6$ is a **wrong answer**." In response, the student might look hard for what he or she has done wrong.

But we will take a gentler point of view. $x^\star = 2.6$ is a **good effort**, a **step** forward. Even though $f(x^star) = 4.16$, which is not exactly what the problem asked for, $x^\star = 4.16$ is much better than our initial guess $x_0 = 3$ which produced $f(x_0) = 6$.  That is, 
$$\underbrace{\left|\strut \overbrace{f(x^\star)}^{4.16} - v \right|}_{\left|4.16 - 4\right| = 0.16}\ \  <\ \ \underbrace{\left|\strut \overbrace{f(x_0)}^6 - v \right|}_{\left|6 - 4\right|= 2}\ .$$
Although we haven't completed the task, we've gotten closer. In much the same way, the task of digging a well needs to start with the first shovelful of dirt removed from ground: It's a start!
:::

One excellent advantage of this approach to $x^\star$ is the ease with which it can be translated to instructions for a computer. We'll write such a function here. We don't expect you to understand every detail, but hopefully you can see an outline of what's going on:

```{r}
solve_step <- function(tilde, v, x0) {
  f <- makeFun(tilde)
  df <- D(tilde)
  xstar <- x0 + (v-f(x0))/df(x0)
  
  return(xstar)
}
```
Now that the computer knows the algorithm, we can easily test it out:
```{r}
f <- makeFun(x^2 - x ~ x)
xstar <- solve_step(f(x) ~ x, v=4, x0=3)
xstar
```

Confirming that `xstar` is the answer ...
```{r}
f(xstar)  # Should be 4
```
Not the end result, but a **good effort.**

By the way, now that we have `solve_step()`, which makes a **good effort**. We can try making another good effort by pluggin in `xstar` in place of our initial guess `x0`. That is,

```{r}
xstar <- solve_step(f(x) ~ x, v=4, x0=xstar)
xstar
```

Do we have the answer now?
```{r}
f(xstar) # should be 4
```
Not quite. But it looks like we're close.

::: {.example data-latex=""}
Calculate $\sqrt[3]{4}$ using the `solve_step()` method.

$\sqrt[3]{6}$ is the solution to the problem traditionally framed as "Solve for $x$ in $x^3 = 6$."

In the function-oriented approach solving described in this section, we have
$$f(x) \equiv x^3 \ \ \text{and} \ \ \ v=6$$
We'll start with a guess: $x_0 = 2$. (This is motivated by noting $2^3 = 8$, which is pretty close to the desired $v=6$.)

```{r}
f <- makeFun(x^3 ~ x)
xstar <- solve_step(f(x) ~ x, v=6, x=2)
xstar
f(xstar)
```

Taking another shovel of dirt out of the well that we've started to dig ...

```{r}
xstar <- solve_step(f(x) ~ x, v=6, x=xstar)
xstar
f(xstar)
```
:::

Let's briefly consider the process of finding $x^\star$ from a slightly different perspective. 

$$x^\star = x_0 + \underbrace{\ \ \frac{v-f(x_0)}{f'(x_0)}\ \ }_{\text{"Newton step"}}$$
We can think of $x^\star$ as being the result of taking a ***step*** from the starting point $x_0$ that leads to the destination. This is often called a ***Newton step*** since Newton was an early author of the method. Overall, the process of taking successive Newton steps to get closer and closer to the goal is called the [***Newton-Raphson method***](https://en.wikipedia.org/wiki/Newton%27s_method).

::: {.takenote data-latex=""}
The process we've presented here is not guaranteed to work. By exploring cases where it fails, ***computational mathematicians***^[A traditional name for such a person is "numerical analyst."] have developed strategies for increasing the range of situations for which it works. 

For instance, consider $x_0$ accidentally picked close to an argmax, that is $f'(x_0) \approx 0$.  The length of the Newton step is proportional to $1/f'(x_0)$, which is large when $f'(x_0) \approx 0$. Such a Newton step can produce $x^\star$ further from the actual solution rather than closer to it. 

This understanding can be integrated into the `solve_step()` algorithm in several ways  A simple modification is to take not a **full** Newton step, but a fraction of one.

Such modifications, including more sophisticated ones that monitor progress toward the goal, can involve elaborate computer programming. Since this book is not about programming *per se*, we won't head down this path. But we do point out that better, more reliable, professional-level computer functions for finding $x^\star$ are available, and you should use them.

The R/mosaic function `Zeros()` incorporates several good practices, including attempting to make a good guess for $x_0$ automatically. To use `findZero()`, you construct a function $g(x) \equiv f(x) - v$, and `findZero()` will produce a good $x^\star$ or, even better, a set of them. Illustrating with a previous example:

```{r}
g <- makeFun(x^3 - 6 ~ x)
xstar <- findZeros(g(x) ~ x, domain(x=c(1,6)))
xstar
```
How well did this work?
```{r}
with(xstar, g(x)) # should be zero
```
:::

Solving a function with two or more inputs can be done in an analogous manner. Pick an initial point $(x_0, y_0, ...)$ and write a first-order polynomial approximation, set it equal to $v$, and find $x^\star, y^\star, ...$. 

$$g(x, y, ...) \approx g(x_0, y_0) + \partial_x g(x_0, y_0)\, \left[x^\star - x_0\right] + \partial_y g(x_0, y_0)\, \left[y^\star - y_0\right] + \cdots = v$$
This is a somewhat daunting jumble of symbols, so the "find $x^\star, y^\star, ...$" may seem easier said than done. And many students, with an experience solving systems of multiple equations, may start out thinking that no solution can be found with just one equation. But, in reality, there is an infinite number of solutions. For instance, you could set all but one of the starred quantities to zero, leaving you with one linear equation in one variable: easy! 

In Block 5, Section \@ref(finding-the-coast) we'll return to this problem of picking suitable values for $(x^\star, y^\star, ...)$ in a framework that dramatically simplifies the notation and points to a solution where the starred values can be as small as possible. 

## Task: Argmax

The task of finding the input value that corresponds to a local maximum  is called ***argmax finding***. We don't need to know the value of the local maximum to solve this problem. Instead, we designate a locale by specifying an initial guess $x_0$ for the argmax. For argmax finding, we seek a $x^\star$ such that $f(x^\star) > f(x_0)$.  

One approach to argmax finding is based on the properties of local maxima that we studied in Chapter \@ref(optim-and-shape). Specifically, we know that local maxima will be found for the input where the derivative of the ***objective function*** equals zero, that is, $f'(x^\star) = 0$. This corresponds to a two-step algorithm:

i. Find the derivative $\partial_x f(x)$ of the objective function $f(x)$.
ii. Carry out the *Solving Task* on $\partial_x f(x)$, with $v=0$ and the given $x_0$.

Here's an implementation with a few bells and whistles.

    i. Rather than just an initial guess $x_0$, you specify a domain over which the search is to be performed.
    ii. In addition to finding the zeros of the derivative of the objective function, the function value and the sign of the concavity at those zeros is calculated.
    

```{r}
local_argM <- function(tilde, domain) {
  # Tilde sets the objective function
  f <- makeFun(tilde)
  # Calculate derivative of objective function
  df <- D(tilde) 
  ddf <- D(df(x) ~ x)
  # Find inputs where the derivative function has value 0
  Zeros(df(x) ~ x, domain=range(domain)) %>%  
    filter(is_in_domain(x, domain)) %>%  
    mutate(.output. = f(x), 
           concavity = sign(ddf(x))) 
}
```

Figure \@ref(fig:argm-ex1) shows the output of `argM()`.

```{r argm-ex1, echo=FALSE}
f <- rfun(~ x, 3215)
Xstar <- local_argM(f(x) ~ x, domain = c(-5, 5))
Xstar
slice_plot(f(x) ~ x, domain(x=-5:5)) %>%
  gf_point(.output. ~ x, data = Xstar, color="magenta") %>%
  gf_lims(x = c(-5,5))
```
Notice that `argM()` identified both a local maximum and a local minimum, that is, one argmax and one argmin. Visually, it's easy to tell which one is which. In terms of the data frame returned by `argM()`, the sign of the concavity does the identification for you: positive concavity points to an argmin, negative concavity to an argmax. The name `argM()`, of course, refers to this versatility of finding both argmins and argmaxes.

The R/mosaic function `argM()` is somewhat more sophisticated, but does essentially the same thing as the short program presented above.

Let's look at the argmax problem another way. We have and objective function $f(x)$ and a current guess $x_0$ that we hope is somewhat close to the argmax. We'll approximate $f(x)$ with a low-order polynomial, as we so often do. We'll call the approximation $\widehat{f(x)}$. In the solving task, the approximation was with a first-order polynomial. But first-order polynomials---that is, straight-line functions---don't have a local argmax. We need to use a second-order polynomial. Easy enough: construct the second-order ***Taylor polynomial*** around $x_0$:

$$\widehat{f(x)} \equiv f(x_0) + f'(x_0) \left[x - x_0\right] + \frac{1}{2} f''(x_0) \left[x-x_0\right]^2$$
Remember that $f(x_0)$, $f'(x_0)$ and $f''(x_0)$ are all fixed quantities; they don't depend on $x$. 

To find the argmax of $\widehat{f(x)}$, differentiate it with respect to $x$ and find the zero of the derivative:
$$\partial_x \widehat{f(x)} = f'(x_0) \underbrace{\partial_x\left[x - x_0\right]}_{1} +
\frac{1}{2} f''(x_0) \underbrace{\partial_x\left[x-x_0\right]^2}_{2 \left[x - x_0\right]} = 0
$$

Giving $$f'(x_0) + f''(x_0) \left[x - x_0\right] = 0\ .$$
As with all solving-task problems, we seek $x=x^\star$ that will make the above statement true. This is
$$x^\star = x_0 - \frac{f'(x_0)}{f''(x_0)}\ .$$
In other words, our new guess $x^\star$ will be a step away from the old guess $x_0$, with the step being $-f'(x_0) / f''(x_0)$.

Look a bit more carefully at the step, $- f'(x_0) / f''(x_0)$. Suppose $x_0$ is such that the function  at that input has a positive slope. To go uphill, we want to follow the positive slope: make a positive step. To get a positive step from $- f'(x_0) / f''(x_0)$ when $0 < f'(x_0)$  means that $f''(x_0)$ would have to be *negative*. But suppose it isn't. Then the argmax algorithm, recognizing that the calculated step would be downhill, should make sure to take the step in the *opposite* direction.


## Task: Iterate

In everyday language, to ***iterate*** means simply to repeat: to do something over and over again. In mathematics and in computing, "iterate" has a more specific meaning: to repeatedly perform an operation, each time taking the output from the previous round as the input to the current round.

For our purposes, it suffices to define iteration in terms of the use of a function $g(x)$. The function must be such that the output of the function can be used as an input to the function; the output must be the same kind of thing as the input. The iteration starts with a specific value for the input. We'll call this value $x_0$. Iteration then means simply to compose the function with itself starting with $x_0$ as the initial input. Here, for instance, is a four-step iteration:
$$g(g(g(g(x_0))))$$
Or, you might choose to iterate for ten steps:
$$g(g(g(g(g(g(g(g(g(g(x_0))))))))))$$
However many iteration steps you take, the output from the final step is what you work with.

Iteration is useful when you have constructed $g()$ to correspond to a mathematical task you need to perform, especially the solve-task or the argmax-task. $g()$ should be made so that the output is a better answer than the input.

To illustrate, consider the solve-task, when we are trying to find an input value to a given function that will produce an output of some desired level, which we'll call $v$. Earlier, we developed the idea of a Newton step for solving a function $f()$:
$$g(x) \equiv \underbrace{x}_{\text{starting value}} + \underbrace{\frac{v - f(x)}{f'(x)}}_{\text{Newton step}}$$
The output of $g()$ is designed to be better than the input. The idea here is to use the derivative of $f()$ in order to tweak an initial guess for the solution to be a little better. A Newton step is actually a rather aggressive proposal for improving the starting value. Like many forms of aggression, it can sometimes get you exactly the opposite of what you sought. So in practice, it may be better to take only part of a Newton step.

Here is a function that creates another function that gives the Newton step for a specified $f()$ and $v$:

```{r}
NewtonStepFun <- function(tilde, v) {
  f <- makeFun(tilde)
  fprime <- D(tilde)
  # create a new function that calculates the Newton step for 
  # this f() and this v, at a specified input x.
  function(x) {
    (v - f(x))/fprime(x)
  }
}
```

To illustrate, let's make an example function $f(x)\equiv \left[x^2 - 3x\right] \left[1.3 + \pnorm(x)\right]$.
```{r nstep-example1, fig.cap="An example function $f(x)$ which we seek to solve for an output of 5."}
f <- makeFun((x^2 - 3*x)*(1.3 + pnorm(x)) ~ x)
slice_plot(f(x) ~ x, domain(x=-2:4)) %>%
  gf_hline(yintercept = ~ 5, color="magenta")
```

We seek to solve $f()$ for an output of 5, that is, we are looking for an input $x^\star$ where $f(x^\star) = 5$.

You can see the two solutions in Figure \@ref(fig:nstep-example1): for an input near $x=-1$ the output is close to 5, as it is for an input of $x=3.5$.

Supposed you were given an initial guess $x_0 = -2$. You can see from the graph that a step of about length 1 to the right will bring you close to where the output of the function will be 5. Likewise, for an initial guess of $x_0 = 4$, you would want to take a step to the left of length about 0.5. That's what a Newton step is supposed to do, bring you closer to the correct answer. Let's construct a function that gives us the Newton step for any starting point $x$, and test it out

```{r}
step_for_f <- 
  NewtonStepFun(f(x) ~ x, v = 5)
step_for_f(-2) # should be about + 1
step_for_f(4)  # should be about - 0.5
```

Now that we have an expression for the step, we package it up into the function $g()$ to be used for the iteration and then iterate $g()$ from a starting guess.
```{r}
g <- function(x) x + step_for_f(x)

g(-2)
g(g(-2))
g(g(g(-2)))
g(g(g(g(-2))))
g(g(g(g(g(-2)))))
```

The first iteration brought us from the $x_0 = -2$ starting point a little closer to the solution. A double iteration brought us further to the right. the successive iterations show little or now change from the output to the input, a good sign that we've done enough work and can declare ourselves done.

Of course, you always want to make sure that the answer gotten, $x^\star = -0.8661108$ is right. For this, we compute $f(x^\star)$
```{r}
f(-0.8661108) # should be 5
```

Other than the exercises in this chapter, you will not have much need in this course to build your own functions for iterative improvement. But it is the mathematical technology behind the operators that perform the solving-task and the argmax-task.

## Software for the tasks

Evaluation of a function---number one in the list at the head of this chapter---is so central to the use of computer languages generally that every language provides a direct means for doing so. In R, as you know, the evaluation syntax involves following the name of the functions by a pair of parentheses, placing in those parenthesis the values for the various arguments to the function. Example: `log(5)`

The other six operations on functions listed above, there is one (or sometimes more) specific R/mosaic functions. Every one of them takes, as a first argument, a ***tilde expression*** describing the function on which the operation is to be formed; on the left side is a formula for the function (which can be in terms of other, previously defined functions), on the right side is the with-respect-to variable.

2. Differentiate  `D()`. Returns a ***function***.
3. Anti-differentiate `antiD()`. Returns a ***function***.
4. Integrate: `Integrate()`. Returns a ***number***.
5. Solve    `Solve()`. Returns a ***data frame*** with one row for each solution found.
6. Argmax   `argM()` Finds one argmax and one argmin in the domain. `local_argM()` looks for all the local argmaxes and argmins. Returns a ***data frame*** with one row for each argmax or argmin found.
7. Iterate  `Iterate()`. Returns a data frame with the value of the initial input and the output after each iteration.

Each of operations 4-6 involves the specification of a ***domain***. For `Integrate()`, this is, naturally, the domain of integration: the upper and lower bounds of the integral

For `Solve()` and `argM()` the domain specifies where to search for the answer. `Iterate()` is slightly different. After the tilde expression comes an initial value $x_0$ and then `n=` which you use to set the number of times to iterate.
 
Another two R/mosaic functions that you frequently use generate graphics. 

8. `slice_plot()`
9. `contour_plot()`

These always take two main arguments: a tilde expression describing the function to be graphed and a plotting domain.

Finally, there is one function for plotting variables from data frames:

10. `gf_point()`. Again, the first argument is a tilde expression specifying two of the data frame's variables: the variable on the left side will be assigned to the vertical axis, the variable on the right side to the horizontal axis. 


## Exercises

- CONSTRUCT SOME NEWTON STEPS by hand
- Pick an initial guess by graphing the function.
- Dimension of $-f'(x_0) / f''(x_0)$
- Create a Newton step for optimization
- Know the arguments to the functions `D()`, `antiD()`, `Solve()`, `argM()`, and `Iterate()`, and be able to use them.
