[["introduction.html", "MOSAIC Calculus: Block 4: Manifestations Introduction", " MOSAIC Calculus: Block 4: Manifestations Daniel Kaplan 2021-10-12 Introduction In Blocks 1 through 3, you learned how to formulate calculus questions in terms of functions and the two main operations of calculus, differentiation and anti-differentiation, which transform one function into another. You also learned the primary use of anti-differentiation, integration, which carries out the accumulation over an interval of the input variable. There is more mathematics yet to come. In Block 5 you’ll meet a major technique for modeling used in essentially all quantitative disciplines and with particularly important applications to data science. In Block 6 we’ll start to work with multi-component systems as they evolve in time. Here, in Block 4, we’re going to step momentarily away from the introduction of new mathematical concepts. Functions, differentiation, and anti-differentiation are impressive and useful in their own right so now we’re going to look at how they are used, how they manifest themselves in diverse ways in the world of science and technology. Our emphasis will not be on field-specific applications, although there are many! Instead, we’ll explore ways the concepts of calculus appear and are used broadly in quantitative work. "],["operations-on-functions.html", "Chapter 31 Operations on functions 31.1 Task: Solve 31.2 Task: Argmax 31.3 Task: Iterate 31.4 Software for the tasks 31.5 Exercises", " Chapter 31 Operations on functions In Block 1, we introduced the idea of mathematical modeling, creating a representation of some aspect of the world out of mathematical “stuff.” As you know, for us the relevant “stuff” includes the concept of a function with its inputs and output, units and dimensions, frameworks such as the basic modeling functions and ways of combining functions via linear combination, composition, and multiplication. When we construct functions, the symbol \\(\\equiv\\) is appropriately used to mean “is defined to be.” We have used the tradition equal sign (\\(=\\)) but always to mean “is” or “amounts to” or “happens to equal.” For instance, we write \\(\\sin(\\pi/2) = 1\\). In high-school algebra, much more emphasis was placed on equations. For instance, you might well see an equation like \\[{\\mathbf{\\text{equation:}}}\\ \\ \\ x^2 - 6 x - 3 = 0\\] in a beginning algebra textbook. It pays to think a little about what such an equation means and what information it is intended to convey. In particular, how is it different from a function like \\[{\\mathbf{\\text{function:}}}\\ \\ \\ p(x) \\equiv x^2 - 6 x - 3 \\ .\\] A simple equation like \\(3 + 4 = 7\\) is a statement of fact: three plus four is indeed exactly the same as seven. But \\(x^2 - 6 x - 3 = 0\\) is not a fact. The equality might be true or false, depending on what \\(x\\) happens to be. In an algebra course, is really meant to be an instruction to a person: \\[x^2 - 6 x - 3 = 0, \\ \\ \\text{Instruction: Find x.}\\] What’s meant by “find \\(x\\)” is to determine which numerical values (if any) when substituted for \\(x\\) in the equation will produce a true statement. This is called solving for \\(\\mathbf x\\) and algebra courses offer a variety of techniques that are effective for different types of problem statements. “Solving for \\(x\\)” is an example of a mathematical task. We undertake such tasks in order to extract useful information from a mathematical object. For instance, in textbook “word problems,” you translate a verbal description of a situation—typically involving canoes paddling across a flowing river—into a matching mathematical form and then, having constructed the mathematical form, you apply some mathematical task to the form in order to reveal the answer you seek. In this chapter, we’re going to look at the mathematical tasks that are commonly performed on functions, that is, operations on functions. We’ll introduce you to algorithms that enable you to construct the task result without having to apply huge mental work or solve puzzles. Importantly, we’ll give a name to each task. That way, confronted with a mathematical problem, you will be able to look down the short menu of common tasks to decide which one is applicable to your circumstance. Even better, once the task has a name, you can tell a computer to do it for you. Here are some common mathematical tasks that you’ve already learned about: Given a function and specific values for the inputs, apply the function to the inputs to produce an output. Another name for this is to evaluate a function on inputs. Given a function and the name of a with-respect-to input, construct a new function that is the derivative of the given function. The name for this task is to differentiate the function. Like (2), given a function and the name of a with respect to input, anti-differentiate the function. Given a function and an interval of the domain of that function, accumulate the function on that interval. This is named to integrate the function on the interval. (You may recognize that you can perform this task by breaking it down into task (3) and then applying task (1) to the result. That is, \\(\\int_a^b f(t) dt = F(b) - F(a)\\).) We’ll focus on these new operations on functions that you may not yet have mastered. Given a function and an output value from the function, find values for an input (or inputs) which will generate that output value. This is the solving task. A closely related task is zero-finding, which is to find an input that will cause the function to produce the output zero. Given a function and an interval of the domain, find an input value that will produce an output value that’s higher than would be produced for nearby inputs. As you might recognize, this is called finding an argmax. The problem of finding an argmin is exactly the same kind of problem, and can be solved by finding the argmax of the negative of the function. Given a function and an input value, iterate the function to produce a new input that will be better than the original for some purpose. These seven tasks allow you to perform a huge variety of the important mathematical work of extracting useful information from a model you’ve constructed of a situation of interest. Human judgement and creativity is needed to construct the model. And judgement and experience is needed to figure out which tasks to perform and in what order. But carrying out the tasks does not require judgement, experience, or creativity. Performing the tasks requires only an algorithm and the tools to step through the algorithm. Computers are excellent for this; you just have to give them the function and whatever additional input is required (e.g. the name of a with-respect-to-variable), and then tell the computer which task it is to perform. 31.1 Task: Solve Starting materials: a function \\(f(x)\\), a known output value \\(v\\), and a candidate for a suitable input value \\(\\color{brown}{x_0}\\). Ideal result from the algorithm: A new candidate \\(\\color{magenta}{x^\\star}\\) such that \\(f(\\color{magenta}{x^\\star}) = v\\) or, equivalently, that \\[\\left|\\strut f(\\color{magenta}{x^\\star}) - v \\right| = 0\\ .\\] Realistic result from the algorithm: The new candidate \\(\\color{magenta}{x^\\star}\\) will be better than \\(x_0\\), that is, \\[ \\left|\\strut f(\\color{magenta}{x^\\star}) - v\\right|\\ \\ {\\mathbf &lt;}\\ \\ \\left|\\strut f(\\color{brown}{x_0}) - v\\right|\\] Easy case: When \\(f(x)\\) is a straight-line function, that is \\(f(x) \\equiv \\line(x) = a x + b\\), the solution can be found by simple arithmetic: \\[a x^\\star + b - v = 0 \\ \\ \\implies \\ \\ \\ x^\\star = \\frac{b-v}{a}\\] Approximation strategy: If \\(f(x)\\) is not a straight-line function, approximate \\(f()\\) by a straight-line function. The approximate function will be \\(\\hat{f}(x) = f(x_0) + f&#39;(x_0) \\left[\\strut x - x_0 \\right]\\).1 Then find the easy-case result for \\(\\hat{f}\\). The easy case is … well, easy. We’ll plug in the desired answer, \\(x^\\star\\) (which we don’t yet know), and solve. \\[f(x_0) + f&#39;(x_0) \\left[\\strut x^\\star - x_0 \\right] = v\\] implying \\[\\left[\\strut x^\\star - x_0 \\right] = \\frac{v - f(x_0)}{f&#39;(x_0)}\\] That is, \\[x^\\star = x_0 + \\frac{v-f(x_0)}{f&#39;(x_0)}\\] Figure 31.1: Calculation of \\(x^\\star\\) seen graphically. The brown function is approximated as a straight-line function at the initial point \\(x_0\\). The resulting \\(x^\\star\\) is where that straight line crosses the value \\(v\\) on the output scale. Here, \\(x^\\star\\) is a little to the left of the actual place where \\(f()\\) crosses \\(v\\). Example 31.1 For the function \\[f(x) \\equiv x^2 - x\\] find \\(x^\\star\\) such that \\(f(x^\\star) = 4\\), that is, \\(v=4\\). We’ll start with a guess: \\(x_0 = 3\\). Note that the guess is not the answer: \\(f(3) = 9 - 3 = 6 \\neq 4\\). We can compute \\(f&#39;(x)\\) in the standard way: \\[f&#39;(x) \\equiv \\partial_x f(x) = 2 x - 1\\], giving \\(f&#39;(x_0) = f&#39;(3) = 6 - 1 = 5\\). Now plug the components \\(x_0 = 3\\) (our guess), \\(f(x_0) = 6\\) (the value of \\(f()\\) at our guess), \\(v=4\\), and \\(f&#39;(x_0) = 5\\) into the formula for \\(x^\\star\\). \\[x^\\star = 3 + (4-6)/5 = 3 - 2/5 = 2.6\\ .\\] We have our answer! We can confirm that \\(x^\\star\\) is the answer by plugging it in to \\(f()\\): \\[f(2.6) = 2.6^2 - 2.6 = 4.16\\ .\\] Wait a minute! The idea was to find \\(x^\\star\\) such that \\(f(x^\\star) = 4\\). We didn’t quite do that. Our proposed candidate, \\(x^\\star = 2.6\\) is almost right, but not exactly right. The traditional math instructor would say, “\\(x^\\star = 2.6\\) is a wrong answer.” In response, the student might look hard for what he or she has done wrong. But we will take a gentler point of view. \\(x^\\star = 2.6\\) is a good effort, a step forward. Even though \\(f(x^star) = 4.16\\), which is not exactly what the problem asked for, \\(x^\\star = 4.16\\) is much better than our initial guess \\(x_0 = 3\\) which produced \\(f(x_0) = 6\\). That is, \\[\\underbrace{\\left|\\strut \\overbrace{f(x^\\star)}^{4.16} - v \\right|}_{\\left|4.16 - 4\\right| = 0.16}\\ \\ &lt;\\ \\ \\underbrace{\\left|\\strut \\overbrace{f(x_0)}^6 - v \\right|}_{\\left|6 - 4\\right|= 2}\\ .\\] Although we haven’t completed the task, we’ve gotten closer. In much the same way, the task of digging a well needs to start with the first shovelful of dirt removed from ground: It’s a start! One excellent advantage of this approach to \\(x^\\star\\) is the ease with which it can be translated to instructions for a computer. We’ll write such a function here. We don’t expect you to understand every detail, but hopefully you can see an outline of what’s going on: solve_step &lt;- function(tilde, v, x0) { f &lt;- makeFun(tilde) df &lt;- D(tilde) xstar &lt;- x0 + (v-f(x0))/df(x0) return(xstar) } Now that the computer knows the algorithm, we can easily test it out: f &lt;- makeFun(x^2 - x ~ x) xstar &lt;- solve_step(f(x) ~ x, v=4, x0=3) xstar ## [1] 2.6 Confirming that xstar is the answer … f(xstar) # Should be 4 ## [1] 4.16 Not the end result, but a good effort. By the way, now that we have solve_step(), which makes a good effort. We can try making another good effort by pluggin in xstar in place of our initial guess x0. That is, xstar &lt;- solve_step(f(x) ~ x, v=4, x0=xstar) xstar ## [1] 2.561905 Do we have the answer now? f(xstar) # should be 4 ## [1] 4.001451 Not quite. But it looks like we’re close. Example 31.2 Calculate \\(\\sqrt[3]{4}\\) using the solve_step() method. \\(\\sqrt[3]{6}\\) is the solution to the problem traditionally framed as “Solve for \\(x\\) in \\(x^3 = 6\\).” In the function-oriented approach solving described in this section, we have \\[f(x) \\equiv x^3 \\ \\ \\text{and} \\ \\ \\ v=6\\] We’ll start with a guess: \\(x_0 = 2\\). (This is motivated by noting \\(2^3 = 8\\), which is pretty close to the desired \\(v=6\\).) f &lt;- makeFun(x^3 ~ x) xstar &lt;- solve_step(f(x) ~ x, v=6, x=2) xstar ## [1] 1.833333 f(xstar) ## [1] 6.162037 Taking another shovel of dirt out of the well that we’ve started to dig … xstar &lt;- solve_step(f(x) ~ x, v=6, x=xstar) xstar ## [1] 1.817264 f(xstar) ## [1] 6.001416 Let’s briefly consider the process of finding \\(x^\\star\\) from a slightly different perspective. \\[x^\\star = x_0 + \\underbrace{\\ \\ \\frac{v-f(x_0)}{f&#39;(x_0)}\\ \\ }_{\\text{&quot;Newton step&quot;}}\\] We can think of \\(x^\\star\\) as being the result of taking a step from the starting point \\(x_0\\) that leads to the destination. This is often called a Newton step since Newton was an early author of the method. Overall, the process of taking successive Newton steps to get closer and closer to the goal is called the Newton-Raphson method. The process we’ve presented here is not guaranteed to work. By exploring cases where it fails, computational mathematicians2 have developed strategies for increasing the range of situations for which it works. For instance, consider \\(x_0\\) accidentally picked close to an argmax, that is \\(f&#39;(x_0) \\approx 0\\). The length of the Newton step is proportional to \\(1/f&#39;(x_0)\\), which is large when \\(f&#39;(x_0) \\approx 0\\). Such a Newton step can produce \\(x^\\star\\) further from the actual solution rather than closer to it. This understanding can be integrated into the solve_step() algorithm in several ways A simple modification is to take not a full Newton step, but a fraction of one. Such modifications, including more sophisticated ones that monitor progress toward the goal, can involve elaborate computer programming. Since this book is not about programming per se, we won’t head down this path. But we do point out that better, more reliable, professional-level computer functions for finding \\(x^\\star\\) are available, and you should use them. The R/mosaic function Zeros() incorporates several good practices, including attempting to make a good guess for \\(x_0\\) automatically. To use findZero(), you construct a function \\(g(x) \\equiv f(x) - v\\), and findZero() will produce a good \\(x^\\star\\) or, even better, a set of them. Illustrating with a previous example: g &lt;- makeFun(x^3 - 6 ~ x) xstar &lt;- findZeros(g(x) ~ x, domain(x=c(1,6))) xstar ## x ## 1 1.8171 How well did this work? with(xstar, g(x)) # should be zero ## [1] -0.0002039858 Solving a function with two or more inputs can be done in an analogous manner. Pick an initial point \\((x_0, y_0, ...)\\) and write a first-order polynomial approximation, set it equal to \\(v\\), and find \\(x^\\star, y^\\star, ...\\). \\[g(x, y, ...) \\approx g(x_0, y_0) + \\partial_x g(x_0, y_0)\\, \\left[x^\\star - x_0\\right] + \\partial_y g(x_0, y_0)\\, \\left[y^\\star - y_0\\right] + \\cdots = v\\] This is a somewhat daunting jumble of symbols, so the “find \\(x^\\star, y^\\star, ...\\)” may seem easier said than done. And many students, with an experience solving systems of multiple equations, may start out thinking that no solution can be found with just one equation. But, in reality, there is an infinite number of solutions. For instance, you could set all but one of the starred quantities to zero, leaving you with one linear equation in one variable: easy! In Block 5, Section ?? we’ll return to this problem of picking suitable values for \\((x^\\star, y^\\star, ...)\\) in a framework that dramatically simplifies the notation and points to a solution where the starred values can be as small as possible. 31.2 Task: Argmax The task of finding the input value that corresponds to a local maximum is called argmax finding. We don’t need to know the value of the local maximum to solve this problem. Instead, we designate a locale by specifying an initial guess \\(x_0\\) for the argmax. For argmax finding, we seek a \\(x^\\star\\) such that \\(f(x^\\star) &gt; f(x_0)\\). One approach to argmax finding is based on the properties of local maxima that we studied in Chapter ??. Specifically, we know that local maxima will be found for the input where the derivative of the objective function equals zero, that is, \\(f&#39;(x^\\star) = 0\\). This corresponds to a two-step algorithm: Find the derivative \\(\\partial_x f(x)\\) of the objective function \\(f(x)\\). Carry out the Solving Task on \\(\\partial_x f(x)\\), with \\(v=0\\) and the given \\(x_0\\). Here’s an implementation with a few bells and whistles. i. Rather than just an initial guess $x_0$, you specify a domain over which the search is to be performed. ii. In addition to finding the zeros of the derivative of the objective function, the function value and the sign of the concavity at those zeros is calculated. local_argM &lt;- function(tilde, domain) { # Tilde sets the objective function f &lt;- makeFun(tilde) # Calculate derivative of objective function df &lt;- D(tilde) ddf &lt;- D(df(x) ~ x) # Find inputs where the derivative function has value 0 Zeros(df(x) ~ x, domain=range(domain)) %&gt;% filter(is_in_domain(x, domain)) %&gt;% mutate(.output. = f(x), concavity = sign(ddf(x))) } Figure ?? shows the output of argM(). ## # A tibble: 4 × 3 ## x .output. concavity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -2.95 3.80 -1 ## 2 -2.95 3.80 -1 ## 3 1.03 -26.5 1 ## 4 1.03 -26.5 1 Notice that argM() identified both a local maximum and a local minimum, that is, one argmax and one argmin. Visually, it’s easy to tell which one is which. In terms of the data frame returned by argM(), the sign of the concavity does the identification for you: positive concavity points to an argmin, negative concavity to an argmax. The name argM(), of course, refers to this versatility of finding both argmins and argmaxes. The R/mosaic function argM() is somewhat more sophisticated, but does essentially the same thing as the short program presented above. Let’s look at the argmax problem another way. We have and objective function \\(f(x)\\) and a current guess \\(x_0\\) that we hope is somewhat close to the argmax. We’ll approximate \\(f(x)\\) with a low-order polynomial, as we so often do. We’ll call the approximation \\(\\widehat{f(x)}\\). In the solving task, the approximation was with a first-order polynomial. But first-order polynomials—that is, straight-line functions—don’t have a local argmax. We need to use a second-order polynomial. Easy enough: construct the second-order Taylor polynomial around \\(x_0\\): \\[\\widehat{f(x)} \\equiv f(x_0) + f&#39;(x_0) \\left[x - x_0\\right] + \\frac{1}{2} f&#39;&#39;(x_0) \\left[x-x_0\\right]^2\\] Remember that \\(f(x_0)\\), \\(f&#39;(x_0)\\) and \\(f&#39;&#39;(x_0)\\) are all fixed quantities; they don’t depend on \\(x\\). To find the argmax of \\(\\widehat{f(x)}\\), differentiate it with respect to \\(x\\) and find the zero of the derivative: \\[\\partial_x \\widehat{f(x)} = f&#39;(x_0) \\underbrace{\\partial_x\\left[x - x_0\\right]}_{1} + \\frac{1}{2} f&#39;&#39;(x_0) \\underbrace{\\partial_x\\left[x-x_0\\right]^2}_{2 \\left[x - x_0\\right]} = 0 \\] Giving \\[f&#39;(x_0) + f&#39;&#39;(x_0) \\left[x - x_0\\right] = 0\\ .\\] As with all solving-task problems, we seek \\(x=x^\\star\\) that will make the above statement true. This is \\[x^\\star = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}\\ .\\] In other words, our new guess \\(x^\\star\\) will be a step away from the old guess \\(x_0\\), with the step being \\(-f&#39;(x_0) / f&#39;&#39;(x_0)\\). Look a bit more carefully at the step, \\(- f&#39;(x_0) / f&#39;&#39;(x_0)\\). Suppose \\(x_0\\) is such that the function at that input has a positive slope. To go uphill, we want to follow the positive slope: make a positive step. To get a positive step from \\(- f&#39;(x_0) / f&#39;&#39;(x_0)\\) when \\(0 &lt; f&#39;(x_0)\\) means that \\(f&#39;&#39;(x_0)\\) would have to be negative. But suppose it isn’t. Then the argmax algorithm, recognizing that the calculated step would be downhill, should make sure to take the step in the opposite direction. 31.3 Task: Iterate In everyday language, to iterate means simply to repeat: to do something over and over again. In mathematics and in computing, “iterate” has a more specific meaning: to repeatedly perform an operation, each time taking the output from the previous round as the input to the current round. For our purposes, it suffices to define iteration in terms of the use of a function \\(g(x)\\). The function must be such that the output of the function can be used as an input to the function; the output must be the same kind of thing as the input. The iteration starts with a specific value for the input. We’ll call this value \\(x_0\\). Iteration then means simply to compose the function with itself starting with \\(x_0\\) as the initial input. Here, for instance, is a four-step iteration: \\[g(g(g(g(x_0))))\\] Or, you might choose to iterate for ten steps: \\[g(g(g(g(g(g(g(g(g(g(x_0))))))))))\\] However many iteration steps you take, the output from the final step is what you work with. Iteration is useful when you have constructed \\(g()\\) to correspond to a mathematical task you need to perform, especially the solve-task or the argmax-task. \\(g()\\) should be made so that the output is a better answer than the input. To illustrate, consider the solve-task, when we are trying to find an input value to a given function that will produce an output of some desired level, which we’ll call \\(v\\). Earlier, we developed the idea of a Newton step for solving a function \\(f()\\): \\[g(x) \\equiv \\underbrace{x}_{\\text{starting value}} + \\underbrace{\\frac{v - f(x)}{f&#39;(x)}}_{\\text{Newton step}}\\] The output of \\(g()\\) is designed to be better than the input. The idea here is to use the derivative of \\(f()\\) in order to tweak an initial guess for the solution to be a little better. A Newton step is actually a rather aggressive proposal for improving the starting value. Like many forms of aggression, it can sometimes get you exactly the opposite of what you sought. So in practice, it may be better to take only part of a Newton step. Here is a function that creates another function that gives the Newton step for a specified \\(f()\\) and \\(v\\): NewtonStepFun &lt;- function(tilde, v) { f &lt;- makeFun(tilde) fprime &lt;- D(tilde) # create a new function that calculates the Newton step for # this f() and this v, at a specified input x. function(x) { (v - f(x))/fprime(x) } } To illustrate, let’s make an example function \\(f(x)\\equiv \\left[x^2 - 3x\\right] \\left[1.3 + \\pnorm(x)\\right]\\). f &lt;- makeFun((x^2 - 3*x)*(1.3 + pnorm(x)) ~ x) slice_plot(f(x) ~ x, domain(x=-2:4)) %&gt;% gf_hline(yintercept = ~ 5, color=&quot;magenta&quot;) Figure 31.2: An example function \\(f(x)\\) which we seek to solve for an output of 5. We seek to solve \\(f()\\) for an output of 5, that is, we are looking for an input \\(x^\\star\\) where \\(f(x^\\star) = 5\\). You can see the two solutions in Figure 31.2: for an input near \\(x=-1\\) the output is close to 5, as it is for an input of \\(x=3.5\\). Supposed you were given an initial guess \\(x_0 = -2\\). You can see from the graph that a step of about length 1 to the right will bring you close to where the output of the function will be 5. Likewise, for an initial guess of \\(x_0 = 4\\), you would want to take a step to the left of length about 0.5. That’s what a Newton step is supposed to do, bring you closer to the correct answer. Let’s construct a function that gives us the Newton step for any starting point \\(x\\), and test it out step_for_f &lt;- NewtonStepFun(f(x) ~ x, v = 5) step_for_f(-2) # should be about + 1 ## [1] 0.9435921 step_for_f(4) # should be about - 0.5 ## [1] -0.3651944 Now that we have an expression for the step, we package it up into the function \\(g()\\) to be used for the iteration and then iterate \\(g()\\) from a starting guess. g &lt;- function(x) x + step_for_f(x) g(-2) ## [1] -1.056408 g(g(-2)) ## [1] -0.8702095 g(g(g(-2))) ## [1] -0.8661124 g(g(g(g(-2)))) ## [1] -0.8661108 g(g(g(g(g(-2))))) ## [1] -0.8661108 The first iteration brought us from the \\(x_0 = -2\\) starting point a little closer to the solution. A double iteration brought us further to the right. the successive iterations show little or now change from the output to the input, a good sign that we’ve done enough work and can declare ourselves done. Of course, you always want to make sure that the answer gotten, \\(x^\\star = -0.8661108\\) is right. For this, we compute \\(f(x^\\star)\\) f(-0.8661108) # should be 5 ## [1] 5 Other than the exercises in this chapter, you will not have much need in this course to build your own functions for iterative improvement. But it is the mathematical technology behind the operators that perform the solving-task and the argmax-task. 31.4 Software for the tasks Evaluation of a function—number one in the list at the head of this chapter—is so central to the use of computer languages generally that every language provides a direct means for doing so. In R, as you know, the evaluation syntax involves following the name of the functions by a pair of parentheses, placing in those parenthesis the values for the various arguments to the function. Example: log(5) The other six operations on functions listed above, there is one (or sometimes more) specific R/mosaic functions. Every one of them takes, as a first argument, a tilde expression describing the function on which the operation is to be formed; on the left side is a formula for the function (which can be in terms of other, previously defined functions), on the right side is the with-respect-to variable. Differentiate D(). Returns a function. Anti-differentiate antiD(). Returns a function. Integrate: Integrate(). Returns a number. Solve Solve(). Returns a data frame with one row for each solution found. Argmax argM() Finds one argmax and one argmin in the domain. local_argM() looks for all the local argmaxes and argmins. Returns a data frame with one row for each argmax or argmin found. Iterate Iterate(). Returns a data frame with the value of the initial input and the output after each iteration. Each of operations 4-6 involves the specification of a domain. For Integrate(), this is, naturally, the domain of integration: the upper and lower bounds of the integral For Solve() and argM() the domain specifies where to search for the answer. Iterate() is slightly different. After the tilde expression comes an initial value \\(x_0\\) and then n= which you use to set the number of times to iterate. Another two R/mosaic functions that you frequently use generate graphics. slice_plot() contour_plot() These always take two main arguments: a tilde expression describing the function to be graphed and a plotting domain. Finally, there is one function for plotting variables from data frames: gf_point(). Again, the first argument is a tilde expression specifying two of the data frame’s variables: the variable on the left side will be assigned to the vertical axis, the variable on the right side to the horizontal axis. 31.5 Exercises CONSTRUCT SOME NEWTON STEPS by hand Pick an initial guess by graphing the function. Dimension of \\(-f&#39;(x_0) / f&#39;&#39;(x_0)\\) Create a Newton step for optimization Know the arguments to the functions D(), antiD(), Solve(), argM(), and Iterate(), and be able to use them. We’re writing \\(f&#39;(x_0)\\) to stand for the more wordy version: \\(\\partial_x f(x=x_0)\\)/↩︎ A traditional name for such a person is “numerical analyst.”↩︎ "],["towards-an-optimum.html", "Chapter 32 Towards an optimum", " Chapter 32 Towards an optimum Gradient ascent A bit of optimization expectation maximization maybe after probability. "],["solving-and-inverting.html", "Chapter 33 Solving and inverting", " Chapter 33 Solving and inverting \\(e^{x+1}\\) \\[\\line(x) \\equiv \\frac{1}{s}\\left[x - x_0\\right]\\] Iteration Zero-finding "],["data-driven-functions.html", "Chapter 34 Data-driven functions", " Chapter 34 Data-driven functions Splines, etc. "],["probability.html", "Chapter 35 Probability", " Chapter 35 Probability Quantifying uncertainty with probability Densities and cumulatives Exponential distributions Gaussian distributions "],["risk-and-expectation.html", "Chapter 36 Risk and expectation", " Chapter 36 Risk and expectation Discounting "],["differential-modeling.html", "Chapter 37 Differential modeling", " Chapter 37 Differential modeling Volumes, lengths, … "],["constraints-and-optimization.html", "Chapter 38 Constraints and optimization", " Chapter 38 Constraints and optimization ta da "],["tukeys-ladder.html", "Chapter 39 Tukey’s ladder", " Chapter 39 Tukey’s ladder Extending the modeling framework "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
