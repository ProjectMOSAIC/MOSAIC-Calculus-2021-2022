# Linear combinations of vectors {#linear-combs-vectors}

```{r include=FALSE}
library(Znotes)
```

::: {.underconstruction}
Chapter not yet released

<!--
$\vec{v}$ \vec

$\mathit{M}$ \mathit

$\color{magenta}{\alpha}$


$\color{red}{\mathit{M}}$
$\color{green}{\mathit{M}}$
$\color{blue}{\mathit{M}}$
$\color{cyan}{\mathit{M}}$
$\color{magenta}{\mathit{M}}$
$\color{yellow}{\mathit{M}}$
$\color{black}{\mathit{M}}$
$\color{gray}{\mathit{M}}$
$\color{white}{\mathit{M}}$
$\color{darkgray}{\mathit{M}}$
$\color{lightgray}{\mathit{M}}$
$\color{brown}{\mathit{M}}$
$\color{lime}{\mathit{M}}$
$\color{olive}{\mathit{M}}$
$\color{orange}{\mathit{M}}$
$\color{pink}{\mathit{M}}$
$\color{purple}{\mathit{M}}$
$\color{teal}{\mathit{M}}$
$\color{violet}{\mathit{M}}$
-->
:::

<div style="float:right;">[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/Block-5/B5-linear-combinations.Rmd)</div>

In this chapter, we introduce ***linear combinations of vectors***. As you recall, a linear combination is a sum of basic elements each of which has been ***scaled***. For instance, in Block 1 we looked at linear combinations of functions such as $$g(t) = A + B e^{kt}$$ which involves the basic functions $\text{one}(t)$ and $e^{kt}$ scaled respectively by $A$ and $B$. Linear combinations of vectors involve scaling and addition, which are simple seen either as numerical operations or a geometric ones. A useful concept will be the set of all vectors that can be constructed as linear combinations of given vectors. This set of all possibilities, called the ***subspace spanned*** by the given vectors is key to understanding how to find the "best" scalars for a given purpose.

## Scaling vectors

To scale a vector means to change its length without altering its direction. Scaling by a negative number flips the vector tip-for-tail. Figure \@ref(fig:scaling) shows two vectors $\vec{v}$ and $\vec{w}$ together with several scaled versions of each.

```{r scaling, echo=FALSE, fig.cap="Vectors $\\vec{v}$ and $\\vec{w}$ and some scaled versions of them."}
Znotes::gvec(from=c(0,2), to=c(1/2, 1.5), color="magenta", label="W", where=0.5, nudge=0.2, flip=TRUE) %>%
  Znotes::gvec(from=c(0.8,1.8), to=c(1.8,0.8), color="magenta", label="2W", nudge=0.2, flip=FALSE) %>%
  Znotes::gvec(from=c(3,0.7), to=c(1.5,2.2), color="magenta", label="--3W", nudge= 0.2, flip=TRUE) %>%
  Znotes::gvec(from=c(3.5,1.2), to=c(3.75,0.95), color="magenta", label="0.5 W", nudge= 0.4, flip=TRUE) %>%
  Znotes::gvec(from=c(2,3.2), to=c(3,3.2), color="brown", label="V", nudge= 0.2, flip=TRUE) %>%
  Znotes::gvec(from=c(2,2.6), to=c(2.75,2.6), color="brown", label="0.75 V", nudge= 0.3, flip=FALSE) %>%
  Znotes::gvec(from=c(1,3.7), to=c(3,3.7), color="brown", label="2 V", nudge= 0.2, flip=TRUE) %>%
  Znotes::gvec(from=c(1.5,2.5), to=c(0,2.5), color="brown", label="-1.5 V", nudge= 0.2, flip=TRUE) %>%
  gf_refine(coord_fixed(xlim=c(0,4), ylim=c(0,4))) %>%
  Znotes::graph_paper(xticks=seq(0,4,by=0.25))
```

The vectors we create by scalar multiplication can be placed anywhere we like.

Arithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g.

$$\vec{u} = \left[\begin{array}{r}1.5\\-1\end{array}\right]\ \ \ \ 2\vec{u} = \left[\begin{array}{r}3\\-2\end{array}\right]\ \ \ \ 
-\frac{1}{2}\vec{u} = \left[\begin{array}{r}-0.75\\0.5\end{array}\right]\ \ \ \ $$

Every vector is associated with a subspace that is ***one-dimensional***; you can only reach the points on a line by stepping in the direction of a vector.

## Adding vectors

To add two vectors, choose either one of the vectors as a start, then move the tail of the second vector to the tip of the first, as in Figure \@ref(fig:add-yellow-green).

```{r add-yellow-green, echo=FALSE, fig.align="center", out.width="50%", fig.cap="Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector."}
knitr::include_graphics("www/pencils/addition.png")
```

Adding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to whatever place is convenient. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result---the blue pencil in the picture above---has the length and direction from the tail of the first pencil (yellow) to the tip of the second (green). But so long as we maintain this length and direction, we can put the result (blue) anywhere we want.


Arithmetically, vector addition is simply a matter of working with each component individually. For instance, consider adding two vectors $\vec{v}$ and $\vec{w}$:
$$\underbrace{\left[\begin{array}{r}1.5\\-1\\2\\6\end{array}\right]}_\vec{v} + \underbrace{\left[\begin{array}{r}2\\4\\-2\\-3.2\end{array}\right]}_\vec{w} = \underbrace{\left[\begin{array}{r}3.5\\3\\0\\2.8\end{array}\right]}_{\vec{v} + \vec{w}}$$   

Unlike our pencil exemplars of vectors, which must of physical necessity always be in the three-dimensional space we inhabit, mathematical vectors can be embedded in any-dimensional space. Addition is applicable to vectors embedded in the same space. Arithmetically, this means that the two vectors to be added must have the same number of components.  

Arithmetic ***subtraction*** of one vector from another is a simple component-wise operation. For example:
$$\underbrace{\left[\begin{array}{r}1.5\\-1\\2\\6\end{array}\right]}_\vec{v} {\Large -} \underbrace{\left[\begin{array}{r}2\\4\\-2\\-3.2\end{array}\right]}_\vec{w} = \underbrace{\left[\begin{array}{r}-0.5\\-5\\4\\9.2\end{array}\right]}_{\vec{v} - \vec{w}}\ .$$
From a geometrical point of view, many people like to think of $\vec{v} - \vec{w}$ in terms of placing the two vectors **tail to tail** as in Figure \@ref(fig:subtract-blue-from-yellow). Read out the result as the vector running from the tip of $\vec{v}$ to the tip of $\vec{w}$. In Figure \@ref(fig:subtract-blue-from-yellow), the yellow vector is $\vec{v}$, the blue vector is $\vec{w}$. The result of the subtraction is the green vector.

```{r subtract-blue-from-yellow, echo=FALSE, fig.align="center", out.width="50%", fig.cap="Subtracting blue from yellow gives green."}
knitr::include_graphics("www/pencils/subtraction.png")
```

::: {.takenote data-latex=""}
Vector addition and subtraction work just like arithmetic with scalars, but vector ***multiplication*** and ***division*** do not follow the same pattern.

As a matter of arithmetic, it's easy to carry out the ***component-wise multiplication*** or ***component-wise division***, for instance

$$\underbrace{\left[\begin{array}{r}1.5\\-1\\2\\6\end{array}\right]}_\vec{v} {\Large \times} \underbrace{\left[\begin{array}{r}2\\4\\-2\\-3.2\end{array}\right]}_\vec{w}\ \ \  = \underbrace{\left[\begin{array}{r}3\\-4\\-4\\-18.12\end{array}\right]}_\text{componentwise product}\ \ \text{is NOT vector multiplication}\ .$$
In vector mathematics, "multiplication" is a much richer concept than componentwise arithmetic. In fact, there are four different kinds of "multiplication" and it's important to keep track of which one you mean in any context. In this book, we will use two kinds of "multiplication":

* ***matrix multiplication*** which, as you'll see later in the chapter, is about linear combinations of vectors.
* ***inner product***, which we prefer to call ***dot product*** and encountered in the previous chapter as a way of calculating vector lengths and the angle between vectors, is a special case of matrix multiplication.

Another two forms of "multiplication," which we won't use in this book are:

* ***cross product*** which is very important in physics and engineering.
* ***outer product*** which has important applications in data science.

:::


## Linear combinations

In the previous chapter, we suggested that you think of a vector as a "step" or displacement in a given direction and of a given magnitude as in, "1 foot to the northeast." This interpretation highlights the mathematical structure of vectors: just a direction and a length, nothing else. 

The "step"-interpretation is also faithful to an important reason why vectors are useful. We use steps to get from one place to another. Similarly, a central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one "place" to another. We've used quotation marks around "place" because we are not necessarily referring to a physical destination. We'll get to what else we might mean by "place" later in this chapter.

As a fanciful example of getting to a "place," consider a treasure hunt. You are given these instructions to get there:

> i. On June 1, go to the flagpole before sunrise.
ii. At 6:32, walk 213 paces away from the sun.
iii. At 12:19, walk 126 paces toward the sun.

The sun position varies over the day, so the direction to the sun on June 1 at 6:32  will be different than at 12:19. Figure \@ref(fig:sun-direction) show an aerial views annotated with the direction of the sun at 6:32 and 12:19 on June 1.

```{r sun-direction, echo=FALSE, out.width="40%", fig.show="keep", fig.cap="Maps showing the directions of sunrise and sunset on June 1 at latitude/longitude (38.0091,-104.8871). The Sun's direction at 6:32 is shown in the left map, the direction at 12:19 in the right map. Source: [suncalc.org](https://www.suncalc.org/)"}
knitr::include_graphics("www/Sun-6-32.png")
knitr::include_graphics("www/Sun-12-19.png")
```

The treasure-hunt directions are in the form of a ***linear combination*** of vectors. For each of the two vectors described in the treasure instructions, the length of the vector is 1 pace. (Admittedly, not a scientific unit of length.) The direction of each vector is toward the sun. Scaling vector (ii) by -213 and vector (iii) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure. 

Since this is a calculus book, and calculus is about functions, our interest in linear combinations of vectors in this book relates to the construction (and de-construction) of functions. 

Recall from Block 1 (Section `r Sections$functions_as_tables`) the idea of representing a function as a ***table*** of inputs and the corresponding outputs. 

Here is such a table with some of our pattern-book functions.

```{r echo=FALSE}
CW <- tibble(t=seq(0,5, by=0.1)) %>%
  mutate(`one(t)` = 1) %>%
  mutate("identity(t)" = t) %>%
  mutate("exp(t)" = exp(t)) %>%
  mutate("sin(t)" = sin(t)) %>%
  mutate("pnorm(t)" = pnorm(t))

and_so_on(CW, top=5, bottom=5) %>% kableExtra::kable_minimal()
```
In this representation, each of the pattern-book functions is a column of numbers, that is, a ***vector***.

Functions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function $g(t) \equiv 3 - 2 t$ is $3\cdot \text{one}(t) - 2 \cdot \text{identity}(t)$

```{r echo=FALSE}
CW <- tibble(t=seq(0,5, by=0.1)) %>%
  mutate(`one(t)` = 1) %>%
  mutate("identity(t)" = t) %>%
  mutate("g(t)" = 3 - 2*t)

and_so_on(CW, top=5, bottom=5) %>% kableExtra::kable_minimal()
```

The table above is a collection of four vectors:  $\vec{\text{t}}$, $\vec{\text{one(t)}}$,  $\vec{\text{identity(t)}}$, and $\vec{\text{g(t)}}$. Each of those vectors has 51 components. In math-speak, we can say that the vectors are "embedded in a 51-dimensional space."




## Matrices and linear combinations

A collection of vectors, such as the one displayed in the previous table, is called a ***matrix***. Each of the vectors in a matrix must have the same number of components. 

As mathematical notation, we will use **bold-faced**, capital letters to stand for matrices, for example $\mathit{M}$. The symbol $\Uparrow$ is a reminder that a matrix can contain multiple vectors, just as the symbol $\uparrow$ in $\vec{v}$ reminds us that the name "$v$" refers to a vector.

In the conventions for data, we give a name to each column of a data frame so that we can refer to it individually. In the conventions used in vector mathematics, names are not used to refer to the individual vectors. 

As a case in point, let's look at a matrix $\mathit{M}$ containing the two vectors which we've previously called $\vec{\text{one(t)}}$ and  $\vec{\text{identity(t)}}$:
$$\mathit{M} \equiv \left[\begin{array}{rr}1 & 0\\
1 & 0.1\\
1 & 0.2\\
1 & 0.3\\
\vdots & \vdots\\
1 & 4.9\\
1 & 5.0\\
\end{array}\right]\ .$$
The linear combination which we might previous have called $3\cdot \vec{\text{one(t)}} - 2\cdot \vec{\text{identity(t)}}$ can be thought of as 
$$\left[\overbrace{\begin{array}{r}
1\\
1 \\
1 \\
1 \\
\vdots &\\
1 \\
1 
\end{array}}^{3 \times}
\stackrel{\begin{array}{r}
\\
\\
\\
\\
\\
\\
\\
\\
\end{array}}{\Large + \ }
\overbrace{\begin{array}{r}
0\\
0.1 \\
0.2 \\
0.3 \\
\vdots\\
4.9 \\
5.0 
\end{array}}^{-2 \times}\right] = \left[\begin{array}{r}
\\ \\ 3\\
2.8\\2.6\\2.4\\\vdots\\-6.8\\-7.0\\ \\ \\
\end{array}\right]\ ,$$ but this is not conventional notation. Instead, we would write this more concisely as 
$$\stackrel{\Large\mathit{M}}{\left[\begin{array}{rr}1 & 0\\
1 & 0.1\\
1 & 0.2\\
1 & 0.3\\
\vdots & \vdots\\
1 & 4.9\\
1 & 5.0\\
\end{array}\right]} \cdot 
\stackrel{\Large\vec{w}}{\left[\begin{array}{r}2\\-3\end{array}\right]}$$
In symbolic form, the linear combination of the columns of $\mathit{M}$ using respectively the scalars in $\vec{w}$ is simply $\mathit{M} \cdot \vec{w}$. This is called ***matrix multiplication***. 

Naturally, the operation only makes sense if there are as many components to $\vec{w}$ as there are columns in $\mathit{M}$.

::: {.takenote data-latex=""}
"Matrix multiplication" might better have been called "$\mathit{M} linearly combined by $\vec{w}$." But "matrix multiplication" is the phrase you will hear in other courses. 
:::

::: {.rmosaic data-latex=""}
In R, you can make vectors with the `rbind()` command, short for "bind rows," as in 
```{r}
rbind(2, 5, -3)
```

with the components of the vector presented as successive arguments to the function. 

One way to make a matrix is with the `cbind()` command, short for "bind columns". The arguments to `cbind()` will typically be vectors created by `rbind().` For instance, the matrix 
$$\mathit{A} \equiv \left[\vec{u}\ \ \vec{v}\right]\ \ \text{where}\ \ \vec{u} \equiv \left[\begin{array}{r}2\\5\\-3\end{array}\right]\ \ \text{and}\ \ \vec{v} \equiv \left[\begin{array}{r}1\\-4\\0\end{array}\right]$$
can be constructed in R with these commands.



```{r}
u <- rbind(2, 5, -3)
v <- rbind(1, -4, 0)
A <- cbind(u, v)
A
```

To compute the linear combination $3 \vec{u} + 1 \vec{v}$, that is, $\mathit{A} \cdot \left[\begin{array}{r}3\\1\end{array}\right]$ you use the matrix multiplication operator `%*%`. For instance, the following defines a vector $$\vec{x} \equiv \left[\begin{array}{r}3\\1\end{array}\right]$$ to do the job in a way that's easy to read:

```{r}
x <- rbind(3, 1)
A %*% x
```

It's a mistake to use `*` instead of `%*%` for matrix multiplication. Remember that `*` is for **componentwise multiplication** which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with:

```{r error=TRUE}
A * x
```
The phrase "non-conformable arrays" is R-speak for saying "The arguments to `*` don't have compatible shapes for componentwise multiplication."
:::


## Sub-spaces

Recall that a vector with $n$ components can be said to be embedded in an $n$-dimensional space. You might like to think of the embedding space as a kind of club with restricted membership. A vector with 2 elements is entitled to join the 2-dimensional club, but a vector with more or fewer than 2 elements cannot be admitted to the club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on.

The clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded. 

Now imagine that the clubhouse can be arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let's say $\vec{v}$ and $\vec{w}$ decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a ***subspace***. 

A subspace has it's own rules for admission. New vectors can be in the subspace only if they can be constructed as a linear combination of the sponsoring members. The subspace itself consists of all vectors that are a linear combination of the sponsors.

As an example, consider the clubhouse that is open to any and all vectors with two components. The diagram in Figure \@ref(fig:vector-2-clubhouse)

```{r vector-2-clubhouse, echo=FALSE, fig.cap="The clubhouse for vectors with two components, with several members in attendence. Member $\\vec{v}$ has sponsored a subspace. All the members of the subspace must be scaled versions of $\\vec{v}$."}
# Show the whole clubhouse and a subspace sponsored by $\vec{v}$
```

Now suppose that some of the other vectors decide to sponsor subspaces. The subspace sponsored by $\vec{u}$ and $\vec{w}$ contains all the vectors that can be constructed as linear combinations of $\vec{u}$ and $\vec{w}$. This subspace is, in fact, the entirety of the 2-dimensional clubhouse.

On the other hand, the subspace sponsored by $\vec{w}$ and $\vec{x}$ is not the entire clubhouse because $\vec{w}$ and $\vec{x}$  are aligned with one another. The $\vec{w}$ and $\vec{x}$ subspace can contain only vectors that are aligned in the same way.

More mathematically, given one or more vectors with the same number of components, the embedding space is the set of all possible vectors with that number of components. A subspace of the embedding space is defined by a collection of one or more vectors. If the vectors are $\vec{u}$ and $\vec{w}$, we speak of the ***subspace spanned*** by $\vec{u}$ and $\vec{w}$. The subspace spanned by a matrix is the subspace spanned by the vectors that compose the matrix. 





Figure \@ref(fig:two-vecs) includes a second vector $\vec{w}$ along with the vector $\vec{v}$ seen in Figure \@ref(fig:single-scale). Each vector has its own subspace, again shown as dotted lines.

```{r two-vecs, echo=FALSE, fig.cap="The subspaces spanned by each of two vectors $\\vec{v}$ and $\\vec{w}$."}
gf_segment(2 + - 5 ~ (-3) + 4, color = "magenta", linetype="dotted") %>%
  gvec(from=c(-1,0), to=c(1, -2), color="magenta", 
       label="") %>%
  gvec(from=c(0,-3), to=c(1,-1), color="brown",
       label="") %>%
  gf_text(-1.5 ~ 0, label="v", color="magenta") %>%
  gf_text(-3 ~ 0.4, label="w", color="brown") %>%
  gf_segment(-5 + 3 ~ -1 + 3, color="brown", linetype="dotted") %>%
  gf_refine(coord_fixed(), 
            scale_y_continuous(breaks=NULL),
            scale_x_continuous(breaks=NULL)) %>%
  gf_labs(y="", x="")
```



Things get interesting when we consider not just the subspace spanned by the vectors individually, but the subspace spanned by them ***jointly***. Recall that the subspace spanned by $\vec{v}$ is all the vectors that can be created by scalar multiplication of $\vec{v}$, that is, all the vectors $\alpha \vec{v}$ for $-\infty < \alpha < \infty$. 

With two vectors, the subspace is all the vectors that can be created by a linear combination of the two, that is $$\alpha \vec{v} + \beta \vec{w}$$ where $-\infty < \alpha < \infty$ and $-\infty < \beta < \infty$. To start the trip, walk to the point $\alpha \vec{v}$ on the subspace of $\vec{v}$. Now, pick up $\vec{w}$ and place its tail at the point you just reached, $\alpha \vec{v}$. Finally, walk from that point in the direction of $\vec{w}$ for $\beta$ steps.

By adjusting $\alpha$ and $\beta$ appropriately, you can get to any point in the plane. In other words, the subspace spanned by the set of vectors $\{\vec{a}, \vec{b}\}$ is the entire plane. 

CAN WE PLACE A CALCPLOT3D picture here.

Figure \@ref(fig:two-vecs), lying as it does on the two-dimensional surface of your screen or paper, fails to indicate clearly that even in three- or higher-dimensional space, two (non-aligned) vectors will span a plane with a particular orientation. To see this better, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. You can point the whole rigid assembly in any direction you like. The angle between them will remain the same. 

Place a card on top of the pencils, slipping it between your pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determine the orientation of the surface. This simple fact will be extremely important later on.

You could replace the pencils with line segments drawn on the card underneath each pencil. Now you have the angle readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors.

Notice that you can also lay a card along a *single* vector. What's different here is that you can roll the card around the pencil; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not determine uniquely the orientation of the planar surface in which the two vectors can reside. But with two fixed vectors, there is only one such surface.

## Functions as vectors

In Section `r Sections$exp_curve_fitting` we looked at the use of the exponential function to describe the temperature of hot water cooling to room temperature. The exponential decreasing function is of course $e^{-kt}$ and we can find $k$ by estimating the half-life of exponential decay, which is about 36 minutes in the data we used. (Figure \@ref(fig:water-scaled-exp)).

In this section, we're going to think about functions in terms of vectors. There are huge advantages to thinking in this way, but it will take some time for you to see them clearly.

Recall the data on cooling water gave the temperature (in degrees C) versus time (in minutes). Economy on the page argues against showing all the rows of the `CoolingWater` data frame, but you have access to it in the `r sandbox_link()`.

```{r echo=FALSE}
CW <- CoolingWater[c(1,3,5,7,10,15,20,30,40,50, 75, 100, 125, 150, 175),]
row.names(CW) <- NULL
knitr::kable(CW) %>% kableExtra::kable_minimal()
```

The `temp` column of the data frame is a set of numbers, hence interpretable as a vector. As a vector in 15 dimensions, there's not much to be said about its *direction*, but the length is easy enough: take the square root of the sum of squares of the components. That comes to 251.3 degrees C. This is merely an arithmetic fact, the consequence of adding together 15 numbers. It has the dimension of temperature T, but has nothing to do with the hot-oven-like 251 C temperature.

This `temp` vector is playing to role of the buried treasure; it's the destination we want to reach. What have we got to reach it with?

This is where our basic modeling function $e^{-kt}$ comes in. By evaluating $e^{-kt}$ at each of the values of $t$ in the `time` column, we create another column, which we'll call `expkt`. Knowing that the half-life is about 36 minutes, we'll use $k=\ln(2)/36 \approx 0.02$. At the same time, anticipating what is to come, we'll add another column which we'll call, following statistical practice, the `intercept` column and which represents the constant function (evaluated at each of the times).

```{r}
CW <- CW %>% 
  mutate(expkt = exp(-0.02*time),
         intercept = 1)
```

```{r echo=FALSE}
knitr::kable(CW) %>% kableExtra::kable_minimal()
```

Confirm for yourself that the `expkt` column really does match an exponential decay with a half-life of about 36 minutes. You can see that at time zero the value of expkt, just as expected for an exponential. At time 36, somewhere between the rows for times 29 and 39, the value is about 0.5. At time 72---two half-lives after the start---the value should be 0.25, closely matching the recorded temperature at time 74.

The two vectors `expkt` and `intercept`, like any two (non-aligned) vectors, span a planar subspace. Since `expkt` and `intercept` are embedded in a 15-dimensional space---this is just saying that there are 15 rows in the data frame---the plane is a subspace of the 15-dimensional space. This statement can seem hopelessly abstract, so let's try to give a more concrete visualization. For the visualization, we'll move onto the familiar ground of a graph of temperature versus time. 

In the domain of temperature vs time, each of the linear combinations $\alpha$`expkt`$\ + \beta$`intercept`, appears as a set of 15 dots. Figure \@ref(fig:water-dots) shows three such sets of 15 does in three different colors, along with the 15 points of the actual temperature data. To show more than the three sets of dots would be visually confusing. Instead, we'll add to the graph functions of the form $\alpha\, e^{-0.02 t} + \beta\, 1$.

```{r water-dots, echo=FALSE, fig.cap="Some of the possible linear combinations of the vectors `expkt` and `intercept`. Each combination is a set of 15 dots, but many of them are shown here as continuous functions that would connect the dots for that particular linear combination.",warning=FALSE}
CW <- CW %>%
  mutate(one = -51*expkt + 80*intercept,
         two = -36*expkt + 120*intercept,
         three = 100*expkt + 15*intercept,
         )
colors <- rainbow(10)
gf_point(temp ~ time, data = CW) %>%
  gf_point(one ~ time, color = "magenta") %>%
  gf_point(two ~ time, color = "brown") %>%
  gf_point(three ~ time, color = "tomato") %>%
  slice_plot(80 - 51*exp(-0.02*time) ~ time, color="magenta") %>%
  slice_plot(120 - 36*exp(-0.02*time) ~ time, color="brown") %>%
  slice_plot(15 + 100*exp(-0.02*time) ~ time, color="tomato") %>%
  slice_plot(35 - 20*exp(-0.02*time) ~ time, color=colors[1]) %>%
  slice_plot(45 - 70*exp(-0.02*time) ~ time, color=colors[2]) %>%
  slice_plot(55 + 45*exp(-0.02*time) ~ time, color=colors[4]) %>%
  slice_plot(65 - 40*exp(-0.02*time) ~ time, color=colors[5]) %>%
  slice_plot(75 + 30*exp(-0.02*time) ~ time, color=colors[6]) %>%
  slice_plot(95 + 10*exp(-0.02*time) ~ time, color=colors[7]) %>%
  slice_plot(25.4 + 65.7*exp(-0.02*time) ~ time, color = "black") %>%
  gf_lims(y = c(0,125))
  
  
```

We we to plot *all* the linear combinations of `expkt` and `intercept`, the graphics frame would be completely covered with ink. But each individual vector produced by a linear combination will look much of a kind with the ten shown here.

The functions shown in Figure \@ref(fig:water-dots) all inhabit the two-dimensional subspace spanned by `expkt` and `intercept`. But there is a lot more to the 15 dimensional space. What do functions look like that inhabit the space outside of the two-dimensional `expkt`&`intercept` subspace? Figure \@ref(fig:13-handful) shows a handful of them. Each is different in kind from the functions shown in Figure \@ref(fig:water-dots).

```{r 13-handful, echo=FALSE, fig.cap="A handful of the vectors in the 15-dimensional space outside of the 2-dimensional `expkt`&`intercept` subspace.", warning=FALSE}
P <- ggplot() + geom_blank()

for (k in 1:8) {
  Pts <- tibble(time = CW$time + runif(15,0, 25)) 
  Pts <- Pts %>%
    mutate(y=runif(15,20,80))
  f <- spliner(y ~ time, data = Pts)
  P <- P %>% 
    slice_plot(f(time) ~ time, color = colors[k], 
               domain(time=0:200), npts=200)
}
P %>% gf_lims(y=c(0,125))
```
There are a lot of crazy-looking functions out there in 15-dimensional space!

Look back at Figure \@ref(fig:water-dots) and focus your attention on the function drawn in black. That function is a reasonable match to the data (plotted as black dots). The question we face now is how to find such a function by searching through a 15-dimensional space. That's the task we take on in the next chapter.

::: {.intheworld}
It's pretty easy to visualize the length of a vector and the arithmetic is straightforward even in n-dimensional space. For a vector $\vec{v}$ with components $$\vec{v} \equiv \left[\strut \begin{array}{c}v_1\\v_2\\\vdots\\v_n\end{array}\right]$$ the length is $\sqrt{\strut v_1^2 + v_2^2 + \cdots + v_n^2}$. Similarly, the ***dot product*** between $\vec{v}$ and $\vec{w}$ is $\vec{v} \cdot \vec{w} \equiv v_1 w_1 + v_2 w_2 + \cdots + v_n w_n$. In terms of the dot product, the vector length is $\sqrt{\strut\vec{v} \cdot \vec{v}}$. And by combining the dot products, we can calculate the angle between two vectors: $$\cos(\theta_{v,w}) = \frac{\vec{v}\cdot\vec{w}}{\sqrt{\strut (\vec{v}\cdot\vec{v})\ (\vec{w}\cdot\vec{w}})}$$

As you know, a vector has two properties: length and direction. Both of these can be calculated using the dot product.

This suggests a way to consider other mathematical objects, such as functions, as vectors. All we need is a reasonable definition for a dot product. Suppose we have two functions, $f(t)$ and $g(t)$ defined on some domain, say $0 < t < 2\pi$. A dot product multiplies the objects together and accumulates the products. When the vectors are sets of numbers, the accumulation is to add up component-by-component products. By analogy, to take the dot product of $f(t)$ and $g(t)$ over the domain, we can do an integral: $$f() \cdot g() \equiv \int_0^{2\pi} f(t) g(t) dt\ .$$

To illustrate, consider these two functions: $f(t) \equiv \sin(t)$ and $g(t) \equiv \sin(2 t)$, plotted below.

```{r, echo=FALSE, results=FALSE}
slice_plot(sin(t) ~ t, domain(t=0:(2*pi)),
           label_text = "f(t)", label_x=.92) %>%
slice_plot(sin(2*t) ~ t, color="magenta", label_text = "g(t)", label_x=.25)
```
How "long" are $f(t)$ and $g(t)$? `

`r {Integrate(sin(t)*sin(t) ~ t, domain(t=0:(2*pi))) %>% sqrt()} |> matrix_block(inline=FALSE, width="90%")`

`r {Integrate(sin(2*t)*sin(2*t) ~ t, domain(t=0:(2*pi))) %>% sqrt()} |> matrix_block(inline=FALSE, width="90%")`

They are both the same "length", which you might recognize as $\sqrt{\strut\pi}$.

What's the cosine of the angle between them?

`r {Integrate(sin(t) * sin(2*t) ~ t, domain(t=0:(2*pi))) / pi} |> matrix_block(inline=FALSE, width="90%")`

The cosine of the angle is zero---`1.3e-16` is just a round-off error. That means that the functions $\sin(t)$ and $\sin(2t)$ are **orthogonal** on the domain $0 < t < 2\pi$.
:::



## Matrices

We have been writing linear combinations of vectors in this format, where $a$ and  $b$are ***scalars***: $$a \vec{u} + b \vec{v}\ .$$
It's helpful to have a standard notation that makes it evident which things are being combined. 

A ***matrix*** is a collection of vectors, all of which live in the same dimensional space. For instance, the matrix containing $\vec{u}$ and  $\vec{v}$ looks like
$$\left[\begin{array}{cc}|&|\\\vec{u} & \vec{v} \\|&|\\ \end{array}\right]$$
The vertical lines are meant to indicate that each of $\vec{u}$ and $\vec{v}$ are a column in the matrix. For instance, supposing $$\vec{u} \equiv \left[\begin{array}{r}2\\5\\-3\end{array}\right]\ \ \text{and}\ \ \ \vec{v} \equiv \left[\begin{array}{r}1\\-4\\0\end{array}\right]\ ,$$
the matrix $\mathbf A$ containing $\vec{u}$ and $\vec{v}$ consists of two columns:
$$ {\mathbf A} \equiv \left[\overset{\strut}{\left[\begin{array}{r}2\\5\\-3\end{array}\right]}\ \left[\begin{array}{r}1\\-4\\0\end{array}\right]\ \right]$$
In this book, when we write the name of a matrix, we'll use a CAPITAL **bold-face** letter, as **M**. Conventionally, we do without the interior square brackets, writing
$$ {\mathbf A} \equiv \left[\begin{array}{r}2\\5\\-3\end{array}\ \begin{array}{r}1\\-4\\0\end{array}\right]$$

In scientific computing languages, there is almost always a type of object called a "matrix" or an "array." The one shown here, ${\mathbf A}$ has 3 rows and 2 columns. (Columns always run vertically, as with the columns in a building.)

Also, as a matter of convention, the linear combination of the vectors in a matrix is indicated by placing to the right of the matrix a column containing the coefficients of the combination $3 \vec{u} + 1 \vec{v}$, as with

$$ {\mathbf A} \left[\begin{array}{c}3\\1\end{array}\right] = \left[\begin{array}{r}2\\5\\-3\end{array}\ \begin{array}{r}1\\-4\\0\end{array}\right] \left[\begin{array}{c}3\\1\end{array}\right] = \left[\begin{array}{r}7\\11\\-9\end{array}\right]$$

Often, we'll want to make several different linear combinations of the vectors in a matrix. Since each individual linear combination is specified with a column of coefficients, the convention is to place the several sets of coefficients as side-by-side columns, that is, as a matrix. 

For example, consider these three different linear combinations of the two vectors $\vec{v}$ and $\vec{w}$:
$$\left[\begin{array}{r}2\\5\\-3\end{array}\ \begin{array}{r}1\\-4\\0\end{array}\right] \left[\begin{array}{rr}3 & 0 & -1\\1 & 2 & 0\end{array}\right] = \left[\begin{array}{r}7 &2 &-2\\11 & -8 & -5\\-9 & 0 & 3\end{array}\right]$$
In R, the left-hand side of the equation would involve constructing a matrix with the three pairs of coefficients:
```{r}
X <- cbind(
  rbind(3, 1),
  rbind(0, 2),
  rbind(-1, 0)
)
A %*% X
```

::: {.intheworld data-latex=""}
A CATEGORICAL VARIABLE AND HOW IT TRANSLATES INTO A SET OF VECTORS.
:::


::: {.example data-latex=""}
**From Talyor to Lagrange**

In Chapter `r Chaps$taylor` we met a method introduced by Brook Taylor (1685–1731) to construct a polynomial of order-$n$ that approximates any smooth function $f(x)$ close enough to some center $x_0$. The method made use of the ability to differentiate $f(x)$ at $x_0$ and produced the general formula:
$$f(x) \approx f(x_0) + \frac{f'(x_0)}{1} \left[x-x_0\right] + \frac{f''(x_0)}{2!} \left[x-x_0\right]^2 + \frac{f'''(x_0)}{3!} \left[x-x_0\right]^3 + \cdots + \frac{f^{(n)}(x_0)}{n!} \left[x-x_0\right]^n$$ where $f'(x_0) \equiv \partial_x f(x)\left.{\Large\strut}\right|_{x=x_0}$ and so on.

Using polynomials as approximating functions has been an important theme in mathematics history. Brook Taylor was neither the first nor the last to take on the problem. 

In 1795, [Joseph-Louis Lagrange](https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange)
(1736 – 1813) published another method for constructing an approximating polynomial of order $n$. Whereas the Taylor polynomial builds the polynomial that exactly matches the first $n$ derivatives at the center point $x_0$, the Lagrange polynomial has a different objective: to match exactly the values of the target function $f(x)$ at a set of ***knots*** (input values) $x_0$, $x_1$, $x_2$, $\ldots, x_n$. Figure \@ref(fig:lagrange-sine) shows the situation with the knots shown as orange dots.

```{r lagrange-sine, echo=FALSE, fig.cap="The Lagrange polynomial of order $n$ is arranged to pass exactly through $n+1$ points on the graph of a function $f(x)$."}
f <- sin
make_lagrange_unit <- function(x, x0) {
  bottom <- paste("(",
                  paste(
                    paste0("(", x0," - ", setdiff(x, x0), ")"), collapse = "*"),
                  ")")
  top <- paste(paste0("(x - ", setdiff(x, x0), ")"), collapse = "*")
  f <- function(x) {}
  body(f) <- parse(text=paste(top, "/", bottom))
  
  f
}
make_lagrange_poly <- function(xs, ys) {
  funs <- lapply(xs, function(p) make_lagrange_unit(xs, p))
  f <- function(x) {
    res <- 0
    vals <- lapply(funs, function(f) do.call(f, list(x)))
    for (k in 1:length(vals))
      res <- res + vals[[k]]*ys[k]
    
    return(res)
  }
  f
}
tp <- makeFun(x - x^3/6 + x^5/120 ~ x)
Pts <- tibble(x=c(-1, 0, 1, 3, 5, 7), y=f(x))
lp <- make_lagrange_poly(Pts$x, Pts$y)
  slice_plot(tp(x) ~ x, domain(x=-3:8), color="green", size=2, alpha=0.7,
             label_text="Taylor polynomial", label_x=0.1) %>%
  slice_plot(f(x) ~ x, domain(x=-3:8), label_text="f(x)") %>%
  slice_plot(lp(x) ~ x, domain(x=-3:8), color="magenta", size=2, alpha=0.5,
             label_text="Lagrange polynomial", label_x = 0.8) %>%
  gf_point(y ~ x, data = Pts, color="orange", size=6, alpha=0.5) %>%
  gf_lims(y=c(-1.5, 1.5))
```

The Lagrange polynomial is constructed of a linear combinations of functions, one for each of the knots. In the example of Figure \@ref(fig:lagrange-sine), there are 6 knots, hence six functions being combined. For knot 2, for instance, has coordinates $\left(\strut x_2, f(x_2)\right)$ and the corresponding function is:

$$p_2(x) = \frac{(x-x_1)}{(x_2 -x_1)}\left[\strut\cdot\right]\frac{(x-x_3)(x-x_4)(x-x_5)(x-x_6)}{(x_2 -x_3)(x_2 -x_4)(x_2 -x_5)(x_2 -x_6)}$$
The gap indicated by $\left[\strut\cdot\right]$ marks where a term being excluded. For $p_2(x)$ that excluded term is $\frac{(x-x_2)}{(x_2 - x_2)}$. The various functions $p_1(x)$, $p_2(x)$, $p_3(x)$ and so on each leave out an analogous term.

Three important facts to notice about these ingenious polynomial functions:

i. They all have the same polynomial order. For $k$ knots, the order is $k-1$.
i. Evaluated at $x_i$, the value of $p_i(x_i) = 1$. For instance, $p_2(x_2) = 1$.
i. Evaluated at $x_j$, where $j\neq i$, the value of $p_j(x_i) = 0$. For example, $p_2(x_3) = 0$.

The overall polynomial will be the linear combination $$p(x) = y_1\, p_1(x) + 
y_2\, p_2(x) + \cdots + y_k\, p_k(x)\ .$$
Can you see why?
:::


## Exercises

`r insert_calcZ_exercise("XX.XX", "9bAVr2", "Exercises/cat-understand-car.Rmd")`

`r insert_calcZ_exercise("XX.XX", "djpDYI", "Exercises/bee-beat-hamper.Rmd")`

`r insert_calcZ_exercise("XX.XX", "K8hC6q", "Exercises/pig-find-canoe.Rmd")`

`r insert_calcZ_exercise("XX.XX", "dIobrt", "Exercises/rabbit-buy-hamper.Rmd")`

`r insert_calcZ_exercise("XX.XX", "DPY4Ue", "Exercises/crow-cut-mug.Rmd")`

