# Parameters {#parameters}

<div style="float:right;">[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/modeling/01-parameters.Rmd)</div>

```{r include=FALSE}
book_file_name <- "modeling/01-parameters.html"
# Initialize skill file for the Block
cat("link, exercise, hash, file, skill\n", file="Skill_list.csv")
```

- Bound parameters listed as arguments to the function with default values

- Unbound parameters need to be specified as inputs when evaluating the function.


Similarly, the mathematical functions used in real-world applications---as opposed to the pattern-book functions---are adorned with ***parameters***. Parameters do the work of fitting the pattern to the units and dimensions of the input and output quantities. Continuing our analogy, if the pattern is for a sock, the parameter is the size needed to fit the foot. And just as there are different systems for sizing shirts, trousers, shoes, and so on, tradition has favored idioms for parameterizing the different kinds of pattern-book functions. We haven't yet introduced these idioms, but when we do in Chapter \@ref(params-intro) you are well advised to learn to interpret them at a glance.

JUST INTRODUCE INPUT AND OUTPUT SCALING and the nonlinear parameters in the basic modeling functions.


The pattern-book functions provide the modeler with a collection of shapes. They are not yet fully suited to represent real-world phenomena. To illustrate, consider Figure \@ref(fig:covid-exp) which shows the number of officially confirmed COVID-19 cases in the US in March 2020.

The case count versus time in the COVID pandemic was widely and appropriately described as "exponential." So it seems appropriate  alongside the data Figure \@ref(fig:covid-exp) shows the function $\text{cases}(t) \equiv e^t$ plotted as a $\color{magenta}{\text{magenta}}$ curve.

```{r covid-exp, echo=FALSE, fig.cap="Cumulative officially confirmed COVID-19 cases during the month of March, 2020. The red curve is $e^t$", warning=FALSE}
March <- Covid_US %>% filter(lubridate::month(date)==3, 
                             lubridate::year(date)==2020) %>%
  mutate(day = lubridate::mday(date))
Pcases <- gf_point(confirmed ~ day, 
         data = March) %>%
  gf_labs(y = "Cumulative confirmed cases", x = "Day in March, 2020") %>%
  gf_lims(y=c(0,200000)) 
Pcases %>%
  slice_plot(exp(t) ~ x, domain(t=c(0,15)), 
             color="magenta", label_text = "exp(t)\n\n",
             label_x=0.6, label_vjust = "left", size=1) 
```

There's an obvious mismatch between the data and the function $e^t$. Does this mean the COVID pattern is not exponential?

For the pattern book functions, the input is always a ***pure number***. For instance, we read $t=10$ off the horizontal axis at which point $e^t {\left.\Large\right|}_{t=10}  \approx 22000$, consistent with the curve shown in the graph. 

But the place on the horizontal axis marked 10 does not correspond to the number 10. Rather, that place stands for "10 days," a quantity with units. Perhaps surprisingly, there is no such thing as $e^{10\,\text{days}}$. The reason for this will be detailed in Chapter \@ref(dimensions-and-units), but for now we'll simply point out that the domain of $\exp()$ is the set of real numbers, and real numbers don't have units. 

If we want the input to $\text{cases}(t)$ to be denominated in days, we'll have to convert $t$ to a pure pure number (e.g. 10, not "10 days") *before* the quantity is handed off as the argument to $\exp()$. We do this by introducing a ***parameter*** when we use the exponential function for describing relationships between quantities. The standard from for this is $e^{kt}$ as opposed to $e^t$. The $k$ parameter will be a quantity with units of "per-day." Suppose we set $k=0.2 per day$. Then $k t{\LARGE\left.\right|}_{t=10 days} = 2$, a pure number:
$$0.2\, \text{day}^{-1} \cdot 10\, \text{days} = 0.2\ .$$
The use of a parameter like $k$ does more than handle the formality of converting input quantities into pure numbers. Having a choice for $k$ allows us to stretch or compress the function to align with the data. Figure \@ref(fig:covid-exp2) plots the modeling version of the exponential function to the COVID-case data:

```{r covid-exp2, echo=FALSE, fig.cap="Using the function form $A e^{kt}$, with parameters $k=0.19$ per day and $A = 573$ cases, matches the COVID-case data well."}
Pcases %>%
  slice_plot(573*exp(0.19*t) ~ t, interval(t=0:31), color="blue", label_x=0.5,
             label_text="A exp(k t)\nA = 593 cases\nk=0.19 per day\n", label_vjust = "left")
```

This chapter will introduce new functions, based on the pattern-book functions, but that have parameters so that the inputs and outputs can be **quantities** rather than pure numbers. 

::: {.intheworld data-latex=""}
You can see in Figure \@ref(fig:covid-exp2) that the exponential function plotted in $\color{blue}{\text{blue}}$ does not align perfectly with the day-by-day data. For instance, during the interval from March 8 to 21, the function output is consistently higher than the data suggest it should be. As part of the ***modeling cycle*** it's important to notice such discrepancies and try to understand them. In this case, it's likely that during the early days of the pandemic the number of reported deaths understated the actual number of COVID-related deaths. This happens because, in the early days many deaths from COVID were mis-attributed to other causes.

A bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that
$$A e^{kt} = e^{\log(A)} e^{kt} = e^{k t + \log(A)} = e^{k\left(\strut t + \log(A)/k\right)} = e^{k(t-t_0)}\ ,$$
where $$t_0 = - \log(A)/k = - \log(593)/0.19 = -33.6\ .$$
You can interpret $t_0$ as the starting point of the pandemic. When $t = t_0$, the model output is $e^{k 0} = 1$: the first case. According to the parameters we matched to the data for March, the pandemic's first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It's remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct.
:::

## Input and output scaling

Recall that each of the pattern-book functions has its own specific shape. For the purposes of modeling, of constructing a function that can match an observed pattern (for instance, seen from data) we need to stretch and shift the input and output to the pattern-book function. Such stretching and shifting has a simple functional form that you are already familiar with, the straight-line function. 

To illustrate, suppose that $f(x)$ is one of the pattern-book function, say, $\sin()$. The input to pattern-book $\sin()$ must always be a pure number and the output will always be a pure number. Consider a phenomenon that shows oscillatory behavior, for instance the length of daylight (in hours) as a function of the day-of-the-year (1 to 365, in days). The output of the modeling function is a quantity in hours, the input is a quantity in days. Neither this input nor the output are pure numbers. 

To use the pattern $\sin()$ as a basis for modeling, we replace the input name $x$ with a straight-line function: $x(t) = a t + b$. This gives us the function $$\sin(a t + b)$$ where $a$ and $b$ are parameters. If $t$ is to be the day-of-year in units of days, then the parameter $a$ will have units "per day," so that $a t$ will be a pure number.

The output of the function $\sin(a t + b)$ will be a pure number. In order to translate this into a quantity such as length of daylight, we apply another straight-line function, for instance $$\text{daylight}(y) \equiv A y + B$$ where $y$ stands for the output from $\sin(a t + b)$ for any input $t$. Putting this all together, we have the function
$$\text{daylight}(t) = A \sin(a t + b) + B\ ,$$ a function with four parameters: $a$, $b$, $A$, and $B$.

For example, for a location at latitude 40$^\circ$N, the length of daylight is approximately $$\text{daylight}_{40^\circ}(t) = 2.75 \sin(0.0173 t - 0.155) + 12\ ,$$ where $t$ is in days (January 1 is $t=1$ and December 31 is $t=365$), 
2.75 and 12 are in hours, 0.0173 is "per day" and 0.155 is a pure number.

Keep in mind that the straight-line function is often written 
$\line(t) = a\left(t-t_0\right)$. 
In this form, the daylight() function would be written
$$\text{daylight}_{40^\circ}(t) = 2.75 \sin(0.0173 \left(t - 9\right) + 12\ ,$$ where the 9 is in days.

## Parallel scales

The graphical format we have been using to display functions of one variable places the input on the horizontal axis and the output on the vertical axis. This is not the only way to draw a function. Consider these everyday objects: a thermometer and a ruler.

![](www/thermometer.png)     ![](www/ruler.jpeg)

Each object presents a read-out of what's being measured---temperature or length---on two different scales. At the same time, the objects provide a way to convert one scale to another.

A function gives the output for any given input. We represent the input value as a position on a number line---which we call an "axis"---and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output. The ***inverse function*** can be evaluated just as easily: switch the roles of the input and output scales.  

Taking the traditional ("imperial"^[Called "imperial" because it was codified in an 1825 Act of the British parliament. Inches are imperial, but temperature isn't part of the Act.]) unit scale as the input and the metric scale as the output, the two functions implemented on the objects are:
$$\underbrace{C(F) \equiv \frac{5}{9}(F-32)}_\text{Fahrenheit to Celcius}\ \ \ \text{and}\ \ \ \ \underbrace{\text{cm(inches)} \equiv 2.54 \times (\text{inches}-0}_\text{inches to cm})$$
These are very simple, straight-line functions, but they play an important role in modeling. 

Each conversion function can be written in the form $\line(x) \equiv m (x - x_0)$. Of course, if you multiply the $m$ through both terms in parentheses, you get $\line(x) = m x - m x_0$ which can be written even more simply as $mx + b$ by setting $b\equiv m x_0$. so the conversion function does conform to the straight-line formula we're used to. 

$m$ and $x_0$ are the ***parameters*** of the straight-line function. In terms of the graph of a straight-line function, $m$ is the slope and $x_0$ is the **x**-intercept respectively. 

Often, functions can be parameterized in other ways. For instance, you likely learned the parameterization $m x + b$, in which $m$ is (still) the slope of the graph but $b$ is the **y**-intercept. (Expanding out $m(x-x_0)$ and comparing to $m x + b$, you can see that $b = m x_0$.)

::: {.takenote  data-latex=""}
We can call $m(x - x_0)$ the "x-intercept parameterization" and $m x + b$ the "y-intercept parameterization. They are equivalent and equally good ways of parameterizing the straight line. There are still other ways of parameterizing, each style reflecting its own format for specifying the two points that make up a line. 
:::

## Scaling the input {#scaling-the-input}

To turn a pattern-book function into a ***basic modeling function*** all we do is use $\line()$ to convert the input *before* applying the pattern-book function.  `r mark(840)`

To illustrate the link between basic modeling functions and their pattern-book prototypes, Figure \@ref(fig:covid-scale) shows the model we fit to the COVID-19 data for the cumulative number of confirmed cases for each day in March: $\text{cases(day)} = e^{0.19(\text{day}- -32)}$

```{r covid-scale, echo=FALSE, fig.cap="A graph of the pattern-book exponential with an additional scale displayed (blue) to match it to the COVID-19 data"}
plot_scaled_input(exp(day) ~ day, domain(day=c(-1,11)), 1/0.19, 0.19*32) 
```

The function being drawn is simply $e^x$: a function from the pattern book. The black horizontal scale shows $x$, the input to the pattern-book function. Where does that value of $x$ come from? It's $0.19(\text{day} - -32)$, where day is the number of the day in March. For instance, on March 20, day$=10$ and $0.19*(\text{day}- -32) = 9.88$. You can see that 20 on the blue scale matches 10 on the black scale. The model says that on day 20 (blue scale) the input to the pattern-book function will be 9.88 (black scale). Plugging the input 9.88 into the pattern-book exponential gives $e^{9.88} = 19536 \approx 20,000$ cases. 

The pattern-book function does not give a good model of the COVID case numbers. But if we scale the input before applying the pattern-book function, we are effectively laying a new axis, the blue one in Figure \@ref(fig:covid-scale), that is stretched and shifted from the pattern-book input (blackscale). Using the blue axis lets us read off the number of cases as a function of the day in March. 

Input scaling empowers the pattern-book functions to model a huge variety of phenomena. There's just one exponential function and it always looks exactly the same. But there is a huge variety of ways to draw a blue axis, that is, to scale the input. With input scaling, the pattern-book function is tailored to become one of our basic modeling functions.
$$\underbrace{e^x}_\text{pattern-book function}\  \text{versus}\  \underbrace{e^{k(x-x_0)}}_\text{basic modeling function}$$
The straight-line function $\line()$ is being written here as $k(x-x_0)$ rather than $m(x-x_0)$. It's traditional when writing exponential functions to use the letter $k$ as the multiplier, but you can use whatever name you wish for a parameter. Knowing and using the idiom of mathematical notation will help you read and write mathematics more fluently.  `r mark(850)`

The table shows a of the mathematical idioms used in parameterizing the pattern-book functions. 

Function    | Written form | Parameter 1 | Parameter 2
------------|--------------|-------------|-------------
Exponential | $e^{kt}$     | $k$ "exponential constant"^[] | Not used
Exponential | $e^{t/\tau}$     | $\tau$ "time constant"^[] | Not used
Exponential | $2^{t/\tau_2}$     | $\tau_2$ "doubling time"^[$-\tau_2$ is sometimes called the "half life."] | Not used    
Power-law   | $[x - x_0]^p$    | $x_0$ x-intercept | exponent
Sinusoid    | $\sin\left(\frac{2 \pi}{P} (t-t_0)\right)$ | $P$ "period" | $t_0$ "time shift" 
Sinusoid | $\sin(\omega t + \phi)$ | $\omega$ "angular frequency" | $\phi$ "phase shift"
Sinusoid | $\sin(2 \pi \omega t + \phi)$ | $\omega$ "frequency" | $\phi$ "phase shift"
Gaussian     | dnorm(x, mean, sd) | "mean" (center)| sd "standard deviation"
Sigmoid  | pnorm(x, mean, sd) | "mean" (center) | sd "standard deviation"
Straight-line | $mx + b$ | $m$ "slope" | $b$ "y-intercept"
Straight-line | $m (x-x_0)$ | $m$ "slope" | $x_0$ "center"


## Scale the output

Just as the natural input usually needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function may need to be scaled before it presents a result suited for interpreting in the real world. 

```{r scaling-nature, echo=FALSE, out.width="100%", fig.cap="Natural **quantities** must be scaled to pure numbers before being suited to the pattern-book functions. The output from the function is a pure number which is scaled to the natural **quantity** of interest."}
knitr::include_graphics("www/scaling-nature.png")
```

The overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.  `r mark(860)`


Name        |  Pattern-book form | Modeling form
------------|-------------|------------------
exponential |  $e^x$      | $A e^{kx} + C$
sinusoid    | $\sin(x)$   | $A \sin\left(\frac{2 \pi}{P} (x-x_0)\right) + C$
proportional | $x$        | $a x + b$

::: {.takenote  data-latex=""}
The parameter $C$ is often called the ***baseline*** or the ***offset***. Statisticians call it the "intercept," because it plays the same role as $b$ in the straight-line function.

When working with sinusoids, parameter $A$ is called the ***amplitude***.

Of course, you're already familiar with $a$ and $b$: the slope and y-intercept of a straight line.

```{r show-sin-params, echo=FALSE, warning=FALSE, fig.cap="Baseline (blue), amplitude (red), and period (green) for the sinusoid."}
# making waveforms for the figure
slice_plot(sin(x) ~ x, domain(x=c(-1, 12))) %>%
  gf_hline(yintercept=0, color="dodgerblue") %>%
  gf_refine(theme_void()) %>%
  gf_errorbar(0 + 1 ~ pi/2, color="orange3") %>%
  gf_errorbar(-1 + 0 ~ 3*pi/2, color="orange3") +
  geom_errorbarh(aes(y=1/2, xmin=0, xmax=2*pi), color="green", size=.5)
```

```{r show-exp-baseline, echo=FALSE, warning=FALSE, fig.cap="The baseline for the exponential is the horizontal asymptote. There is no vertical asymptote."}
slice_plot(exp(x) ~ x, domain(x=c(-2, 2))) %>%
  gf_hline(yintercept=0, color = "dodgerblue") %>%
  gf_segment(0 + 0 ~ 0 + (-2), color="orange", arrow=arrow(ends="last", type="closed")) %>%

  gf_refine(theme_void())
```
:::

::: {.why  data-latex=""}
Our prefered form for the input scaling in the basic modeling functions is $m(x - x_0)$. Sometimes it can be a little hard to see this form given the conventions for parameterization. For instance, in the basic modeling sinusoid $$\sin\left(\underbrace{\frac{2 \pi}{P}}_m\ \ (x-x_0)\right)$$ the multiplier is written $\frac{2\pi}{P}$ rather than $m$. `r mark(863)`

In the basic modeling exponential and straight line functions $$A e^{kx} + C \ \ \ \text{and}\ \ \ a x + b$$
The $x_0$ does not appear. Why not?

Suppose we did write the exponential as $$A_2 e^{k(x-x_0)} + B$$ This could be simplified algebraically to $$A_2 e^{kx_0} e^{kx} + B$$ Although this looks a bit different, note that $e^{k x_0}$ is simply a number, not a function of $x$. We can therefore combine it with $A_2$, giving $A = A_2 e^{k x_0}$. The $A$ coefficient in the exponential makes any use of $x_0$ redundant and unnecessary. 

A similar algebra logic applies to the straight-line function $ax + b$. We don't need an $x_0$ there, either.
:::

## Exercises

`r insert_calcZ_exercise(7.2, "uKCIE", "Exercises/scale-input-1.Rmd")`

`r insert_calcZ_exercise(7.3, "BLECL", "Exercises/scale-input-2.Rmd")`

`r insert_calcZ_exercise(7.1, "MWLCS", "Exercises/fiducial-point.Rmd")`


`r insert_calcZ_exercise(7.5, "FKLEU", "Exercises/two-sines.Rmd")`

