--- 
title: "&nbsp;"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
biblio-style: apalike
link-citations: yes
description: "Tools for modeling"
---
```{r include=FALSE, cache=FALSE}
library(Zcalc) # New, catch-all package
library(Znotes)
library(here) # for file locations
library(thematic)
library(gridExtra) # for arranging plots.
library(kableExtra)

# theme the plots to match the document
thematic_rmd() #(bg="#8F8F8C", fg=NA, accent=NA)
ggplot2::theme_set(theme_bw(base_size = 16))


knitr::opts_chunk$set(out.width="90%", fig.align="center", fig.margin=TRUE,
                      collapse=TRUE # don't split output box from input.
                      )
#for regular pdf but not Tufte add:            fig.pos = "h", out.extra = "")

# Resolve the exercise number assigned to a permanent name like "3KEgLM"
# or "chicken-sit-table".
# See script in <_make_exercise_index.R>
if (file.exists("_exercise_cross_reference.csv")) {
  exercise_cross_reference <- readr::read_csv("_exercise_cross_reference.csv")
}

ref_ex <- function(perm_name) {
  # This is not yet implemented but will be based on a program that searches through all the
  # insertion commands in the chapters and assembles a table with exercise number, perm name, word-name. This
  # function will read that table and replace <perm_name> with the exercise number.
  res <- if (nchar(perm_name) < 10) { # It's a hash
      exercise_cross_reference %>% filter(hash==!!perm_name) %>% .$number
  } else {
    exercise_cross_reference %>% filter(wordname == !!perm_name) %>% .$number
  }

  if (is.null(res) || nchar(res) == 0 ) return("**MISSING EXERCISE NUMBER**")
  else return(res)
}


if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())

MC_counter <- Znotes::letter_counter()



sandbox_link <- function() {
  "[SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/)"
}

drill_link <- function() {
  "[DRILL QUESTIONS](https://maa-statprep.shinyapps.io/Zdrill/)"
}


Chaps <- list(
  parameters = 8,
  magnitudes = 15,
  dimension = 16.00,
  pattern_book_derivs = 19.00,
  fun_notation = 5.00,
  slope_function = 9.00,
  gradient = 24.00,
  local_approx = 25.00,
  iteration = 32.00,
  taylor = 26.00,
  optimization = 23.00,
  splines = 33.00,
  modeling_cycle = 14.00,
  expectation_value = 35.00,
  foobar = NA
)

Sections <- list(
  exp_curve_fitting = "8.003",
  gradient_vector = "24.003",
  functions_as_tables = "4.002",
  likelihood = "35.004"
)

under_construction <- function() {
  "::: {.underconstruction}
**Under Construction**

Content subject to revision.
:::
"
}

# For gradescope output
# askMC <- Znotes::askGS

```








<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(Zcalc) # New, catch-all package
library(Znotes)
library(here) # for file locations
library(thematic)
library(gridExtra) # for arranging plots.
library(kableExtra)

# theme the plots to match the document
thematic_rmd() #(bg="#8F8F8C", fg=NA, accent=NA)
ggplot2::theme_set(theme_bw(base_size = 16))


knitr::opts_chunk$set(out.width="90%", fig.align="center", fig.margin=TRUE,
                      collapse=TRUE # don't split output box from input.
                      )
#for regular pdf but not Tufte add:            fig.pos = "h", out.extra = "")

# Resolve the exercise number assigned to a permanent name like "3KEgLM"
# or "chicken-sit-table".
# See script in <_make_exercise_index.R>
if (file.exists("_exercise_cross_reference.csv")) {
  exercise_cross_reference <- readr::read_csv("_exercise_cross_reference.csv")
}

ref_ex <- function(perm_name) {
  # This is not yet implemented but will be based on a program that searches through all the
  # insertion commands in the chapters and assembles a table with exercise number, perm name, word-name. This
  # function will read that table and replace <perm_name> with the exercise number.
  res <- if (nchar(perm_name) < 10) { # It's a hash
      exercise_cross_reference %>% filter(hash==!!perm_name) %>% .$number
  } else {
    exercise_cross_reference %>% filter(wordname == !!perm_name) %>% .$number
  }

  if (is.null(res) || nchar(res) == 0 ) return("**MISSING EXERCISE NUMBER**")
  else return(res)
}


if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())

MC_counter <- Znotes::letter_counter()



sandbox_link <- function() {
  "[SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/)"
}

drill_link <- function() {
  "[DRILL QUESTIONS](https://maa-statprep.shinyapps.io/Zdrill/)"
}


Chaps <- list(
  parameters = 8,
  magnitudes = 15,
  dimension = 16.00,
  pattern_book_derivs = 19.00,
  fun_notation = 5.00,
  slope_function = 9.00,
  gradient = 24.00,
  local_approx = 25.00,
  iteration = 32.00,
  taylor = 26.00,
  optimization = 23.00,
  splines = 33.00,
  modeling_cycle = 14.00,
  expectation_value = 35.00,
  foobar = NA
)

Sections <- list(
  exp_curve_fitting = "8.003",
  gradient_vector = "24.003",
  functions_as_tables = "4.002",
  likelihood = "35.004"
)

under_construction <- function() {
  "::: {.underconstruction}
**Under Construction**

Content subject to revision.
:::
"
}

# For gradescope output
# askMC <- Znotes::askGS

```

***Mathematical modeling*** is a process where mathematics is used to understand, analyze, or predict a real-world situation. This process often occurs in cycles that revisit each of the following three stages: `r mark(200)`

1. Translate the real-world situation into a mathematical form (called a model)
2. Shape the model to answer the question at hand
3. Evaluate the model's reliability and agreement with the real-world situation

We'll discuss each stage of the modeling process in the following paragraphs and explore as a concrete example the way Sir Isaac Newton applied his theory of universal gravitation to the motion of planets.

The first stage of mathematical modeling involves **translating the real-world situation into a mathematical form**; the output or result of this stage is called a ***mathematical model***. **In this book** the objective of the translation phase will almost always be to choose or construct one or more  ***function***s that represent the relationships involved in the system being studied. You'll learn several strategies for turning what you know about the system into appropriate functions. And you'll learn some standard ***frameworks*** for organizing your ideas to help you determine how functions will be related, for instance Newton's framework relating position, velocity, and acceleration as rates of change of one another. `r mark(210)`

As a concrete example of this first stage, consider how Newton had to translate observations about planetary motion into a mathematical form. He was trying to make sense of the motion of planets in light of his novel theory that gravity is a universal force of attraction between *all* masses, rather than something peculiar to celestial bodies. He showed that the motions of planets, moons, and the Sun are governed by the same basic principles as an apple falling from a tree. To do this, he modeled force as a quantity that can be measured, simplified the complexity of the planets to mere masses (another measurable quantity), and supposed that position and motion were related.
<!-- [[Taken out. because it's unclear what is being said about velocity & acceleration in this context.]] ##: He assumed ***velocity***  (the rate of change of position with respect to time) and ***acceleration*** (the rate of change of velocity with respect to time). -->
His theory of the motion of planets was based on two fundamental mathematical models: First,
$$F = ma\ \ \ \text{(Newton's Second Law)}$$ which relates force, mass, and acceleration. Second, the
$$F = \frac{G m_1 m_2}{d^2}\ \ \ \text{(Law of Universal Gravitation)}$$
(sometimes called the inverse square law), which relates the mass of two objects ($m_1$ and $m_2$), and the distance between them to the force of gravitation. We'll get to the purpose of the $G$ in Chapter \@ref(dimensions), but for the moment, understand that this process of translating observations about planetary motion into a mathematical form is characteristic of the first stage of the modeling process.

The second stage of modeling involves doing mathematical work to **shape the model into a form that can directly answer the modeler's question of interest**. For instance, Newton knew from previous astronomical observations and Kepler's theorizing that planets' orbits are elliptical in shape that he needed to do some work on his two models in order to deduce the orbital shapes consistent with his Second Law and Universal Gravitation.

**In this book** you'll learn to use several key tools for doing mathematical work on functions. We might as well name them now: ***differentiation***, ***anti-differentiation***, ***optimization***, and ***zero-finding***. These tools derive directly from Newton's work on planetary motion, but thankfully, we are not constrained to using the technology of his day in their application. Instead, we will leverage modern computers to make each tool easier to use. Your job is to learn enough about them that you will know when, how, and why to use each tool.

The third and final stage of mathematical modeling often involves **evaluating the model**. The goal is to determine how reliable is the model's answer and to see if the consequences of the model are consistent with what happens in the real world. The evaluation often leads to a re-assessment of the model and a return to stage 1 to improve things. The resulting loop is called the ***modeling cycle***. For example, it seems likely that Newton did not start out knowing that gravitation follows an inverse square law. Perhaps he went through several ***modeling cycles*** until he found the form that was consistent with Kepler's elliptical orbits.

**In this book** you'll learn techniques for comparing models to data and for making predictions from models that can be compared to observations. You'll also see examples of when the evaluation of a function suggests revisions that might improve the model.


<!--chapter:end:modeling-preface.Rmd-->

```{r include=FALSE, cache=FALSE}
library(Zcalc) # New, catch-all package
library(Znotes)
library(here) # for file locations
library(thematic)
library(gridExtra) # for arranging plots.
library(kableExtra)

# theme the plots to match the document
thematic_rmd() #(bg="#8F8F8C", fg=NA, accent=NA)
ggplot2::theme_set(theme_bw(base_size = 16))


knitr::opts_chunk$set(out.width="90%", fig.align="center", fig.margin=TRUE,
                      collapse=TRUE # don't split output box from input.
                      )
#for regular pdf but not Tufte add:            fig.pos = "h", out.extra = "")

# Resolve the exercise number assigned to a permanent name like "3KEgLM"
# or "chicken-sit-table".
# See script in <_make_exercise_index.R>
if (file.exists("_exercise_cross_reference.csv")) {
  exercise_cross_reference <- readr::read_csv("_exercise_cross_reference.csv")
}

ref_ex <- function(perm_name) {
  # This is not yet implemented but will be based on a program that searches through all the
  # insertion commands in the chapters and assembles a table with exercise number, perm name, word-name. This
  # function will read that table and replace <perm_name> with the exercise number.
  res <- if (nchar(perm_name) < 10) { # It's a hash
      exercise_cross_reference %>% filter(hash==!!perm_name) %>% .$number
  } else {
    exercise_cross_reference %>% filter(wordname == !!perm_name) %>% .$number
  }

  if (is.null(res) || nchar(res) == 0 ) return("**MISSING EXERCISE NUMBER**")
  else return(res)
}


if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())

MC_counter <- Znotes::letter_counter()



sandbox_link <- function() {
  "[SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/)"
}

drill_link <- function() {
  "[DRILL QUESTIONS](https://maa-statprep.shinyapps.io/Zdrill/)"
}


Chaps <- list(
  parameters = 8,
  magnitudes = 15,
  dimension = 16.00,
  pattern_book_derivs = 19.00,
  fun_notation = 5.00,
  slope_function = 9.00,
  gradient = 24.00,
  local_approx = 25.00,
  iteration = 32.00,
  taylor = 26.00,
  optimization = 23.00,
  splines = 33.00,
  modeling_cycle = 14.00,
  expectation_value = 35.00,
  foobar = NA
)

Sections <- list(
  exp_curve_fitting = "8.003",
  gradient_vector = "24.003",
  functions_as_tables = "4.002",
  likelihood = "35.004"
)

under_construction <- function() {
  "::: {.underconstruction}
**Under Construction**

Content subject to revision.
:::
"
}

# For gradescope output
# askMC <- Znotes::askGS

```
# Parameters {#parameters}

<div style="float:right;">[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/modeling/01-parameters.Rmd)</div>

```{r include=FALSE}
book_file_name <- "modeling/01-parameters.html"
# Initialize skill file for the Block
cat("link, exercise, hash, file, skill\n", file="Skill_list.csv")
```

The pattern-book functions provide the modeler with a collection of shapes. They are not yet fully suited to represent real-world phenomena. To illustrate, consider Figure \@ref(fig:covid-exp) which shows the number of officially confirmed COVID-19 cases in the US in March 2020.

The case count versus time in the COVID pandemic was widely and appropriately described as "exponential." So it seems appropriate  alongside the data Figure \@ref(fig:covid-exp) shows the function $\text{cases}(t) \equiv e^t$ plotted as a $\color{magenta}{\text{magenta}}$ curve.

```{r covid-exp, echo=FALSE, fig.cap="Cumulative officially confirmed COVID-19 cases during the month of March, 2020. The red curve is $e^t$", warning=FALSE}
March <- Covid_US %>% filter(lubridate::month(date)==3, 
                             lubridate::year(date)==2020) %>%
  mutate(day = lubridate::mday(date))
Pcases <- gf_point(confirmed ~ day, 
         data = March) %>%
  gf_labs(y = "Cumulative confirmed cases", x = "Day in March, 2020") %>%
  gf_lims(y=c(0,200000)) 
Pcases %>%
  slice_plot(exp(t) ~ x, domain(t=c(0,15)), 
             color="magenta", label_text = "exp(t)\n\n",
             label_x=0.6, label_vjust = "left", size=1) 
```

There's an obvious mismatch between the data and the function $e^t$. Does this mean the COVID pattern is not exponential?

For the pattern book functions, the input is always a ***pure number***. For instance, we read $t=10$ off the horizontal axis at which point $e^t {\left.\Large\right|}_{t=10}  \approx 22000$, consistent with the curve shown in the graph. 

But the place on the horizontal axis marked 10 does not correspond to the number 10. Rather, that place stands for "10 days," a quantity with units. Perhaps surprisingly, there is no such thing as $e^{10\,\text{days}}$. The reason for this will be detailed in Chapter \@ref(dimensions-and-units), but for now we'll simply point out that the domain of $\exp()$ is the set of real numbers, and real numbers don't have units. 

If we want the input to $\text{cases}(t)$ to be denominated in days, we'll have to convert $t$ to a pure pure number (e.g. 10, not "10 days") *before* the quantity is handed off as the argument to $\exp()$. We do this by introducing a ***parameter*** when we use the exponential function for describing relationships between quantities. The standard from for this is $e^{kt}$ as opposed to $e^t$. The $k$ parameter will be a quantity with units of "per-day." Suppose we set $k=0.2 per day$. Then $k t{\LARGE\left.\right|}_{t=10 days} = 2$, a pure number:
$$0.2\, \text{day}^{-1} \cdot 10\, \text{days} = 0.2\ .$$
The use of a parameter like $k$ does more than handle the formality of converting input quantities into pure numbers. Having a choice for $k$ allows us to stretch or compress the function to align with the data. Figure \@ref(fig:covid-exp2) plots the modeling version of the exponential function to the COVID-case data:

```{r covid-exp2, echo=FALSE, fig.cap="Using the function form $A e^{kt}$, with parameters $k=0.19$ per day and $A = 573$ cases, matches the COVID-case data well."}
Pcases %>%
  slice_plot(573*exp(0.19*t) ~ t, interval(t=0:31), color="blue", label_x=0.5,
             label_text="A exp(k t)\nA = 593 cases\nk=0.19 per day\n", label_vjust = "left")
```

This chapter will introduce new functions, based on the pattern-book functions, but that have parameters so that the inputs and outputs can be **quantities** rather than pure numbers. 

## Parallel scales

At the heart of how we're going to use the pattern-book functions to model the relationship between quantities is the idea of conversion between one scale and another.  Consider these everyday objects: a thermometer and a ruler.

![](www/thermometer.png)       ![](www/ruler.jpeg)

Each object presents a read-out of what's being measured---temperature or length---on two different scales. At the same time, the objects provide a way to convert one scale to another.

A function gives the output for any given input. We represent the input value as a position on a number line---which we call an "axis"---and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output. 

We can translate the correspondance between one scale and the other into the form of a straight-line function. For instance, if we know the temperature in Fahrenheit ($^\circ$F) and want to convert it to Celsius ($^\circ C$) we have the following function:
$$C(F) \equiv {\small\frac{5}{9}}(F-32)\ .$$
Similarly, converting inches to centimeters  can be accomplished with
$$\text{cm(inches)} \equiv 2.54 \, (\text{inches}-0)\ .$$
Both of these scale conversion functions have the form of the straight-line function, which can be written as 
$$f(x) \equiv a x + b\ \ \ \text{or, equivalently as}\ \ \ \ f(x) \equiv a(x-x_0)\ ,$$ where $a$, $b$, and $x_0$ are ***parameters***.

In Section \@ref(input-scaling), we'll use the $ax + b$ form of scale conversion, but we could equally well have used $a(x-x_0)$.

In Section \@ref(output-scaling) we'll introduce a second scale conversion function, still in the form of a straight-line function: $A x + B$. The use of the lower-case parameter names ($a$, $b$) versus the upper-case parameter names ($A$, $B$) will help us distinguish the two different uses for scale conversion, namely ***input scaling*** versus ***output scaling***.

## Input scaling {#input-scaling}

Figure \@ref(fig:tides-RI1) is based on the data frame `RI-tide` which is a minute-by-minute record of the tide level in Providence, Rhode Island (USA) for the period April 1 to 5, 2010. The `level` variable is measured in meters; the `hour` variable gives the time of the measurement in hours after midnight at the start of April 1.

```{r tides-RI1, echo=FALSE, fig.cap="Tide levels oscillate up and down over time. This is analogous to the $\\sin(t)$ pattern-book function."}
Pa <- gf_line(level ~ hour, data = RI_tide, color="blue") %>%
  gf_labs(subtitle="Tide level", x="time (hours)", y="level (meters)") %>%
  gf_theme(axis.text.x = element_text(colour = "blue"),
           panel.grid.minor = element_line(color = "blue", size=0.1),
           panel.grid.major = element_line(color="blue", size=0.1),
           axis.text.y = element_text(color = "magenta"),
           axis.title.y = element_text(color = "magenta"),
           axis.title.x = element_text(color = "blue"),
           title = element_text(color="blue")
           ) %>%
  gf_theme(scale_x_continuous(breaks=seq(0, 110, by=10)))
Pb <- slice_plot(sin(t) ~ t, domain(t=0:(17*pi)), npts=500) %>%
  gf_labs(subtitle=latex2exp::TeX("$sin(t)$ from the pattern book")) %>%
  gf_theme(scale_x_continuous(breaks=seq(0, 55, by=5))) %>%
  gf_lims(y=c(-1.5, 1.5))
gridExtra::grid.arrange(Pa, Pb, ncol=1)
```
The pattern-book $\sin()$ and the function $\color{magenta}{\text{level}}\color{blue}{(hour)}$ have similar shapes, so it seems reasonable to model the tide data as a sinusoid. However, the scale of the axes is different on the two graphs.

To model the tide with a sinusoid, we need to modify the sinusoid to change the scale of the input and output. First, let's look at how to accomplish the ***input scaling***. Specifically, we want the pure-number input $t$ to the sinusoid be a function of the quantity $hour$. Our framework for this re-scaling is the straight-line function. We will replace the pattern-book input $t$ with a function $$t(\color{blue}{hour}) \equiv a\, \color{blue}{hour} + b\ .$$

The challenge is to find values for the parameters $a$ and $b$ that will transform the $\color{blue}{\mathbf{\text{blue}}}$ horizontal axis into the **black** horizontal axis, like this:
```{r echo=FALSE}
scale_shift(0, 53, r=2, x0=-1, nticks=10) %>%
  gf_vline(xintercept=~ 4) %>%
  gf_vline(xintercept=~49)
```
By comparing the two axes, we can estimate that $\color{blue}{10} \rightarrow 4$ and $\color{blue}{100} \rightarrow 49$. With these two coordinate points, we can find the straight-line function that turns $\color{blue}{\mathbf{\text{blue}}}$ into **black** by plotting the coordinate pairs $(\color{blue}{0},1)$ and $(\color{blue}{100}, 51)$ and finding the straight-line function that connects the points.

```{r blue-to-black-1, echo=FALSE}
Pts <- tibble(x=c(10, 100), y=c(4, 49), text=c("(10,4)","(100,49)"))
gf_point(y ~ x, data = Pts) %>%
  gf_vline(xintercept = ~ x, color="blue", alpha=0.5) %>%
  gf_hline(yintercept = ~ y, color="black", alpha=0.5) %>%
  gf_theme(scale_x_continuous(limits=c(-20,110),
                              breaks=seq(-20, 110, by=10))) %>%
  gf_theme(scale_y_continuous(limits=c(-10, 60),
                              breaks=seq(-10, 60, by=5))
  ) %>%
  gf_labs(x = "time (hours)", y="t (pure number)") %>%
  gf_theme(axis.text.x = element_text(colour = "blue"),
           axis.title.x = element_text(color = "blue")
           ) %>%
  gf_text(y ~ x, label=~text, hjust=1, vjust=-.5) %>%
  slice_plot(-1 + .5*hour ~ hour, domain(hour=c(-20,110)),
             size=3, alpha=.4) %>%
  gf_refine(coord_fixed())
```
You can calculate for yourself that the function that relates $\color{blue}{\mathbf{\text{blue}}}$ to **black** is $$t(\color{blue}{time}) = \underbrace{\frac{1}{2}}_a \color{blue}{time}  \underbrace{-1\LARGE\strut}_b$$

Replacing the pure number $t$ as the input to pattern-book $\sin(t)$ with the transformed $\frac{1}{2} \color{blue}{time} 1 1$ we get a new function:
$$g(\color{blue}{time}) \equiv \sin\left(\strut {\small\frac{1}{2}}\color{blue}{time} - 1\right)\ .$$
Figure \@ref(fig:tides-RI2) plots $g()$ along with the actual tide data.
library(gtable)
g2 <- ggplotGrob(p2)
g3 <- ggplotGrob(p3)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)
```{r tides-RI2, echo=FALSE}
g1 <- ggplotGrob(Pa)
Pb2 <- slice_plot(sin(time/2 - 1) ~ time,
                  domain(time=0:107), npts=500) %>%
  gf_labs(x="time (hours)",
          y=latex2exp::TeX("$g(time)$")) %>%
  gf_theme(scale_x_continuous(limits=c(0, 110),
                              breaks=seq(0, 110, by=10))) %>% 
  gf_lims(y=c(-1.5,1.5)) %>%
  ggplotGrob()
g <- rbind(g1, Pb2, size="first")
g$widths <- grid::unit.pmin(g1$widths, Pb2$widths)

grid::grid.newpage()
grid::grid.draw(g)
 
#gridExtra::grid.arrange(Pa, Pb2,widths=c(.95, 1), ncol=1)
```

## Output scaling {#output-scaling}

Just as the natural input needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function needs to be scaled before it presents a result suited for interpreting in the real world. 

```{r scaling-nature, echo=FALSE, out.width="100%", fig.cap="Natural **quantities** must be scaled to pure numbers before being suited to the pattern-book functions. The output from the pattern-book function is a pure number which is scaled to the natural **quantity** of interest."}
knitr::include_graphics("www/scaling-nature.png")
```

The overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.

Let's return to Figure \@ref(fig:tides-RI2) which shows that the function $g(\color{blue}{time})$, which scales the input to the pattern-book sinusoid, has a much better alignment to the tide data. Still, the vertical axes of the two graphs in the figure are not the same.

This is the job for ***output scaling***, which takes the output of $g(\color{blue}{time})$ (bottom graph) and scales it to match the $\color{magenta}{level}$ axis on the top graph. That is, we seek to align the **black** vertical scale with the $\color{magenta}{\mathbf{\text{magenta}}}$ vertical scale.
To do this, we note that the range of the $g(\color{blue}{time})$ is -1 to 1, whereas the range of the tide-level is about 0.5 to 1.5. The output scaling will take the straight-line form
$$\color{magenta}{\text{level}}(\color{blue}{time}) = A\, g(\color{blue}{time}) + B$$
or, in graphical terms
```{r echo=FALSE, warning=FALSE, message=FALSE}
scale_shift(-1.5, 1.75, r=.5, x0=-2, color="magenta", nticks=10) %>%
  gf_vline(xintercept=~ 1) %>%
  gf_vline(xintercept=~-1) %>%
  gf_refine(coord_flip())
```

We can figure out parameters $A$ and $B$ by finding the straight-line function that connects the coordinate pairs $(-1, \color{magenta}{0.5})$ and $(1, \color{magenta}{1.5})$ as in Figure \@ref(fig:black-to-magenta-1).

```{r black-to-magenta-1, echo=FALSE, fig.cap="Finding the straight-line function that converts $-1 \\rightarrow \\color{magenta}{0.5}$ and converts $1 \\rightarrow \\color{magenta}{1.5}$"}
Pts <- tibble(x=c(-1, 1), y=c(0.5, 1.5), text=c("(-1,0.5)","(+1,1.5)"))
gf_point(y ~ x, data = Pts) %>%
  gf_vline(xintercept = ~ x, color="black", alpha=0.5) %>%
  gf_hline(yintercept = ~ y, color="magenta", alpha=0.5) %>%
  gf_theme(scale_x_continuous(limits=c(-1.5,1.5),
                              breaks=seq(-1.5, 1.5, by=0.5))) %>%
  gf_theme(scale_y_continuous(limits=c(0, 2),
                              breaks=seq(0, 2, by=.5))
  ) %>%
  gf_labs(x = "g(time)", y="level(g(time))") %>%
  gf_theme(axis.text.y = element_text(colour = "magenta"),
           axis.title.y = element_text(color = "magenta")
           ) %>%
  gf_text(y ~ x, label=~text, hjust=1, vjust=-.5) %>%
  slice_plot(1 + .5*g ~ g, domain(g=c(-1.5,1.5)),
             size=3, alpha=.4) %>%
  gf_refine(coord_fixed())
```
You can confirm for yourself that the function that does the job is $$\color{magenta}{\text{level}} = 0.5 g(\color{blue}{time}) + 1\ .$$


Putting everything together, that is, scaling both the input to pattern-book $\sin()$ and the output from pattern-book $\sin()$, we get

$$\color{magenta}{\text{level}}(\color{blue}{time}) = \underbrace{0.5}_A \sin\left(\underbrace{\small\frac{1}{2}}_a \color{blue}{time}  \underbrace{-1}_b\right) + \underbrace{1}_B$$

## A procedure for building models

We've been using pattern-book functions as the intermediaries between input scaling and output scaling, using this format.

$$f(x) \equiv A e^{ax + b} + B\ .$$
We can use the other pattern-book functions---the gaussian, the sigmoid, the logarithm, the power-law functions---in exactly the same way. That is, the basic framework for modeling is this:

$$\text{model}(x) \equiv A\, {g_{pattern\_book}}(ax + b) + B\ ,$$ where $g_{pattern\_book}()$ is one of the pattern-book functions. To construct a basic model, you task has two parts:

1. Pick the specific pattern-book function whose shape resembles that of the relationship you are trying to model. For instance, we picked $e^x$ for modeling COVID cases versus time (at the start of the pandemic). We picked $\sin(x)$ for modeling tide levels versus time.

2. Find numerical values for the parameters $A$, $B$, $a$, and $b$. In Chapter \@ref(fitting-by-eye) we'll show you some ways to make this part of the task easier.

It's remarkable that models of a very wide range of real-world relationships between pairs of quantities can be constructed by picking one of a handful of functions, then scaling the input and the output. As we move on to other Blocks in *MOSAIC Calculus*, you'll see how to generalize this to potentially complicated relationships among more than two quantities. That's a big part of the reason you're studying calculus.

## Other formats for scaling

Often, modelers choose to use input scaling in the form $a (x - x_0)$ rather than $a x + b$. The two are completely equivalent when $x_0 = - b/a$. The choice between the two forms is largely a matter of convention. But almost always the output scaling is written in the format $A y + B$. 

::: {.example data-latex=""}
For the COVID case-number data shown in Figure \@ref(fig:covid-exp2), we found that a reasonable match to the data can be had by input- and output-scaling the exponential: $$\text{cases}(t) \equiv  \underbrace{573}_A e^{\underbrace{0.19}_a\ t}\ .$$

You might wonder why the parameters $B$ and $b$ aren't included in the model. One reason is that cases and the exponential function already have the same range: zero and upwards. So there's no need to shift the output with a parameter B. 

Another reason has to do with the algebraic properties of the exponential function. Specifically, 
$$e^{a x + b}= e^b e^{ax} = {\cal A} e^{ax}$$ where ${\cal A} \equiv e^b$.

In the case of exponentials, writing the input scaling in the form $e^{a(x-x_0)}$ can provide additional insight. 

A bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that
$$A e^{at} = e^{\log(A)} e^{at} = e^{a t + \log(A)} = e^{a\left(\strut t + \log(A)/a\right)} = e^{a(t-t_0)}\ ,$$
where $$t_0 = - \log(A)/a = - \log(593)/0.19 = -33.6\ .$$
You can interpret $t_0$ as the starting point of the pandemic. When $t = t_0$, the model output is $e^{k 0} = 1$: the first case. According to the parameters we matched to the data for March, the pandemic's first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It's remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct.
:::

## Parameterization idioms

English has many *idioms*, phrases to express an idea to those in the know, but will be confusing to others to take the phrase literally. Examples of such idioms (many of which come from Shakespeare's plays):

- break the ice
- it's Greek to me
- elbow room
- beat around the bush
- break a leg

Part of the process of learning a language is gaining familiarity with idioms.

The same is true for mathematics as it's spoken in different disciplines. Such idioms appear in the way input scaling is parameterized and the names used for the parameters. Often, the idiomatic parameterization is intended to make it easy to give a value to a parameter from easy-to-make observations. Knowing and using the idiom of mathematical notation will help you read and write mathematics more fluently. 

Here are some input-scaling parameterizations that are used in practice.

Function    | Written form | Parameter 1 | Parameter 2
------------|--------------|-------------|-------------
Exponential | $e^{kt}$     | $k$ "exponential constant"^[] | Not used
Exponential | $e^{t/\tau}$     | $\tau$ "time constant"^[] | Not used
Exponential | $2^{t/\tau_2}$     | $\tau_2$ "doubling time"^[$-\tau_2$ is sometimes called the "half life."] | Not used    
Power-law   | $[x - x_0]^p$    | $x_0$ x-intercept | exponent
Sinusoid    | $\sin\left(\frac{2 \pi}{P} (t-t_0)\right)$ | $P$ "period" | $t_0$ "time shift" 
Sinusoid | $\sin(\omega t + \phi)$ | $\omega$ "angular frequency" | $\phi$ "phase shift"
Sinusoid | $\sin(2 \pi \omega t + \phi)$ | $\omega$ "frequency" | $\phi$ "phase shift"
Gaussian     | dnorm(x, mean, sd) | "mean" (center)| sd "standard deviation"
Sigmoid  | pnorm(x, mean, sd) | "mean" (center) | sd "standard deviation"
Straight-line | $mx + b$ | $m$ "slope" | $b$ "y-intercept"
Straight-line | $m (x-x_0)$ | $m$ "slope" | $x_0$ "center"


## Exercises

`r insert_calcZ_exercise(7.2, "uKCIE", "Exercises/scale-input-1.Rmd")`

`r insert_calcZ_exercise(7.3, "BLECL", "Exercises/scale-input-2.Rmd")`

`r insert_calcZ_exercise(7.1, "MWLCS", "Exercises/fiducial-point.Rmd")`


`r insert_calcZ_exercise(7.5, "FKLEU", "Exercises/two-sines.Rmd")`


`r insert_calcZ_exercise("XX.XX", "amCjZG", "Exercises/walnut-run-dish.Rmd",skill="input & output scaling")`

`r insert_calcZ_exercise("XX.XX", "2iItLg", "Exercises/boy-put-sofa.Rmd", skill= "Input and output scaling")`

<!--chapter:end:01-parameters.Rmd-->

```{r include=FALSE, cache=FALSE}
library(Zcalc) # New, catch-all package
library(Znotes)
library(here) # for file locations
library(thematic)
library(gridExtra) # for arranging plots.
library(kableExtra)

# theme the plots to match the document
thematic_rmd() #(bg="#8F8F8C", fg=NA, accent=NA)
ggplot2::theme_set(theme_bw(base_size = 16))


knitr::opts_chunk$set(out.width="90%", fig.align="center", fig.margin=TRUE,
                      collapse=TRUE # don't split output box from input.
                      )
#for regular pdf but not Tufte add:            fig.pos = "h", out.extra = "")

# Resolve the exercise number assigned to a permanent name like "3KEgLM"
# or "chicken-sit-table".
# See script in <_make_exercise_index.R>
if (file.exists("_exercise_cross_reference.csv")) {
  exercise_cross_reference <- readr::read_csv("_exercise_cross_reference.csv")
}

ref_ex <- function(perm_name) {
  # This is not yet implemented but will be based on a program that searches through all the
  # insertion commands in the chapters and assembles a table with exercise number, perm name, word-name. This
  # function will read that table and replace <perm_name> with the exercise number.
  res <- if (nchar(perm_name) < 10) { # It's a hash
      exercise_cross_reference %>% filter(hash==!!perm_name) %>% .$number
  } else {
    exercise_cross_reference %>% filter(wordname == !!perm_name) %>% .$number
  }

  if (is.null(res) || nchar(res) == 0 ) return("**MISSING EXERCISE NUMBER**")
  else return(res)
}


if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())

MC_counter <- Znotes::letter_counter()



sandbox_link <- function() {
  "[SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/)"
}

drill_link <- function() {
  "[DRILL QUESTIONS](https://maa-statprep.shinyapps.io/Zdrill/)"
}


Chaps <- list(
  parameters = 8,
  magnitudes = 15,
  dimension = 16.00,
  pattern_book_derivs = 19.00,
  fun_notation = 5.00,
  slope_function = 9.00,
  gradient = 24.00,
  local_approx = 25.00,
  iteration = 32.00,
  taylor = 26.00,
  optimization = 23.00,
  splines = 33.00,
  modeling_cycle = 14.00,
  expectation_value = 35.00,
  foobar = NA
)

Sections <- list(
  exp_curve_fitting = "8.003",
  gradient_vector = "24.003",
  functions_as_tables = "4.002",
  likelihood = "35.004"
)

under_construction <- function() {
  "::: {.underconstruction}
**Under Construction**

Content subject to revision.
:::
"
}

# For gradescope output
# askMC <- Znotes::askGS

```
# Assembling functions {#assembling}

<div style="float:right;">[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/modeling/02-assembling-functions.Rmd)</div>

```{r include=FALSE}
book_file_name <- "modeling/02-assembling-functions.html"
```

When we need a new function for some purpose, we practically always build it out of existing functions. For instance, a  parameterized function like $$f(x) \equiv A \sin\left(\frac{2 \pi}{P}x\right) + B$$ is built by assempling together a straight-line input scaling, a pattern-book $\sin()$ function, and another straight-line function for scaling the output from $\sin()$. This is an example of ***function composition*** where functions are "layered," the output of one function being given as the input to another. For instance,
$f(x)$ is put together from three composed functions, $\text{output}()$, $\sin()$, $\text{input}()$ where $$f(x) = \text{output}(sin(\text{input}(x))$$ where 
$$\text{output}(x) \equiv Ax + B \ \ \ \text{and}\ \ \ \text{input}(x) \equiv a x + b$$


In this chapter, we'll review three general frameworks for combining functions: linear combination, composition, and multiplication. You have almost certainly seen all three of these frameworks in your previous mathematical studies, although you might not have known that they have names.

## Linear combination 

One of the most widely used sorts of combination is called a ***linear combination***. The mathematics of linear combination is, it happens, at the core of the use of math in applications, whether that be constructing a Google-like search engine or analyzing medical data to see if a treatment has a positive effect.

You've worked for many years with one kind of linear combination: polynomials. No doubt you've seen functions^[It's likely that you saw polynomials as things to be factored, rather than as functions taking an input and producing an output. So they were written as *equations*: $3 x^2 + 5x - 2 = 0$.] like $$f(x) \equiv 3 x^2 + 5 x - 2$$

There are three pattern-book functions in this polynomial. In  polynomials the functions being combined are all power-law functions: $g_0(x) \equiv 1$, $g_1(x) \equiv x$, and $g_2(x) \equiv x^2$. With these functions defined, we can write the polynomial $f(x)$ as $$f(x) \equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)$$
Each of the functions is being scaled by a quantity---3, 5, and -2 in this example---and the scaled functions are added up. That's a linear combination; scale and add. (Later, we'll see that the ***scalars*** generally come with units. So we might well have a metric polynomial and an equivalent traditional-unit polynomial. Just wait.)


There are other places where you have seen linear combinations: 

- The parameterized ***sinusoid***  $$A \sin\left(\frac{2 \pi}{P}t\right) + B$$ is a linear combination of the functions $h_1(t) \equiv \sin\left(\frac{2 \pi}{P} t\right)$ and $h_2(t) \equiv 1$. The linear combination is $A\,h_1(t) + B\, h_2(t)$.
- The parameterized ***exponential*** $$A e^{kt} + B$$ The functions being combined are $e^{kt}$ and $1$. The scalars are, again, $A$ and $C$.
- The straight line function, such as $\mbox{output}(x) \equiv A x + B$ and  $\mbox{input}(x) \equiv a x + b$. The functions being combined are $x$ and $1$, the scalars are $a$ and $b$.  

Note that neither the parameterized exponential or the parameterized sinusoid is a polynomial simply because it is not constructed exclusively from monomials.

There are a few reasons for us to be introducing linear combinations here.

1. You will see linear combinations everywhere once you know to look for them.
2. There is a highly refined mathematical theory of linear combinations that gives us powerful ways to think about them as well as computer software that can quickly find the best scalars to use to match input-output data.
3. The concept of linear combination generalizes the simple idea that we have been calling "scaling the output." From now on, we'll use the linear-combination terminology and avoid the narrower idea of "scaling the output."
4. Many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule or a helicopter in flight or a building shaken by an earthquake are described in terms of simple "modes" which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones.
5. Many modeling tasks can be put into the framework of choosing an appropriate set of simple functions to combine and then figuring out the best scalars to use in the combination. (Generally, the computer does the figuring.)


## Function composition {#function-composition}

To ***compose*** two functions, $f(x)$ and $g(x)$, means to apply one of the functions to the output of the other. "$f()$ composed with $g()$" means $f(g(x))$. This is generally very different from "$g()$ composed with $f()$" which means $g(f(x))$.

For instance, suppose you have recorded the outdoor temperature over the course of a day and packaged this into a function $\text{AirTemp}(t)$: temperature as a function of time $t$. Your digital thermometer uses degrees Celsius, but you want the output units to be degrees Kelvin. The conversion function is $$\text{CtoK}(C) \equiv C + 273.15$$
Notice that CtoK() takes temperature in $^\circ C$ as input. With this, we can write the "Kelvin as a function of time" as $$\text{CtoK}\left(\text{AirTemp}(t)\right)$$

It's important to distinguish the above time $\rightarrow$ Kelvin function from something that looks very much the same but is utterly different: $\text{AirTemp}\left(\text{CtoK}(C)\right)$. In the first, the input is time. In the second, it is temperature in celsius.


Here is a model of the length of daylight (in hours) as a function of latitude $L$ and the declination angle $\delta$ of the sun. 

$$\text{daylight}(L, \delta) \equiv {\small\frac{2}{15}} \arccos\left(-\tan(L)*\tan(\delta)\right)$$
The declination angle is the latitude of the point on the earth's surface pierced by an imagined line connecting the centers of the earth and the sun. On the summer solstice, the longest day of the year, it is $23.44^\circ$. On $day$, where midnight before January 1 is $day=0$ and the end of December 31 is $day=365.25$, the declination is
$$\delta(day) = 23.44 \sin\left({\small\frac{2\pi}{365.25}} (day-9)\right)\ ,$$
a composition of $\sin()$ with the straight-line function ${\small\frac{2\pi}{365.25}} (day - 9)$.

Composing day_length$(L, \delta)$ onto $\delta(day)$ gives the length of daylight as a function of day of the year:
$$\text{light}(L, day) \equiv {\small\frac{2}{15}} \arccos\left(-\tan(L)*\tan(\delta(day))\right)\ .$$
Function composition enables us to transform a function that takes one kind of thing as input (say, declination) and turn it into a function that takes another kind of thing as input (say, day of the year).



::: {.intheworld  data-latex=""}
Income inequality is a matter of perennial political debate. In the US, most people support Social Security, which is an income re-distribution programming dating back almost a century. But other re-distribution policies are controversial. Some believe they are essential to a healthy society, others that the "cure" is worse than the "disease." 

Whatever one's views, it's helpful to have a way to quantify inequality. There are many ways that this might be done. A mathematically sophisticated one is called the ***Gini coefficient***.

Imagine that society was divided statistically into income groups, from poorest to richest. Each of these income groups consists of a fraction of the population and has, in aggregate, a fraction of the national income. Poor people tend to be many in number but to have a very small fraction of income. Wealthy people are few in number, but have a large fraction of income. The table shows data for US households in 2009:^[These data, as well as the general idea for the topic come from La Haye and Zizler (2021), "The Lorenz Curve in the Classroom", *The American Statistician*, 75(2):217-225]

group label  | population | aggregate income | cumulative income | cumulative pop.
:------------|-----------:|-----------------:|:-------
poorest      | 20%   | 3.4%  | 3.4%         | 20%
low-middle   | 20%   | 8.6%  | 12.0%        | 40%
middle       | 20%   | 14.6% | 26.6%        | 60%
high-middle  | 20%   | 23.2% | 47.8%        | 80%
richest      | 20%   | 50.2% | 100.0%       | 100%


The ***cumulative*** income shows the fraction of income of all the people in that group or poorer.  The cumulative population adds up the population fraction in that row and previous rows. So, a cumulative population of 60% means "the poorest 60% of the population" which, as the table shows, earn as a group 14.6% of the total income for the whole population.

A function that relates the cumulative population to the cumulative income is called a ***Lorenz function***. The data are graphed in Figure \@ref(fig:lorenz-data) and available as the `US_income` data frame in the `r sandbox_link()`. Later, in Figure \@ref(fig:lorenz-1-fun), we'll fit parameterized functions to the data.

```{r lorenz-data, echo=FALSE, fig.cap="Data on household incomes in the US in 2009."}
Income <- tibble::tribble(
  ~ income, ~ pop,
  0,     0,
  3.4,  20,
  12.0, 40, 
  26.6, 60,
  47.8, 80,
  100, 100
) %>%
  mutate(income=income/100, pop=pop/100)
P <- gf_point(income ~ pop, data = Income) %>%
  gf_labs(x="Population percentile (p)", y="Cumulative income percentile (L)") %>%
  gf_refine(coord_fixed())
P
```
Lorenz curves must:

- Be concave up, which amounts to saying that the curve gets steeper and steeper as the population percentile increases. (Why? Because at any point, poorer people are to the left and richer to the right.) 
- Connect (0,0) to (1, 1). 

Calling the income percentile $L$ a function of the population percentile $p$, a Lorenz function is $L(p)$ that satisfies the requirements in the previous paragraph.
Here are some functions that meet the requirements:

- $L_b(p) \equiv p^b$ where $1 \leq b$.
- $L_q(p) \equiv 1 - (1-p)^q$ where $0 < q \leq 1$

Notice that each of these functions has just one parameter. It seems implausible that the workings of a complex society can be summarized with just one number. We can use the curve-polishing techniques that will be introduced in Section \@ref(modeling-cycle) to find the "best" parameter value to match the data.  `r mark(1340)`

```{r}
Lb <- fitModel(income ~ pop^b, data = Income, start=list(b=1.5))
Lq <- fitModel(income ~ 1 - (1-pop)^q, data = Income, start=list(q=0.5))
```

Figure \@ref(fig:lorenz-1-fun) compares the polished functions to the data.
```{r lorenz-1-fun, echo=FALSE, fig.cap="Lorenz curves $L_b(p)$ (blue) and $L_q(p)$ (magenta) fitted to the household income data."}
P %>%
  slice_plot(Lb(pop) ~ pop, color="dodgerblue", npts=500) %>%
  slice_plot(Lq(pop) ~ pop, color="orange3", npts=500) %>%
  slice_plot(.55*Lb(pop) + .45*Lq(pop) ~ pop, color="magenta")
```
Neither form $L_b(p)$ or $L_q(p)$ gives a compelling description of the data. Where should we go from here?

We can provide more parameters by constructing more complicated Lorenz functions. Here are two ways to build a new Lorenz function out of an existing one:

- The product of any two Lorenz functions, $L_1(p) L_2(p)$ is itself a Lorenz function.
- A linear combination of any two Lorenz functions, $a L_1(p) + (1-a) L_2(p)$, so long as the scalars add up to 1, is itself a Lorenz function. For instance, the magenta curve in Figure \@ref(fig:lorenz-1-fun) is the linear combination of 0.45 times the tan curve plus 0.55 times the blue curve.

Question: Is the composition of two Lorenz functions a Lorenz function? That is, does the composition meet the two requirements for being a Lorenz function?

To get started, figure out whether or not $L_1(L_2(0)) = 0$ and $L_1(L_2(1)) = 1$. If the answer is yes, then we need to find a way to compute the concavity of a Lorenz function to determine if the composition will always be concave up. We'll need additional tools for this. We'll introduce these in Block 2.

:::





## Function multiplication

The third in our repertoire of methods for making new function out of old is plain old multiplication. With two functions $f(x)$ and $g(x)$, the product is simply $f(x)g(x)$.

It's essential to distinguish between function multiplication and function composition:

$$\underbrace{f(x) g(x)}_\text{multiplication}\ \ \ \ \underbrace{f(g(x)) \ \ \text{or}\ \ \ g(f(x))}_\text{composition}$$

In function composition, only one of the functions---the ***interior function*** is applied to the overall input, $x$ in the above example. The ***exterior function*** is fed its input from the output of the interior function. 

In multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.

In function composition, the order of the functions matters: $f(g(x))$ and $g(f(x))$ are in general completely different functions.

In function multiplication, the order doesn't matter because multiplication is ***commutative***, that is, if $f()$ and $g()$ are the functions to be multiplied $f(x) \times g(x) = g(x)\times f(x)$. 


::: {.example data-latex=""}
***Transient vibration***

A guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string. 

After plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.

Function multiplication is used so often in modeling that you'll see it in many modeling situations. Here's one example that is important in physics and communication: the ***wave packet***. Overall, the wave packet is a localized oscillation as in Figure \@ref(fig:wave-packet). 

```{r wave-packet, echo=FALSE, fig.cap="A *wave packet* constructed by multiplying a sinusoid and a gaussian function."}
wave_packet <- function(x, A=1, center = 0, width = 5, P=1, phase = 0) {
  A*sin(2*pi*x/P)*dnorm((x - center)/width)
}
slice_plot(wave_packet(x, A=2, center=3, width  = 4, P  = 10/5 ) ~ x, 
           domain(x = c(-15, 15)), npts = 500)
```
This is the product of two simple functions: a gaussian times a sinusoid.

```{r making-wave-packet, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="The two components of the wave packet in Figure \\@ref(fig:wave-packet)"}
sinusoid <- makeFun(sin(2*pi*x/P) ~ x, P=10/5)
envelope <- makeFun(2*dnorm((x - center)/width) ~ x, center=3, width=4)
slice_plot(envelope(x) ~ x, domain(x = c(-15, 15)), color="dodgerblue") %>%
  gf_labs(title="The \"envelope\" of the wave packet")
slice_plot(sinusoid(x) ~ x, domain(x = c(-15, 15)),
           color="orange3", npts=500) %>%
  gf_labs(title="The oscillation in the wave packet. ")
```
:::



::: {.intheworld  data-latex=""}
The initial rise in popularity of the social media platform [Yik Yak](https://en.wikipedia.org/wiki/Yik_Yak) was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.

On way to model this pattern is by multiplying a sigmoid by an exponential.(See Figure \@ref(fig:yikyak).)

```{r yikyak, fig.cap="Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017."}
yikyak <- makeFun(
  pnorm(year, mean=2014.5, sd=0.7) * exp(-(year-2014)) ~ year)
slice_plot(yikyak(year) ~ year, domain(year=2010:2018))
```


:::





Functions constructed as a ***product*** of simple functions can look like this in tradition notation:
$$h(t) \equiv \sin(t) e^{-t}$$
and like this in computer notation:
```{r}
h <- makeFun(sin(t)*exp(-t) ~ t)
```



## Watch your domain!

Each of our pattern-book functions, with two exceptions, has a domain that is the entire number line $-\infty < x < \infty$. No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with, since we never have to worry about the input going out of bounds. 

The two exceptions are:

1. the logarithm function, which is defined only for $0 < x$.
2. some of the power-law functions: $x^p$. 
    - When $p$ is negative, the output of the function is undefined when $x=0$. You can see why with a simple example: $g(x) \equiv x^{-2}$. Most students had it drilled into them that "division by zero is illegal," and $g(0) = \frac{1}{0} \frac{1}{0}$, a double law breaker. 
    - When $p$ is not an integer, that is $p \neq 1, 2, 3, \cdots$ the domain of the power-law function does not include negative inputs. To see why, consider the function $h(x) \equiv x^{1/3}$. 
    
::: {.rmosaic data-latex=""}
It can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It's a standard part of such hardware that whenever a function is handed an input that is not part of that function's domain, one of two special "numbers" is returned. To illustrate:
```{r warning=FALSE}
sqrt(-3)
(-2)^0.9999
1/0
```
`NaN` stands for "not a number." Just about any calculation involving `NaN` will generate `NaN` as a result, even those involving multiplication by zero or cancellation by subtraction or division.^[One that does produce a number is `NaN^0`.] For instance:
```{r warning=FALSE}
0 * NaN
NaN - NaN
NaN / NaN
```

Division by zero produces `Inf`, whose name is reminiscent of "infinity." `Inf` infiltrates any calculation in which it takes part:  `r mark(1510)` 
```{r warning=FALSE}
3 * Inf
sqrt(Inf)
0 * Inf
Inf + Inf
Inf - Inf
1/Inf
```

To see the benefits of the `NaN` / `Inf` system let's plot out the logarithm function over the graphics domain $-5 \leq x \leq 5$. Of course, part of that graphics domain, $-5 \leq x \leq 0$ is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The `NaN` provides some room for politeness. 

Open an R console and see what happens when you make the plot.
```{r eval=FALSE}
slice_plot(log(x) ~ x, domain(x=c(-5,5)))
```
:::

## Splitting the domain

Let's consider a familiar mathematical function: the absolute-value function:

$$abs(x) = \left|x\right|$$
Written this way, the definition of $abs()$ is a tautology. Unless you already know what $\left|x\right|$ means, you will have no clue what's going on.

So, instead, let's define $abs()$ in terms of pattern-book functions and scaling. It will look like this:
But with the ability to divide the domain into pieces, we gain access to a less mysterious sort of arithmetic operation and can re-write $$abs(x) \equiv \left\{ 
\begin{array}{rl}  x & \text{for}\ 0 \leq x \\ 
- x & \text{otherwise}\\\end{array}
\right.$$ 

This is an example of a ***piecewise function***, that is a function whose domain is split into two or more intervals and defined by different formulas on those intervals. In the conventional mathematical notation, there is a large $\LARGE\left\{\right.$ followed by two or more lines. Each line gives a formula for that part of the function and indicates to which interval the formula applies.

Another piecewise function widely used in technical work, but not as familiar as $abs()$ is the ***Heaviside function***:
Less familiar is the ***Heaviside function*** which has important uses in physics and engineering:

$$\text{Heaviside}(x) \equiv \left\{ 
\begin{array}{cl} 1 & \text{for}\ 0 \leq x \\0 & \text{otherwise}\end{array}
\right.$$

```{r echo=FALSE}
slice_plot(0 ~ x, domain(x = c(-10, 0)), size=2) %>%
  slice_plot(1 ~ x, domain(x = c(0,10)), size=2) %>%
  gf_labs(subtitle="Heaviside function") %>%
  gf_lims(y = c(-0.5, 1.5))
```
::: {.rmosaic}

The traditional piecewise notation involving $\LARGE\left\{\right.$ is not directly useful as computer notation. In R, a handy way to define a piecewise function uses the R function `ifelse()` whose name is remarkably descriptive. The `ifelse()` function takes three arguments. The first is the question to be asked, the second is the value to return if the answer is "yes," and the third is the value to return for a "no" answer. Here's an example:

```{r}
H <- makeFun(ifelse(0 <= x, 1, 0) ~ x)
```


What takes getting used to here is the expression `0 <= x`. That expression is a ***question***; it is not a statement of fact. 

The table shows computer notation for some common sorts of questions.  `r mark(1520)` 

R notation              | English
------------------------|---------
`x > 2`      | "Is $x$ greater than 2?"
`y >= 3`     | "Is $y$ greater than or equal to 3?"
`x == 4`     | "Is $x$ exactly 4?"
`2 < x & x < 5`| "Is $x$ between 2 and 5?"^[Literally, "Is $x$ both greater than 2 and less than 5?"]
`x < 2 | x > 6` | "Is $x$ either less than 2 or greater than 6?"
`abs(x-5) < 2` | "Is $x$ within two units of 5?"




The vertical gap between the two pieces of the Heaviside function is called a ***discontinuity***. Intuitively, you cannot draw a discontinuous function ***without lifting the pencil from the paper***. The Heaviside function has a discontinuity at $x=0$.

Similarly, the ***ramp function*** is a kind of one-sided absolute value:
$$\text{ramp}(x) \equiv \left\{ 
\begin{array}{cl} x & \text{for}\ 0 \leq x\\0 & \text{otherwise}\end{array}
\right.$$
Or, in computer notation:
```{r}
ramp <- makeFun(ifelse(0 < x, x, 0) ~ x)
slice_plot(ramp(x) ~ x, domain(x=c(-3, 3)))
```

A linear combination of two input-shifted ramp functions gives a piecewise version of the sigmoid.
```{r}
sig <- makeFun(ramp(x+0.5) - ramp(x-0.5) ~ x)
slice_plot(sig(x) ~ x, domain(x=c(-3, 3)), npts=501)
```


::: {.intheworld  data-latex=""}
Figure \@ref(fig:gas-use-2) is a graph of monthly natural gas use in the author's household versus average temperature during the month. (Natural gas is measured in cubic feet, appreviated *ccf*.)  `r mark(1530)` 

```{r gas-use-2, echo=FALSE, fig.cap="The amount of natural gas used for heating the author's home varies with the outside temperature."}
gf_point(ccf ~ temp, data = Home_utilities, alpha=0.5) %>%
  gf_labs(title="Household natural gas use", x = "Average temperature for the month (deg. F)", y = "Volume of gas used (cubic feet)") %>%
  gf_lims(x = c(0, 85)) %>%
  slice_plot(4.3*ramp(62 - temp) + 15 ~ temp, color="dodgerblue", size=2, alpha=0.5)
```
The graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below $60^\circ$F and constant for higher temperatures.  The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more of less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that's needed only when the temperature is less than about $60^\circ$F.  

We can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is  $$\text{gas_ccf}(x) \equiv 4.3\,  \text{ramp}(62-x)  + 15$$
Even simpler is the model for the other uses of natural gas:
$$\text{other_ccf}(x) \equiv 15$$. 
:::



## Exercises

`r insert_calcZ_exercise("XX.XX", "FCXT1L", "Exercises/seahorse-build-pot.Rmd", skill="function composition")`

`r insert_calcZ_exercise("XX.XX", "3N3hCR", "Exercises/dog-dive-kitchen.Rmd",skill="function composition, function multiplication, linear combination")`

`r insert_calcZ_exercise("XX.XX", "HMTG1w", "Exercises/pine-bring-dish.Rmd", skill="function multiplication")`


Use `datasets::co2` as an example of a product of functions. Maybe pull out a smoother as the baseline and see if the amplitude changes with time. Or maybe look at successive differences, fit a sine with a time dependent amplitude.

<!-- Piecewise functions -->

`r insert_calcZ_exercise(13.05, "EDKYV", "Exercises/bigger-two.Rmd")`

`r insert_calcZ_exercise("13.07", "E9e7c6", "Exercises/goat-walk-window.Rmd")`


`r insert_calcZ_exercise("XX.XX", "mtHyvJ", "Exercises/beech-ride-table.Rmd", skill="asymptotes")`

<!--chapter:end:02-assembling-functions.Rmd-->

