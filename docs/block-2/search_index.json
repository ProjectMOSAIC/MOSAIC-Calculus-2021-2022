[["prolog-differentiation-and-birth.html", "Prolog: Differentiation and birth", " Prolog: Differentiation and birth Stories are an important way in which people communicate and receive their understanding of the world. History, as the name suggests, is replete with stories. The definition of history as the “study of what happened in the light of what might have happened” even suggests a counter-factual aspect to some stories.1 Mathematics instruction is weak on stories, with a few exceptions such as the Eureka! moment or the apple falling from an orchard tree. Our new topic, differentiation, might be well served by connecting it to stories … but judge for yourself. The ancient Greek understanding of the origins and actions of the world was conveyed by stories of titans, gods, nymphs, and other characters. For the most part, these characters were created by the coupling of two parents, somewhat as we can create new functions by bringing together existing ones in composition, multiplication, and (linear) combination. In Greek mythology, the twelve Olympians of the pantheon, are Zeus, Hera, Poseidon, Demeter, Athena, Apollo, and others such as Ares (war fighting) and Aphrodite (love, beauty). Each Olympian has his or her own character and special power and authority. Athena, for instance, is a god of wisdom, craft, and war strategy. Similarly, we have our nine pattern-book functions, each of which can morph into a variety of shapes by introducing parameters. Athena is said to have emerged fully formed from the forehead of her father Zeus, a strange form of birth but a birth nonetheless. The technical word for birth is “parturition” whose first syllable is reminiscent of “to part” or “parting”: the separation of child from parent. And, as every parent and child knows, a child is different from—apart from—the parent. A person in their own right. Their characteristics and hopes are, even if related, distinct and they can play utterly different roles in history or myth. In Greek mythology, parthenogenesis—procreation from a single parent—is common. For instance, Dionysos sprang from the virgin Persephone. The topic of this Block, differentiation, is to be thought of much like parthenogenesis. There is one parent—a function. From this parent springs a new being, also a function. Among our pattern-book functions, there are many parent-child pairs, just as the Olympian Athena is the child of Zeus. For instance, the reciprocal function is the child of the logarithm. The exponential is its own parent and the sine is its own grandparent. Insofar as the role of functions is to convey information about relationships and patterns, the child function created by differentiation conveys different information or a different perspective or emphasis than the parent. (Not so true in the case of the exponential, which is its own child, grandchild, and down the line. As such, it has a special role to play in modeling.) Calculus books tend to be technical. Their description of the the process of one-parent birth—that is, differentiation—usually emphasizes the details of pushing and pulling, viscera and parturition.2 But usually these efforts are details that contribute little or nothing to the important story and don’t make it into polite conversation. The basis of the story is the fact of the parent-child relationship and the understanding that the child and the parent are different people whose characteristics and actions, desires, fates, and foibles are intertwined but different. So before we delve into the obstetric details of differentiation in the next chapters, keep in mind that the important part is the creation of a new and different being whose story will convey to us different information and insight than the parent. Examples of the parent-child connection created by differentiation are all around us. When the parent function signifies the volume of water in a lake, differentiation produces a new function that tells us about the flows of streams into and out of the lake, rain, and evaporation. Differentiating the function that conveys the position of an object generates a new function that represents the velocity of that object. If the parent is the function of time that tells the size of retirement investment, the birth by differentiation creates a function with a different story: savings, expenditure, profit, and loss. Just as people have a solid, immediate understanding of the parent-child relationship, people have an almost tangible grasp of differentiation relationships. You can sense position with your eyes, as you can sense velocity by the wind in your face regardless of where you happen to be at the time. You know whether a flow is great or small even without knowing the volume of the lake or ocean to which the flow is delivered. Science as a story is full of parent-child relationships, and sometimes is constructed out of the child-grandparent relationship. For instance, the function representing acceleration is the child of velocity, which is in turn the child of position. And, occasionally in science and mathematics, the great-grandchild or the great-great-grandchild are part of the relationships being described. A warning: There is one important way in which thinking of differentiation as parthenogenesis is utterly misleading. With functions, birth can be run in reverse, generating the parent function from the child. The process of this utterly unbiological reverse birthing is called “anti-differentiation,” and is every bit as important as differentiation even if it is harder to envision. The American Heritage Dictionary of the English Language, 5th Edition gives this word origin for “history”: “Middle English histoire from Old French from Latin historia from Greek historiā from historein to inquire from histōr learned man weid- in Indo-European roots.” [weid is also the origin of “wit”.]↩︎ Athena’s birth story features iron-mongering within Zeus’s head, and the male mid-wife’s use of an axe-blow to the head.↩︎ "],["change-relationships.html", "Chapter 17 Change relationships 17.1 Mathematics in motion 17.2 Continuous time 17.3 Instantaneous rate of change 17.4 Slopes and motion 17.5 Acceleration 17.6 Notations for differentiation 17.7 Visualizing the slope function 17.8 Dimension of derivatives 17.9 Exercises", " Chapter 17 Change relationships As you know, a function is a mathematical idea used to represent a relationship between quantities. For instance, the water volume of a reservoir behind a dam varies with the seasons and over the years; there is a relationship between water volume (one quantity) and time (another quantity). Similarly, the flow in a river feeding the reservoir has a relationship with time. In spring the river may be rushing with snow-melt, in late summer the river may be dry, but after a summer downpour the river flow again rises briefly. In other words, river flow is a function of time. Differentiation is a way of describing a relationship between relationships. The water volume in the reservoir has a relationship with time. The river flow has a relationship with time. Those two relationships are themselves related: the river flow feeds the reservoir and thereby influences the water volume. It’s not so easy to keep straight what’s going on in a relationship between relationships. When you can describe such a thing, it often gives great insight to the mechanisms that drive the world. For instance, Johannes Kepler (1572-1630) spent years analyzing the data collected by astronomer Tycho Brahe (1546-1601). The data showed clearly a relationship between time and the speed of a planet across the sky. Long-standing wisdom claimed that there is also a specific relationship between a planet’s position and time. From antiquity it had been certain that planets moved in circular orbits. Kepler worked hard to find the relationship between the two relationships: speed vs time and position vs time. He was unsuccessful until he dropped the assumption that planet orbits are circular. Positing orbits as elliptical, Kepler was able to find a simple relationship between speed vs time and position vs time. Building on Kepler’s work, Newton hypothesized that planets might be influenced by the same gravity that pulls an apple to the ground. It was evident from human experience that gravity has the most trivial relationship with time: gravity is constant! But Newton could not find a link between this constant notion of gravity and Kepler’s planetary motion as a function of time. Success came when Newton hypothesized—without any direct evidence from experience—that gravity is a function of distance. Newton’s formulation of the relationship between relationships— gravity-as-a-function-of-distance and orbital-position-as-a-function-of time—became the a foundation of model science. Newton’s theories of gravity, force, and motion created an extremely complicated chain or reasoning that is still almost impossible to grasp. Or, more precisely, it is almost impossible to grasp until you have the language for describing relationships between relationships. Newton invented this language, the language of differentiation. As you learn to understand this language, you will find it easier to express and understand relationships between relationships, that is, the mechanisms that account for the ever changing quantities around us. 17.1 Mathematics in motion The questions that started it all had to do with motion. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location and distance: far and near, long and short, here and there. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and of speed (quick and slow)? 2000 Galileo (1564-1642) started the ball rolling.3 As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. When a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After accidentally attending a lecture on geometry, he turned to mathematics and natural philosophy. Inventing the telescope, his observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him to measure accurately the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball’s passage were spaced arithmetically in musical time: 1, 2, 3, 4, …. But the distance between the gates was geometric: 1, 4, 9, 16, …. Thus he established a mathematical relationship between increments in time and increments in position. Time advanced as 1, 1, 1, 1, … and position as 1, 3, 5, 7, …. He observed that the second increments of position, the increments of the increments 1, 3, 5, 7, …, were themselves evenly spaced: 2, 2, 2, …. 2005 Putting these observations in tabular form, and adding columns for the first increment \\(y(t) \\equiv x(t+1) - x(t)\\) and the second increment \\(y(t+1) - y(t)\\) \\(t\\) \\(x(t)\\) first increment second increment 0 0 1 2 1 1 3 2 2 4 5 2 3 9 7 4 16 Galileo had neither the mathematics nor the equipment to measure motion continuously in time. So what might be obvious to us now, that position is a function of time \\(x(t)\\), would have had little practical significance to him. But we discover in his first increments of \\(x\\) something very much like the slope function in Chapter ??. 2010 \\[{\\cal D}_t\\, x(t) \\equiv \\frac{x(t + 1) - x(t)}{1}\\] From his data, he observed that \\({\\cal D}_t\\, x(t)\\) increases linearly in \\(t\\): \\[{\\cal D}_t x(t) = 2 t + 1\\] Calculating the second increments of \\(x\\) is done by the “slope function of the slope function,” which we can call \\({\\cal D}_{tt}\\): \\[{\\cal D}_{tt} x(t) \\equiv {\\cal D}_t \\left[{\\cal D}_t x(t)\\right] = {\\cal D_t} \\left[\\strut 2t+1\\right] = \\frac{\\left[\\strut2(t+1) + 1\\right] - \\left[\\strut 2 t + 1\\right]}{1} = 2\\] 17.2 Continuous time Newton placed the motion in continuous time rather than Galileo’s discrete time. He reframed the slope function from the big increments of the slope operator \\({\\cal D}_t\\) to imagined vanishingly small increments of a operator that we shall denote \\(\\partial_t\\) and call differentiation. 2015 The kind of question for which Newton wanted to be able to calculate the answer was, “How to find the function \\(x(t)\\) whose second increment, \\(\\partial_{tt} x(t) = 2\\)?” His approach, which he called the “method of fluxions,” became so important that its name became, simply, “Calculus.” 2020 Over the next three centuries, calculus evolved from a set of techniques for describing motion into the general-purpose mathematics of change. Applying calculus in the real world involves understanding change relationships between quantities. To give some examples: 2025 Electrical power is the change with respect to time of electrical energy. Birth rate is one component of the change with respect to time of population. Interest, as in bank interest or credit card interest, is the change with respect to time of assets. Inflation is the change with respect to time of prices. Disease incidence is one component of the change with respect to time of disease prevalence. Force is the change with respect to position of energy. 17.3 Instantaneous rate of change On the radio once, I heard a baseball fanatic describing the path of a home run slammed just inside the left-field post. “Coming off the bat, the ball screamed upwards, passing five stories over the head of the first baseman and still gaining altitude. Then, somewhere over mid left-field, gravity caught up with the ball, forcing it down faster and faster until it crashed into the cheap seats.” A nice image, perhaps, but wrong about the physics. Gravity doesn’t suddenly catch hold of the ball; even when upward bound, gravity influences the ball to exactly the same extent as it does at the peak of the flight and as the ball falls back down. The vertical velocity of the ball is positive while climbing and negative on descent, but that velocity is steadily changing all through the flight: a smooth, almost linear numerical decrease in velocity from the time the ball leaves the bat to when it lands in the bleachers. At each instant of time, the vertical velocity of the ball has a numerical value in feet-per-second. That value changes continuously and is never the same at any two points in the ball’s flight. If \\(Z(t)\\) is the height of the ball at time \\(t\\), and \\(v_Z(t)\\) is the vertical velocity at time \\(t\\), then the slope function \\[{\\cal D}_t Z(t) \\equiv \\frac{Z(t+h) - Z(t)}{h}\\] tells us the average velocity of the ball over a time interval of \\(h\\). The “average velocity” is a human construction. At each instant in time the ball has a velocity that is constantly changing. The reality of the ball is that it has only an instantaneous velocity. The average velocity is merely a concession to the way we might measure the velocity, by recording the height at two different times and computing the difference in height divided by the difference in time. Our measurement of the average velocity gets closer to the instantaneous velocity when we make the time interval \\(h\\) smaller. Ideally, to genuinely reflect the state of the ball at a instant, we would make the interval of time infinitely small, that is, we would make \\(h = 0\\). One thing that happens when we make \\(h = 0\\) is that the formula for \\({\\cal D}_t Z(t)\\) suffers from a divide by zero; a meaningless arithmetic operation. So it would seem that “instantaneous velocity” is a mathematical non-starter, even if it is a physical reality. But there’s something else that happens when \\(h = 0\\), the two heights \\(Z(t + h)\\) and \\(Z(t)\\) become equal, so \\(Z(t + h) - Z(t) = 0\\). Not only are we dividing by zero when calculating \\({\\cal D}_t Z(t)\\), the quantity that we are dividing zero into is itself zero. We have 0/0. That’s a doubly mysterious quantity, an arithmetic non-entity. The mystery of 0/0 baffled mathematicians and philosophers for thousands of years. It was Newton who turned it into a computational reality, although his reasoning was regarded with suspicion for two hundred years. The world’s best mathematicians struggled for centuries with the logic of finding a mathematical framework for making sense of what a baseball and gravity do naturally. Rather than ourselves dealing with the intricacies of mathematical logic, we can gain an adequate understanding of the situation by avoiding \\(h=0\\) in favor of a gentler, gradual, evanescent h. The type of slope function calculated with this (as yet undefined) evanescent h is called a derivative and corresponds to the instantaneous rate-of-change function. The process of constructing the derivative of a function \\(f(t)\\) is called differentiation. And to help us keep track of things, whenever we construct a derivative of \\(f(t)\\), we will name the constructed function \\(\\partial_t f(t)\\). Similarly, the name of the function that is the derivative of \\(g(x)\\) will be \\(\\partial_x g(x)\\) 17.4 Slopes and motion Consider a graph of the position of a car along a road as in Figure 17.1. Over the course of an hour, the car traveled about 25 miles. In other words, the average speed is 25 miles/hour: the slope of the tan-colored line segment. Given the traffic, sometimes the car was stopped (time C), sometimes crawling (time D) and sometimes much faster than average (time B). 2030 Figure 17.1: The position of an imagined car over an hour of time. (black) The tan-colored line shows what the position would have been if the car had travelled steadily at the average speed for the hour. Of course, when you’re driving you are aware of the car’s speed at any instant. You need only look at the speedometer to read off the value (in miles per hour). Speedometers don’t show the average speed for the entire trip. The average speed is the slope of the tan-colored line in Figure 17.1, 25 miles in one hour, usually stated 25 miles-per-hour. 2035 In terms of Figure 17.1, the speedometer reading is the slope of \\(f(t)\\) at the given instant. You can see from the Figure that at instant A the speed is very close to the average speed for the entire trip. At instant B the car is going faster; the slope is much steeper. On the other hand, at instant C the car is at a standstill; its position doesn’t change at all. 2040 A car’s speedometer shows the speed at each moment—or instant—of the trip. As you can see in Figure 17.1, the speed varies and is sometimes less than the average speed, sometimes greater, and occasionally equal to the average speed over the trip. 2045 You can easily judge from a graph of \\(f(t)\\) whether the instantaneous speed is faster or slower than the average speed. Better, however, if we simply record the speedometer reading and graph that, as in 17.2. You can read off the speed from the graph at any instant of time simply by reference to the vertical axis. Figure 17.2: The instantaneous speed of the car whose position vs time is shown in Figure 17.1. The two graphs in Figures 17.1 and 17.2 show exactly the same car trip. For each of the two graphs, the presentation of the data makes it easy to see some things and hard to see others. For instance, figuring out when the car is at a stand-still is harder in the position-vs-time graph than in the speed-vs-time graph. 2050 17.5 Acceleration Having worked out a theory of slope functions, Newton was ready to express the laws of motion in continuous time. He did this by denoting position as \\(x(t)\\). The familiar concepts of velocity and force could then be defined in terms of slope functions of position and the “quantity of matter,” which we call “mass.” 2055 Velocity is the slope function of position: \\(v(t) \\equiv {\\cal D}_t x(t)\\). Net force is the slope function of velocity times mass: \\(F(t) \\equiv m {\\cal D}_t v(t) = m {\\cal D}_{tt} x(t)\\) To take mass out of the formulation, we give a name specifically to the slope function of velocity: acceleration. Acceleration is the slope function of velocity: \\(a(t) \\equiv {\\cal D}_t v(t) = {\\cal D}_{tt} x(t)\\). With acceleration as a concept, we can define net force as mass times acceleration. This is Newton’s Second Law of Motion. We used net force as the quantity we related to mass and the slope function of velocity. There are different sources of forces which add up and can cancel out. Famously, Newton formulated the law of universal gravitation which ascribed the force between masses as proportional to the product of the two masses and inversely proportional to the square of the distance between them. But a mass on a table has no net force on it, since the table pushes back (push = force) on the mass to cancel out the force due to gravity. “Net force” takes such cancellation into account. 2060 17.6 Notations for differentiation There are several traditional notations for differentiation of a function named \\(f()\\). Here’s a list of some of them, along with the name associated with each: Leibnitz: \\(\\frac{df}{dx}\\) Partial: \\(\\frac{\\partial f}{\\partial x}\\) Euler: \\(D_x f\\) Newton (or “dot”): \\(\\dot{f}\\) Lagrange (or “prime”): \\(f&#39;\\) One-line: \\(\\partial_x f\\) (This hybrid of partial and Euler notation, will be the main differential notation used in this book.) It is a fact of mathematical and scientific life that a variety of notations are used for differentiation. To some extent, this reflects historical precedence and, to be honest, nationalistic European politics of the 18th century. To make sense of mathematical writing in the many areas in which calculus is used, you have to recognize all of them for what they are. Your skill will be enhanced if you also memorize the names of the different styles. It’s not all that different from the pattern in English of having multiple words for the same sort of object, for instance: car, automobile, junker, ride, wheels, crate, jalopy, limo, motor car, horseless carriage. 2100 In the days when carriages where pulled by horses, the phrase “horseless carriage” made a useful distinction. Today, when horses are rarely seen on the road, it makes sense to trim down the notation to its essentials: horseless cariage. Think of \\(\\partial_x\\) as this sort of minification of older notations.4 If you’ve studied calculus before, you have likely seen the \\(f&#39;\\) notation. This is admirably concise but is only viable in a narrow circumstance: functions that take a single input. What \\(f&#39;\\) leaves out is a means to specify a crucial aspect of differentiation, the with-respect-to variable. The general situation for differentiation involves functions of one or more variables, for example, \\(g(x, y, z)\\). For such functions, you need to specify which is the with-respect-to variable. For instance, we can differentiate \\(g()\\) three different ways, each way incrementing one or another of the three inputs: 2075 \\[\\partial_x g(x, y, z) \\equiv \\frac{g(x+h, y, z) - g(x, y, z)}{h}\\] \\[\\partial_y g(x, y, z) \\equiv \\frac{g(x, y+h, z) - g(x, y, z)}{h}\\] \\[\\partial_z g(x, y, z) \\equiv \\frac{g(x, y, z+h) - g(x, y, z)}{h}\\] At this point in your studies, you haven’t seen why you might choose to differentiate a function with respect to one variable or another. That will come in time. But we want to set you up with a notation that won’t narrow your options. In this book, we will mainly use the one-line notation, \\(\\partial_x f\\), but it means exactly the same as the Leibnitz and Partial notations, which are much more widely used in textbooks. 2080 Both the Leibnitz and Partial notations are explicit in identifying the function and the with-respect-to-variable. For example, using the Partial differentiation notation, the three ways of differentiating our example function \\(g(x, y, z)\\) are labeled : 2085 \\[\\frac{\\partial f}{\\partial x},\\ \\ \\ \\frac{\\partial f}{\\partial y},\\ \\ \\text{and}\\ \\ \\frac{\\partial f}{\\partial z}\\] Our R/mosaic computer differentiation is longer but explicit: g &lt;- makeFun(__formula__ ~ x &amp; y &amp; z) # define a function dx_g &lt;- D(g(x, y, z) ~ x) dy_g &lt;- D(g(x, y, z) ~ y) dz_g &lt;- D(g(x, y, z) ~ z) The names assigned to the result of the D() operator can be any names you like. What’s nice about dx_g and the others is that it mimics the math notation \\(\\partial_x g()\\). Notice that the R/mosaic operator for differentiation is named D() and that it is a function. It follows the same pattern as makeFun() or slice_plot() or contour_plot(): the first argument is a tilde expression, for instance g(x, y, z) ~ x, which identifies the mathematical function to work with (g()) and the name of the with-respect-to input to that function. The R/mosaic notation makes it clear that differentiation is an operation on a function. The D() operator takes a function as input and produces as output another function. We’ve seen similar behavior with, say, slice_plot(), which takes a function as input and produces graphics as output. Both D() and slice_plot() need to know the identity of the with-respect-to variable as well as the function to work with. That’s why both pieces of input are packaged into a tilde expression. 2090 We’re calling D() an operator rather than a function. The reason is purely for communication with other people. There are so many “functions” in a calculus course that we thought it would be helpful to distinguish between the kinds of functions that take quantities as input and produce a quantity as output and the functions that take a function as input and produce a function as output.5 Both sorts are called “functions” in R terminology. But a sentence like, “Differentiation is a function that takes a function as input and produces a function as output,” true though it be, is dizzying. 2095 17.7 Visualizing the slope function Look back at Figures 17.1 and 17.2 (which we reproduce here). We know the two functions are closely related—one is the position of the car and the other the speed. But it’s hard to see the relationship at a glance. You have to go patiently back and forth between the two graphs, comparing a slope in one graph to an output value in the other graph. We can make things easier by taking an unconventional approach to graphing the slope function. Rather than showing the slope as the vertical position on a graph, let’s show the slope with an actual slope! Perhaps this non-standard visualization will give you a better way to understand slope functions. If so, good. The ultimate benefit of a way to show \\(\\diff{x} f(x)\\) and \\(f(x)\\) in the same frame will come when we introduce the operation of anti-differentiation. 2130 Recall that the basic model of change in Calculus is the straight-line function \\(\\line(x) \\equiv a x + b\\). The slope \\(a\\) of \\(\\line(x)\\) tells how the output changes for a unit change in input. In differentiation, we 2135 approximate the parent function \\(f(x)\\) as a series of local line segments. extract the slope of each line segment as the value of the slope function at each input \\(x\\). Figure 17.3 shows the segment by segment approximation around each of several input values (marked in green). The slope function visualization is constructed by throwing away the vertical offset of each of the line segments and plotting them horizontally adjacent to one another. 2140 Figure 17.3: A function \\(f(x)\\) shown along with the tangent line segment touching \\(f()\\) at each of the green points. For the slope function visualization, the tangent line segments are moved down to the horizontal axis. You can see that the slopes are a function of \\(x\\), that is, the slope changes with \\(x\\). Because the function and it’s slope function are shown on the same graph in the same way, it’s easy to verify that the slope as a function of \\(x\\) corresponds to the behavior of the function itself. Figure 17.4 shows several examples of the slope function visualization. Figure 17.4: Slope-function visualizations (left) of several pattern-book functions (right). Calculus and the Wealth of Nations 1776 can be reckoned as the birth year of two revolutions: the American Declaration of Independence and Adam Smith’s publication of the Wealth of Nations. Smith, considered the intellectual father of free-market economics, explored the origins of the supply and demand of commodities, labor, and money. A key figure of the Scottish Enlightenment, Smith would have been well aware of Newton, his work, and the many advances enabled by the creation of calculus. Wealth of Nations lays out dozens of relationships between different quantities — wages, labor, stock, interest, prices, profits, and coinage among others. Yet Wealth of Nations does not use the concepts or language of calculus. Lacking this, Smith’s arguments, sophisticated though they be, are based on the Aristotelian notions of tendency toward a “natural” resting place.6 Consider this characteristic statement in Wealth of Nations: The market price of every particular commodity is regulated by the proportion between the quantity which is actually brought to market, and the demand of those who are willing to pay the natural price of the commodity… Such people may be called the effectual demanders, and their demand the effectual demand. Smith’s “natural price” and “effectual demand” are fixed quantities. But Smith lived near the end of a centuries-long period of static economies. Transportation, agriculture, manufacture, population were all much as they had been for the past 500 years or longer.7 Calculus was invented to deal with dynamics: how things change. It took the industrial revolution and nearly a century of intellectual development before economics was seen dynamically. In this dynamical view, supply and demand are not seen as mere quantities, but as functions of which price is the major input. The tradition in economics is to use the word “curve” instead of “function,” giving us the phrases “supply curve” and “demand curve.” Many students starting out in economics can easily see supply and demand as quantities. Making the transition from quantity to function, that is, between a single amount and a relationship between amounts is a core challenge to those learning economics. Once this transition is accomplished, economics students are taught essential concepts of calculus—particularly first and second derivatives, the subjects of this Block—although the names used are peculiar to economics, for instance, “elasticity”, “marginal returns” and “diminishing marginal returns.” Figure 17.5: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. 17.8 Dimension of derivatives The function named \\(\\partial_t f(t)\\) which is the derivative of \\(f(t)\\) takes the same input as \\(f(t)\\); the notation makes that pretty clear. Let’s suppose that \\(t\\) is time and so the dimension of the input is \\([t] = \\text{T}\\). The outputs of the two functions, \\(\\partial_t f(t)\\) and \\(f(t)\\) will not, in general, have the same dimension. Why not? Recall that a derivative is a special case of a slope function, the instantaneous slope function. It’s easy to calculate a slope function: \\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] The dimension of the quantity \\(f(t+h) - f(t)\\) must be the same as the dimension of \\(f(t)\\); the subtraction wouldn’t be possible otherwise. The dimension of \\(h\\), similarly, must be the same as the dimension of \\(t\\); the addition wouldn’t make sense otherwise. Whereas the dimension of the output \\(f(t)\\) is simply \\(\\left[f(t)\\right]\\), the dimension of the quotient \\(\\frac{f(t+h) - f(t)}{h}\\) will be different. The output of the derivative function \\(\\partial_t f(t)\\) will be \\[\\left[\\partial_t f(t)\\right] = \\left[f(t)\\right] / \\left[t\\right] .\\] Suppose \\(x(t)\\) is the position of a car as a function of time \\(t\\). Position has dimension L. Time has dimension T. The function \\(\\partial_t x(t)\\) will have dimension L/T; that’s what velocity is, for instance miles-per-hour. Another example: Suppose the function pressure() takes as input altitude input (in km) and returns as output a pressure (in kPa, “kiloPascal”8). The derivative function, let’s call it \\(\\partial_\\text{altitude} \\text{pressure}()\\), also takes an input in km, but produces an output in kPA per km: a rate. 17.9 Exercises Exercise 17.02: slwdkw Each question involves a pair of quantities that are a function of time and that might or might not be a quantity/rate-of-change pair. If they are, say which quantity is which. Feel free to look up a dictionary definition of words you are uncertain about. Question A Deficit and debt Deficit is the rate of change of debt with respect to time.Nice!  Debt is the rate of change of deficit with respect to time.︎✘ They are not a rate of change pair.︎✘ Question B water contained and flow Flow is the rate of change of water contained with respect to time.Right!  Water contained is the rate of change of flow with respect to time.︎✘ They are not a rate of change pair.︎✘ Question C Interest rate and debt owed on credit card Interest rate is the rate of change of credit card debt with respect to time.Correct.  Credit card debt is the rate of change of interest rate with respect to time.︎✘ They are not a rate of change pair.︎✘ Question D Rain intensity and total rainfall Rain intensity is the rate of change of total rainfall with respect to time.Good.  Total rainfall is the rate of change of rain intensity with respect to time.︎✘ They are not a rate of change pair.︎✘ Question E Force and acceleration Force is the rate of change of acceleration with respect to time.︎✘ Acceleration is the rate of change of force with respect to time.︎✘ They are not a rate of change pair.Nice! The dimension of force is \\(ML/T^2\\). The dimension of acceleration is \\(L/T^2\\). A rate of change with respect to time should have an extra T in the denominator of the dimensions. Question F Position and acceleration Position is the rate of change of acceleration with repect to time.︎✘ Acceleration is the rate of change of position with respect to time.︎✘ They are not a rate of change pair.Right! The dimension of position is \\(L\\). The dimension of acceleration is \\(L/T^2\\). The rate of change of position would have dimension \\(L/T\\). That’s called ‘velocity’. Question G Velocity and air resistence Velocity is the rate of change of air resistence with repect to time.︎✘ Air resistence is the rate of change of velocity with respect to time.︎✘ They are not a rate of change pair.Excellent! Air resistence is a force, with dimension \\(M L/T^2\\). Velocity has dimension \\(L/T\\). The rate of change of velocity with respect to time is acceleration, which has dimension \\(L/T^2\\). Exercise 17.04: eodlt Recall from Section ?? that a function is monotonically increasing on a given domain when the function’s slope is positive everywhere in that domain. A monotonically decreasing function, similarly, has a negative slope everywhere in the domain. When the slope is zero, or positive in some places and negative in others, the function is neither monotonically increasing or decreasing. Each of the following graphs shows the derivative of some function \\(f(x)\\). (Note: the graph doesn’t show \\(f(x)\\) but rather the function \\(\\partial_x f(x)\\)) For each graph, say whether the function \\(f()\\) is monotonically increasing, monotonically decreasing, or neither. (Note that the horizontal scale is the same in every graph, but the vertical scale can be different from one scale to another.) Question A Function A is … monotonically increasingNice! A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonic︎✘ A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info provided︎✘ This is the case if you cannot tell if the derivative is positive or negative. Question B Function B is … monotonically increasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasingExcellent! A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonic︎✘ A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info provided︎✘ This is the case if you cannot tell if the derivative is positive or negative. Question C Function C is … monotonically increasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonicCorrect. A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info provided︎✘ This is the case if you cannot tell if the derivative is positive or negative. Question D Function D is … monotonically increasingCorrect. A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonic︎✘ A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info provided︎✘ This is the case if you cannot tell if the derivative is positive or negative. Question E Function E is … monotonically increasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonic︎✘ A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info providedRight! This is the case if you cannot tell if the derivative is positive or negative. Question F Function F is … monotonically increasingGood. A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) monotonically decreasing︎✘ A monotonically increasing function has a function that is everywhere \\(&gt; 0\\) constant︎✘ A constant function has a derivative that is everywhere 0. non-monotonic︎✘ A non-monotonic function goes up and down, hence the derivative is positive in some places and negative in others. Can’t tell from the info provided︎✘ This is the case if you cannot tell if the derivative is positive or negative. Exercise 17.06: iclcws Here are graphs of various functions. The right column shows functions named \\(f_1()\\), \\(f_2()\\), and so on. The left column shows functions \\(A()\\), \\(B()\\), \\(C()\\), and so on. Most of the functions on the right are the derivative of some function on the left, and most of the functions on the left have their corresponding derivative on the right. Your task: Match the function on the left to it’s derivative on the right. Question A The derivative of Function A() is which of the following:     f1()︎✘                   f2()\\(\\heartsuit\\ \\)       f3()︎✘                 f4()︎✘                 not shown︎✘ Question B The derivative of Function B() is which of the following:     f1()\\(\\heartsuit\\ \\)       f2()︎✘                f3()︎✘                f4()︎✘                not shown︎✘ Question C The derivative of Function C() is which of the following:     f1()︎✘                f2()︎✘                f3()︎✘                f4()\\(\\heartsuit\\ \\)       not shown︎✘ Question D The derivative of Function D() is which of the following:     f1()︎✘                 f2()︎✘                  f3()︎✘                 f4()︎✘                 not shown\\(\\heartsuit\\ \\) Exercise 17.08: helxs The left column of graphs shows functions A(), B(), C(), and D(). The right column shows functions dd1(), dd2(), and so on. Find which function (if any) in the right column corresponds to the 2nd derivative of a function in the left column. Remember the concepts of “concave up” (a smile!) and “concave down” (a frown). At those values of \\(x\\) for which the 2nd derivative of a given function is positive, the given function will be concave up. When the 2nd derivative is negative, the given function will be concave down. Question A The second derivative of Function A() is which of the following:     dd1()︎✘        dd2()︎✘        dd3()︎✘        dd4()\\(\\heartsuit\\ \\) Question B The second derivative of Function B() is which of the following:     dd1()︎✘        dd2()︎✘        dd3()\\(\\heartsuit\\ \\)       dd4()︎✘ Question C The second derivative of Function C() is which of the following:     dd1()︎✘        dd2()\\(\\heartsuit\\ \\)       dd3()︎✘        dd4()︎✘ Question D The second derivative of Function D() is which of the following:     dd1()\\(\\heartsuit\\ \\)       dd2()︎✘        dd3()︎✘        dd4()︎✘ Exercise 17.10: 1kZXxT The plots each show a function graphed in the usual way, and a slope function graphed using the slope function visualization. Your task is to determine whether the slope function being displayed in each graph is a match to the function in that graph. Question A In plot (A), does the slope function displayed correspond to the function that’s graphed?     Yes\\(\\heartsuit\\ \\)       No︎✘ Question B In plot (B), does the slope function displayed correspond to the function that’s graphed?     Yes︎✘        No\\(\\heartsuit\\ \\) Question C In plot (C), does the slope function displayed correspond to the function that’s graphed?     Yes\\(\\heartsuit\\ \\)       No︎✘ Question D In plot (D), does the slope function displayed correspond to the function that’s graphed?     Yes︎✘        No\\(\\heartsuit\\ \\) Exercise 17.12: eyded In the following, three different functions are described. Your task is to write down the dimension of the input and of the output. Do this both for the function itself, and for the derivative of the function. For example, the dimension of the output of \\(N(y)\\) given below is P, for population. The input has dimension T, for time. A. The given function is \\(N(y)\\), the population of the Netherlands in year \\(y\\). Dimension of input to \\(N(y)\\)? Dimension of output from \\(N(y)\\)? Dimension of input to \\(\\partial_y N(y)\\)? Dimension of output from \\(\\partial_y N(y)\\)? B. The given function is \\(p(u)\\), the net profit from a manufactured good as a function of the number of units manufactured. Dimension of input to \\(p(u)\\)? Dimension of output from \\(p(u)\\)? Dimension of input to \\(\\partial_u p(u)\\)? Dimension of output from \\(\\partial_u p(u)\\)? C. The given function is \\(w(t)\\), the amount of water in a leaky bucket at any time after the bucket was filled. Dimension of input to \\(w(t)\\)? Dimension of output from \\(w(t)\\)? Dimension of input to \\(\\partial_t w(t)\\)? Dimension of output from \\(\\partial_t w(t)\\)? Exercise 17.14: elclvd Question A Tanks for bulk storage of natural gas are typically large cylinders with a cap that can move up and down. The volume of the tank is a function of the position of the cap. What is the dimension of the derivative of cylinder volume with respect to cap position?     \\(L^2\\)\\(\\heartsuit\\ \\)       \\(L\\)︎✘        \\(L^3\\)︎✘        \\(L^3/T\\)︎✘        \\(T/L^3\\)︎✘ Exercise 17.16: nsmx8w3 The standard model of epidemics used in public health planning is called the SIR model. (SIR stands for “Susceptible (S), Infective (I), Recovered (R)”, the sequence that a person starts in, moves to, and ends up in (hopefully!) in an epidemic.) One of the equations in the SIR model is \\[\\frac{dS}{dt} = -a S I\\] The notation \\(dS/dt\\) means “the rate of change of number of susceptibles, S, with respect to time.” This has dimension “people/T”. The dimensions \\([S]\\) and \\([I]\\) are each simply “people.” Question A What is \\([a]\\)? T︎✘ T\\(^{-1}\\)︎✘ people/T︎✘ Then \\([a S I]\\) would be people\\(^3\\)/T, but that’s not the same as \\([dS/dt]\\). people\\(^{-1}\\) T\\(^{-1}\\)Correct. This correctly gives \\([a S I]\\) as people/T, which is the same as \\([dS/dt]\\). people \\(\\times\\) T︎✘ None of the above.︎✘ Another equation in the SIR model describes how the number of infective people changes over time: \\[\\frac{dI}{dt} = - a S I - b I\\] where \\([\\frac{dI}{dt}] =\\) people/T. Question B What is \\([b]\\)? T︎✘ T\\(^{-1}\\)Right!  people/T︎✘ Then \\([a S I]\\) would be people\\(^3\\)/T, but that’s not the same as \\([dS/dt]\\). people\\(^{-1}\\) T\\(^{-1}\\)︎✘ If this were true, \\([bI]\\) would be T\\(^{-1}\\). But \\([bI]\\) has to be the same as \\([dI/dt]\\), which is people \\(T^{-1}\\). people \\(\\times\\) T︎✘ None of the above.︎✘ Galileo was not aware of Kepler’s elliptical theory, even though they lived at the same time.↩︎ Yes, “minification” is a word!↩︎ It’s pretty easy to see in an expression like \\(f(x,y)\\) why we call \\(f()\\) a function. But an expression like \\(3+2\\) also involves a function of two inputs. It’s just that we write the name of the function (+) in between the two inputs. This is called infix notation.↩︎ For more discussion, see Tony Aspromourgos (2007) “Adam Smith’s treatment of market prices and their relationship to &lt;&gt; and &lt;&gt;” History of Economic Ideas , 2007, Vol. 15, No. 3 (2007), pp. 27-57 Link↩︎ Smith commented on the difference between “demand” and “effectual demand:” “A very poor man may be said, in some sense, to have a demand for a coach and six [a carriage pulled by six horses]; he might like to have it; but his demand is not an effectual demand, as the commodity can never be brought to market in order to satisfy it.” In today’s economy, of course, transportation superior to a coach and six is readily demanded and supplied.↩︎ Air pressure at sea level is about 100 kiloPascal.↩︎ "],["evanescent-h.html", "Chapter 18 Evanescent h", " Chapter 18 Evanescent h Recall that the local rate of change of a function can be written as a ratio of rise-over-run: \\[\\partial_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] where \\(h\\) is the length of the “run.” The idea of the instantaneous rate of change is to make \\(h\\) as small as possible. In the very early days of calculus, the vanishing \\(h\\) was described as “evanescent.” (Dictionary definition: “tending to vanish like vapor.”9) Another good image of \\(h\\) becoming as small as possible comes from the same University of Oxford mathematician whose poem The Jabberwocky we introduced in Chapter ??. In Alice in Wonderland, Dodgson introduced the character of the Cheshire Cat. 2370 Figure 18.1: Vanishing \\(h\\) in the form of the Chesire Cat from Alice in Wonderland. Start our story with two of the basic modeling functions that, like the characters from Alice in Wonderland, have considerable “personality”: the sinusoid (sin()) and the sigmoid (pnorm()). Figure 18.2: The pattern-book sinusoid and sigmoidal functions. A vertical blue line has been added to mark the input \\(t=0\\) The computer can easily construct the slope functions for the sinusoid and sigmoid, which we’ll call Dsin() and Dsigma() respectively. Dsin &lt;- makeFun(( sin(t+h) - sin(t))/h ~ t, h=0.1) Dsigma &lt;- makeFun((pnorm(t+h) - pnorm(t))/h ~ t, h=0.1) In the tilde expression handed to makeFun(), we’ve identified t as the name of the input and given a “small” default value to the h parameter. But R recognizes that both Dsin() and Dsigma() are functions with two inputs, t and h, as you can see in the parenthesized argument list for the functions. Dsin ## function (t, h = 0.1) ## (sin(t + h) - sin(t))/h Dsigma ## function (t, h = 0.1) ## (pnorm(t + h) - pnorm(t))/h This is a nuisance, since when using the slope functions we will always need to think about h, a number that we’d like to describe simply as “small,” but for which we always need to provide a numerical value. A surprisingly important question in the development of calculus is, “What can we do to avoid this nuisance?” To find out, let’s look at Dsin() and Dsigma() for a range of values of h, as in Figure 18.3. 2372 Figure 18.3: The slope functions of the sinusoid and sigmoid. Each curve shows the slope function for a particular numerical choice of h. Both panels show \\(h=2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001\\). Some observations from this numerical experiment: As \\(h\\) gets very small, the slope function doesn’t depend on the exact value of \\(h\\). As you can see in Figure 18.3, the graphs of the functions with the smallest \\(h\\) (blue), with labels near the top of the graph) lie on top of one another. This will provide a way for us, eventually, to discard \\(h\\) so that the slope function will not need an \\(h\\) argument. For small \\(h\\), we have \\(\\partial_t \\sin(t) = \\sin(t + \\pi/2) = \\cos(t)\\). That is, taking the slope function of a sinusoid gives another sinusoid, shifted left by \\(\\pi/2\\) from the original. Or, in plain words, for small \\(h\\) the cosine is the slope function of the sine. For small \\(h\\), we have \\(\\partial_t \\pnorm(t) = \\dnorm(t)\\). That is, for small \\(h\\) the gaussian function is the slope function of the sigmoid \\(\\dnorm()\\) function. You can confirm these last two statements by comparison with the original functions, especially the alignment of the peaks of the slope functions with respect to the peak of the sinusoid and the half-way point of the sigmoid. 2374 Here you use \\(t\\) as the name of the input and \\(\\partial_t\\) as the notation for differentiation. Previously in this block you used \\(x\\) as the input name and \\(\\partial_x\\) for differentiation. Are they the same? 2376 Mathematically, the name of the input makes no difference whatsoever. We could call it \\(x\\) or \\(t\\) or \\(y\\) or Josephina. What’s important is that the name be used consistently on the left and right sides of \\(\\equiv\\), and that the derivative symbol \\(\\partial\\) has a subscript that identifies the with-respect-to input. All these are the same statement mathematically: 2378 \\[\\partial_x\\, x = 1\\ \\ \\ \\ \\partial_t\\, t = 1\\ \\ \\ \\ \\partial_y\\, y = 1\\ \\ \\ \\ \\partial_\\text{Josephina} \\text{Josephina} = 1\\] Admittedly, the last one is hard to read. When we look at derivatives of functions of multiple variables we will need to be thoughtful about our choice of the with-respect-to input. But we want you to get used to seeing different input names used for differentiation. 2380 Now consider the slope functions of the logarithm and exponential functions. Figure 18.4: The slope functions of the logarithm and exponential. These numerical experiments with the logarithm and exponential functions are more evidence that, as \\(h\\) gets small, the slope function doesn’t depend strongly on \\(h\\). And, we find that: For small \\(h\\), the slope function of the logarithm is a power-law function: \\(\\partial_t \\ln(t) = \\frac{1}{t}\\). For small \\(h\\), the slope function of the exponential is the exponential itself: \\(\\partial_t e^x = e^x\\). You can confirm these by evaluating the slope function of the exponential at \\(t=0\\) and \\(t=1\\), and the slope function of the logarithm at \\(t= 2, 1, 1/2, 1/4, 1/8.\\) “Small” and “zero,” although related, are different. In constructing a derivative, we use smaller and smaller \\(h\\), but never zero. Let’s see what happens if instead of evanescent h, we use zero h. For example, we can use the slope function Dsin() and Dsigma() that we created earlier. Setting \\(h\\) to zero does not give a result that is the instantaneous rate of change of anything: 2384 Dsin(t=1, h=0) ## [1] NaN Dsigma(t=0, h=0) ## [1] NaN In NaN, you can hear the echo of your fourth-grade teacher reminding you that it is illegal to divide by zero. Think of evanescent \\(h\\) as the vapor in the definition of “evanescent”: “tending to vanish like vapor.” This vapor is like the solvent in paint. You don’t want the solvent once the paint is on the wall; wet paint is a nuisance. But getting the paint from the container to the wall absolutely needs the solvent. 2386 We used the solvent \\(h\\) earlier in the chapter in the numerical experiments that led us to the derivatives of the pattern-book functions, for instance \\(\\partial_x e^x = e^x\\) or \\(\\partial_x \\sin(x) = \\cos(x)\\). Eventually, we’ll construct an \\(h\\)-free theory of differentiation, reducing the process to a set of algebraic rules in which \\(h\\) never appears. With this as our goal, let’s continue using \\(h\\) for a while to find some additional useful facts about derivatives. 2388 Source↩︎ "],["computing-derivs.html", "Chapter 19 Computing derivatives 19.1 A function from a function 19.2 Finite differencing 19.3 The slope-function operator 19.4 Symbolic differentiation 19.5 Exercises", " Chapter 19 Computing derivatives To differentiate a function \\(g(x)\\) means simply to produce the corresponding function \\(\\partial_t g(x)\\). This is often called “finding the derivative,” language that resonates with the high-school algebra task of “finding \\(x\\).” Rather than conjuring an image of searching high and low for a missing function, it’s more accurate to say, “compute the derivative.” 2150 In this chapter we’ll introduce two ways of computing a derivative. For simplicity we will write \\(x\\) for the with-respect-to-variable, although in practice you might be using \\(t\\) or \\(z\\) or something else. 2155 Symbolic differentiation, which uses a set of re-writing rules Finite-differencing, which is based directly on the \\({\\cal D_x}\\) operator In the days when functions were always presented using formulas, symbolic differentiation was usually the only method taught. Nowadays, when functions are just as likely to be described using data and an algorithm, finite-differencing provides the practical approach. 2160 19.1 A function from a function Recall that the goal of differentiation is to make a function out of an already known function. We’ll call the already known function \\(g(x)\\). In Chapter 17 we’ve outlined the properties that the new function should have and gave a nice naming convention, \\(\\partial_x g(x)\\) that shows where the new function comes from. In this section we’ll put that aside and focus on the question of what it means to “make a function.” 2165 When mathematics is done with paper and pencil, “making a function” is a matter of writing a formula, such as \\(x^2 \\sin(x) + 3\\) and sometimes giving a name to the formula, e.g. \\(h(x) \\equiv x^2 \\sin(x) + 3\\). We are essentially writing something down that will make sense when viewed by another person trained in the conventions of mathematical notation. 2170 For a computer, on the other hand, a function is a definite kind of thing. We “make a function” by creating that kind of thing and, usually, giving it a name. We evaluate a function—that is, apply the function to inputs to produce an output—by using specific punctuation, which in R involves the use of parentheses, for instance name(input). 2175 The computer language itself provides specific means to define a new function. In R/mosaic, you first construct a tilde expression naming the function inputs (right side of the tilde) and specifying the algorithm of that function (left side of the tilde), as with this formula: f_description &lt;- x^2 * sin(x) + 3 ~ x # a tilde expression On its own, f_description cannot be used like a function because it was constructed as something else: a tilde expression. Trying to use f_description in the way one uses a function produces an error. f_description(2) Error in f_description(2): could not find function &quot;f_description&quot; In between the tilde expression and the final result—a function—is software that translates from tilde-expressions into functions: f &lt;- makeFun(f_description) The new creation, f() can now be used like any other function, e.g. f(2) ## [1] 6.63719 Down deep inside, makeFun() uses a more basic function-creation syntax which looks like this function(x) {x^2 * sin(x) + 3} ## function(x) {x^2 * sin(x) + 3} You can see all the same information that was in the tilde description, just arranged differently. Almost every computer language provides something like function. The internal workings of function are elaborate and detailed … only advanced programmers need to be aware of them. This, in much the same way as it’s unnecessary to understand the workings of a transistor in order to use a computer, or comprehend the biochemistry of a COVID vaccine in order to benefit from it. 2180 In the same spirit as makeFun(), which translates a tilde-expression into the corresponding function, in R/mosaic you have D() which takes a tilde expression and translates it into the derivative of the function described.10 For example: D(f_description) ## function (x) ## 2 * x * sin(x) + x^2 * cos(x) 19.2 Finite differencing You can use the definition of the slope function \\[{\\cal D}_x f(x) = \\frac{f(x+0.1) - f(x)}{0.1}\\] to create an approximation to the derivative of any function. Like this: g &lt;- makeFun(sin(2*x)*(pnorm(x/3)-0.5) ~ x) dg &lt;- makeFun((g(x+0.1) - g(x))/0.1 ~ x) Whenever you calculate a derivative function, you should check against mistakes or other sources of error. For instance, whenever the derivative is zero, the original function should have an instantaneous slope of zero. Figure 19.1 shows a suitable plot for supporting this sort of check. zeros_of_dg &lt;- findZeros(dg(x) ~ x, xlim=c(-5,5)) slice_plot(g(x) ~ x, domain(x=c(-5,5)), npts=500) %&gt;% slice_plot(dg(x) ~ x, color=&quot;magenta&quot;, npts=500) %&gt;% gf_hline(yintercept = ~ 0, color = &quot;orange&quot;, size=2, alpha=0.2) %&gt;% gf_vline(xintercept = ~ x, data=zeros_of_dg, color=&quot;blue&quot;) Figure 19.1: The x-position of zero crossings of the derivative function (magenta) are marked with blue lines. The zero crossings correspond to local maxima or minima in the original function (black). This is because the original function has slope zero at maxima and minima. Look very closely at Figure 19.1, particularly at the places where the blue vertical markers cross the function \\(g(x)\\) (black). They should cross exactly at the flat zone, but they are a little shifted to the left. (You might have to zoom in on the plot to see the offset between the vertical blue marker and the local maximum of the function.) That’s the sense in which the finite-difference approach gives an approximation. The small left-shift stems from the use of 0.1 in the definition of the zero function. Use a smaller value, say 0.01 or 0.001, and you won’t be able to see the shift at all. 2185 In modeling work, there’s nothing wrong with an approximation so long as it is good enough for your purposes. We picked the value 0.1 for our definition of the slope function because it works very well with the pattern-book functions. Here, “very well” means you can’t easily see in the graph any deviation compared to the exact derivative. 2190 When a calculation can be done exactly (without outrageous effort) it certainly makes sense to use the exact method. However: It’s useful to have an easy, approximate method always at hand. This lets you check the results of other methods for the possibility of some blunder or mis-conception. The slope function approach to differentiation is certainly easy, and if you think the approximation isn’t good enough, then instead of 0.1 use something smaller. (Chapter 18 discusses how small is too small.) The computer makes it practical to employ the slope function as a useful approximation to the derivative. There are many other mathematical methods that the computer has made feasible, for instance the methods of machine learning. These methods create functions that sometimes cannot be handled by the traditional (“exact”) methods of differentiation. 2195 19.3 The slope-function operator Take a look at the statement we used to construct the slope function of g(): dg &lt;- makeFun((g(x+0.1) - g(x))/0.1 ~ x) There is almost nothing about this statement that has anything to do with the specifics of how we defined g(); we could have used any \\(g()\\). The “almost” in the previous sentence is about the choice of 0.1, which isn’t guaranteed to be small enough. 2200 In today’s world, considerable mathematical content is conveyed to users not directly with formulas but with software that implements the formulas. And in software, it’s a good idea to have a name for each operation so that the readers and authors of software have a completely explicit indication that a particular operation is being used. When you have many slope functions to compute in some application, it can be error prone to write many statements that are versions of dg &lt;- makeFun((g(x+0.1) - g(x))/0.1 ~ x) It’s too easy to make a mistake in copying this over, producing a wrong computation that can be hard to detect in the code. For instance, each of these statements looks a lot like the above, but all of them are different and none is constructing a slope function: df &lt;- makeFun((f(x-0.1) - f(x))/0.1) dh &lt;- makeFun((h(x+0.1) - f(x))/0.1) du &lt;- makeFun((u(x+0.1) - u(x))/1.0) dg &lt;- makeFun(g(x+0.1) - g(x)/0.1) Much easier to read and more reliable would be something like this: df &lt;- slopeFun(f(x) ~ x) dh &lt;- slopeFun(h(x) ~ x) du &lt;- slopeFun(u(x) ~ x) Creating such an R operator is a programming task and in that sense beyond the scope of this course. Still, it’s a good idea to get in the habit of reading programming code. So here goes … Creating a slopeFun() operator: 2205 Instead of makeFun() which is really just maknig for mathematical functions, R programmers use a construction named function(). The name of the arguments goes inside the parentheses. The function algorithm goes between curly braces: { } We’re going to use a tilde expression as the input to slopeFun(). This is how the other R/mosaic operators work. That will be easier for the user and will also give us access to those other operators if we need them in writing slopeFun(). The object returned by the slopeFun() operator will be, of course, a function. We’ve been using makeFun() to make our mathematical functions, so expect to see that in the code for slopeFun(). There’s the matter of whether 0.1 is small enough. So let’s use an h argument in place of 0.1 that we can change when needed. Putting this together, here is a slopeFun() operator that takes a tilde expression (as do makeFun() and slice_plot()) and produces a new mathematical function that is the slope function for the mathematical function described in the tilde expression. (There are a couple of R programming elements in slopeFun() that you aren’t expected to understand completely. But do try reading the code to see what sense you can make of it.) # two arguments, a tilde expression and a choice for h # with a default value slopeFun &lt;- function(tilde, h=0.1) { # Turn the tilde expression into a function g &lt;- makeFun(tilde) # just like before, with h instead of 0.1 makeFun((g(x + h) - g(x))/h ~ x, h=h) } Another important advantage of centralizing computations in a single operator is that the operator can be made more sophisticated without being harder to use. For instance, R/mosaic provides the D() operator for computing derivatives. This knows the rules for symbolic differentiation that will be introduced in Chapter 22 and switches to a finite-difference method (like slopeFun(), but more sophisticated) when symbolic differentiation isn’t applicable. In practice, instead of home-brewed functions like slopeFun(), you can use the R/mosaic D() instead, which takes a more careful approach to the computer numerics and uses symbolic differentiation whenever possible to give results without numerical error. 19.4 Symbolic differentiation Symbolic differentiation is the process of taking a formula and translating it to a new formula according to certain patterns or rules. Each rule is ultimately derived from the definition of the slope function and the differencing operator. 2215 As you recall, the differencing operator \\(\\diff{x}\\) turns a function into its slope function \\[\\diff{x} f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] 19.4.1 The line rule Let’s look at one where we already know the result: The straight line function \\(\\line(x) \\equiv a x + b\\) has a slope function that is constant: \\(\\diff{x}\\line(x) = a\\) \\[\\diff{x}\\line(x) = \\frac{\\line(x+h) - \\line(x)}{h} = \\frac{\\left[\\strut a (x+h) + b\\right] - \\left[\\strut a x + b\\right]}{h} = \\frac{ah}{h} = a\\] The derivative is the slope function with \\(h\\) made as small as possible. It’s tempting to think of this as \\(h = 0\\), but that would imply dividing by zero in the differencing operator. Being wary about the possibility of dividing by zero, mathematicians adopt a convention which indicates clearly that \\(h\\) is to be small, but not zero. This convention is marked with the notation \\(\\lim_{h \\rightarrow 0}\\), which means “as close as you can get to zero, but not zero exactly”. 2220 \\[\\partial_x \\line(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{\\line(x+h) - \\line(x)}{h} =\\\\ \\ \\\\ = \\lim_{h\\rightarrow 0} \\frac{a h}{h} = a\\] This derivation is unarguably correct for any non-zero \\(h\\). This short derivation gives us a basic differentiation rule which we can divide into 3 special cases. Line rule: \\(\\partial_x ax + b = a\\) \\(\\partial_x ax = a\\). The function \\(ax\\) is \\(\\line(x)\\) with \\(b=0\\). \\(\\partial_x b = 0\\). The function \\(b\\) is \\(\\line(x)\\) with \\(a=0\\) and thus is the constant function. \\(\\partial_x x = 1\\). The function \\(x\\) is \\(\\line(x)\\) with \\(a=1\\) and \\(b=0\\). Remember that \\(\\partial_x f(x)\\) is always a function no matter what kind of function \\(f(x)\\) is. The functions associated with the line rule are all constant functions, meaning the output doesn’t depend on the input. 19.4.2 The square rule Only for the \\(\\line()\\) function and its three special cases is the derivative a constant function. And \\(\\line()\\) is the only function for which the \\(h\\) in the differencing operator disappears on its own. For instance, consider the square function: \\(g(x) \\equiv x^2\\). 2225 \\[\\partial_x [x^2] = \\lim_{h\\rightarrow 0}\\frac{(x+h)^2 - x^2}{h} =\\\\ \\ \\\\ =\\lim_{h\\rightarrow 0}\\frac{(x^2 + 2 x h + h^2) - x^2}{h} =\\\\ \\ \\\\ = \\lim_{h\\rightarrow 0}\\frac{2 x h + h^2}{h} =\\\\ \\ \\\\ =\\lim_{h\\rightarrow 0} [2x + h]\\\\ \\] It’s accepted that the limit of a sum is the sum of the limits, so … \\[\\lim_{h\\rightarrow 0} \\left[\\strut 2 x + h\\right]= \\lim_{h\\rightarrow 0} 2x + \\lim_{h\\rightarrow 0}h\\] The limit of something not involving \\(h\\) is just that thing, giving \\[\\lim_{h\\rightarrow 0}2x = 2x\\ .\\] Finally, when an expression including \\(h\\) doesn’t involve division by \\(h\\), we can remove the \\(\\lim_{h\\rightarrow 0}\\) and just use \\(h=0\\) instead, so \\[\\lim_{h\\rightarrow 0} h = 0\\ .\\] Putting these together gives \\[\\partial_x [x^2] = 2x + \\lim_{h\\rightarrow 0}h = 2x\\] We’ll write this as another differentiation rule. Quadratic rule: \\(\\partial_x [x^2] = 2x\\) 19.4.3 The exponential rule Let’s take on the exponential function \\(h(x) \\equiv e^x\\): \\[\\partial_x e^x = \\lim_{h\\rightarrow 0}\\frac{e^{x+h} - e^x}{h} = e^x \\lim_{h\\rightarrow 0}\\left[\\frac{e^h - 1}{h}\\right]\\] At a glance, it can be hard to know what to make of \\(\\lim_{h\\rightarrow 0} (e^h-1)/h\\). Setting \\(h=0\\) in the denominator is perfectly legitimate and gives \\(e^0 - 1 = 0\\). But that still leaves the \\(h\\) in the numerator. Still, for any non-zero \\(h\\), the division is legitimate, so let’s see what happens as \\(h \\rightarrow 0\\): f &lt;- makeFun((exp(h) - 1)/h ~ h) f(0.1) ## [1] 1.051709 f(0.01) ## [1] 1.005017 f(0.001) ## [1] 1.0005 f(0.0001) ## [1] 1.00005 f(0.0000001) ## [1] 1 f(0.0000000001) ## [1] 1 Setting \\(h\\) exactly to zero, however, won’t work: it produces NaN. f(0) ## [1] NaN Since \\(\\lim_{h\\rightarrow 0} (e^h-1)/h = 1\\), we have Exponentiation rule: \\(\\partial_x e^x = e^x\\) 19.4.4 The reciprocal rule Still another example: the reciprocal function, written equivalently as \\(1/x\\) or \\(x^{-1}\\) \\[\\partial x^{-1} = \\lim_{h\\rightarrow 0}\\frac{\\frac{1}{x+h} - \\frac{1}{x}}{h} \\\\ \\ \\\\ = \\lim_{h\\rightarrow 0} \\frac{\\frac{x - (x+h)}{x(x+h)}}{h}\\\\ \\ \\\\ = \\lim_{h\\rightarrow 0}\\frac{x - x+h}{x(x+h)h} = -\\lim_{h\\rightarrow 0}\\frac{h}{x(x+h)h} \\\\ \\ \\\\ = -\\lim_{h\\rightarrow 0}\\frac{1}{x^2 + hx}\\] So long as \\(x \\neq 0\\), there is no divide-by-zero problem even when \\(h=0\\), but let’s see what the computer thinks: g &lt;- makeFun(-1/(x^2 + h*x) ~ h, x=10) g(0.1) ## [1] -0.00990099 g(0.01) ## [1] -0.00999001 g(0.001) ## [1] -0.009999 g(0.0001) ## [1] -0.0099999 g(0.0000001) ## [1] -0.01 g(0.0000000001) ## [1] -0.01 g(0) ## [1] -0.01 Setting \\(h\\) to zero in the last expression gives another differentiation rule: Reciprocal rule: \\(\\partial_x \\frac{1}{x} = -\\frac{1}{x^2}\\) 19.4.5 Power-law rule We don’t yet have the tools needed to prove a formula for the derivative of power-law functions, but we already have some instances where we know the derivative: \\(\\partial_x x^2 = 2 x\\) \\(\\partial_x x^1 = 1\\) \\(\\partial_x x^0 = 0\\) A rule that fits all these examples is \\[\\partial_x x^p = p\\, x^{p-1}\\ .\\] For instance, when \\(p=2\\) the rule gives \\[\\partial_x x^2 = 2\\, x^1 = 2 x\\] since \\(p-1\\) will be 1$. It’s not too hard to do the algebra to find the derivative of \\(x^3\\). According to the proposed rule, the derivative should be \\[\\partial_x x^3 = 3 x^2\\ .\\] Let’s check this via the definition of the derivative: \\[\\partial_x x^3 = \\lim_{h \\rightarrow 0}\\frac{(x+h)^3 - x^3}{h}\\] Direct multiplication \\((x+h)(x+h)(x+h)\\) gives \\[(x+h)^3 = x^3 + 3\\, x^2 h + 3\\, x h^2 + h^3\\] Consequently, the limit definition amounts to: \\[\\partial_x x^3 = \\lim_{h\\rightarrow 0}\\frac{3\\, x^2 h + 3\\, x h^2 + h^3}{h} = \\lim_{h\\rightarrow 0} \\left[\\strut 3\\, x^2 + 3\\,x h + h^2\\right]\\] But there is no divide-by-\\(h\\) in sight, so we can resolve \\(\\lim_{h\\rightarrow 0}\\) to \\(h=0\\), giving \\[\\partial_x x^3 = 3 x^2\\ ,\\] consistent with the proposed rule. Those familiar with the use of Pascal’s Triangle for finding the terms of binomial expansions such as \\((x + h)^p\\) will gain insight into the \\(p x^{p-1}\\) rule. 19.4.6 List of pattern-book rules We’ll save for later the derivation of the derivatives of the other pattern-book functions, but note that the gaussian function is defined to be the derivative of the sigmoidal function. Name \\(f(x)\\) \\(\\partial_x f(x)\\) exponential \\(e^x\\) \\(e^x\\) logarithm (natural) \\(\\ln(x)\\) \\(1/x\\) sinusoid \\(\\sin(x)\\) \\(\\cos(x)\\) square \\(x^2\\) \\(2x\\) proportional \\(x\\) \\(1\\) constant 1 0 reciprocal \\(1/x\\) or \\(x^{-1}\\) \\(-1/x^2\\) gaussian (hump) \\(\\dnorm(x)\\) \\(-x\\, \\dnorm(x)\\) sigmoid \\(\\pnorm(x)\\) \\(\\dnorm(x)\\) Figure 19.2: A diagram showing how differentiation connects the pattern-book functions to one another. 19.5 Exercises Exercise 19.1: kelwx The most common programming pattern in the R/mosaic calculus commands is: Operator(tilde_expression, [optional details]) Some operators: slice_plot(), contour_plot, make_Fun(), D(), antiD(), findZeros() For each of the R/mosaic expressions, determine which kind of thing is being created. Feel free to run the expressions in a SANDBOX. Question A makeFun(a*x - b ~ x) a function of x︎✘ Fair enough. But the function also has arguments a and b a function of x, a, and bRight!  a tilde expression︎✘ The tilde expression is the input to the operator. The operator translates the tilde expression into something else. a plot︎✘ a data frame︎✘ an error︎✘ Question B D(a*x - b ~ x) a function of a︎✘ a function of x, a, and bGood.  a tilde expression︎✘ a plot︎✘ a data frame︎✘ an error︎✘ Question C antiD(a*x - b ~ x) a function of a︎✘ a function of x, a, and bRight!  a tilde expression︎✘ a plot︎✘ a data frame︎✘ an error︎✘ Question D slice_plot(a*x - b ~ x, domain(x=c(0,5))) a function of x︎✘ a function of x, a, and b︎✘ a tilde expression︎✘ a plot︎✘ The expression is intended to make a plot, but it doesn’t work. Specific numerical values would need to be provided for a and b. You could have provided numerical values to a and b by assigning them in previous statements. If that’s what you had in mind, you deserve full credit. a data frame︎✘ an errorGood.  Question E f &lt;- makeFun(a*x + b ~ x, a=2, b=-4) slice_plot(f(x) ~ x, domain(x=c(0,5))) a function of x︎✘ a function of x, a, and b︎✘ a tilde expression︎✘ a plotExcellent! This works because there are specific values provided for the a and b parameters. a data frame︎✘ an error︎✘ Question F findZeros(a*x - b ~ x, domain(x=c(0,5))) a function of x︎✘ a function of x, a, and b︎✘ a tilde expression︎✘ a plot︎✘ The expression is intended to make a data frame, but it doesn’t work. Specific numerical values would need to be provided for a and b. a data frame︎✘ an errorCorrect.  Question G a*x - b ~ x a function of x︎✘ a function of x, a, and b︎✘ a tilde expressionRight!  a plot︎✘ The expression is intended to make a plot, but it doesn’t work. Specific numerical values would need to be provided for a and b. a data frame︎✘ an error︎✘ Question H f &lt;- makeFun(a*x + b ~ x, a=2, b=-4) findZeros(f(x) ~ x) a function of x︎✘ a function of x, a, and b︎✘ a tilde expression︎✘ a plot︎✘ a data frameGood.  an error︎✘ Question I Suppose you create a function in the usual way, e.g. f &lt;- makeFun(a*x + b ~ x, a=2, b=-4). Which of the following will plot a straight-line function with a slope of 5. slice_plot(f(x) ~ x, domain(x=c(-5, 5)))︎✘ The default value of a is 2, so the line would have a slope of 2. slice_plot(f(x, b=2), domain(x=c(-5, 5))︎✘ It’s a that is the slope parameter. slice_plot(f(x, a=5), domain(x=c(-5, 5))Right!  Exercise 19.2: K05LrF As you know, given a function \\(g(x)\\) it’s easy to construct a new function \\({\\cal D}_x g(x)\\) that will be an approximation to the derivative \\(\\partial_x g(x)\\). The approximation function, which we call the slope function, can be \\[{\\cal D}_x g(x) \\equiv \\frac{g(x + 0.1) - g(x)}{0.1}\\] Open a SANDBOX and use makeFun() to create a function \\(g(x) \\equiv \\sin(x)\\) and another that will be the slope function, called it slope_of_g(). g &lt;- makeFun(sin(x) ~ x) slope_of_g &lt;- makeFun( _your_tilde_expression_here ) Question A What’s the value of slope_of_g(1)?     0.3749︎✘        0.4973\\(\\heartsuit\\ \\)       1.3749︎✘        1.4973︎✘ Using your sandbox, plot both g() and slope_of_g() (in blue) on a domain \\(-5 \\leq x \\leq 5\\). This can be done with slicePlot() in the following way: slice_plot(g(x) ~ x, domain(x=c(-5,5))) %&gt;% slice_plot(slope_of_g(x) ~ x, color=&quot;blue&quot;) Question B Which of these statements best describes the graph of \\(g()\\) compared to slope_of_g()? slope_of_g() is shifted left by about \\(\\pi/2\\) compared to g(x).Nice!  slope_of_g() is shifted left by about \\(\\pi\\) compared to g(x).︎✘ slope_of_g() has a larger amplitude than g()︎✘ The output of slope_of_g() is always positive︎✘ slope_of_g() is practically the same function as g(). That is, for any input the output of the two functions is practically the same.︎✘ Exercise 19.5: qCfet3 Recall the differentiation rules for three of the pattern-book functions as presented in Section 19.4: Function name Formula Formula for derivative power-law exponent \\(p\\) Identity \\(x\\) \\(1\\) 1 Square \\(x^2\\) \\(2\\, x\\) 2 Reciprocal \\(1/x\\) \\(-1/x^2\\) -1 All three of these pattern-book functions are members of the power-law family: \\(x^p\\). They differ only in the value of \\(p\\). There is a differentiation rule for the power-law family generally. The next question offers several formulas for this rule, only one of which is correct. You can figure out which one by trying the pattern-book functions in the table above and seeing which formula gives the correct answer for the derivative. Question A Which of these formulas gives the correct differentiation rule for the power-law family \\(x^p\\)? \\(p x^{p-1}\\)Correct.  \\((p-1) x^{p+1}\\)︎✘ If this were true, the derivative of \\(x^2\\) would be \\(x^3\\). \\(x^{p-1}\\)︎✘ \\((p-1) x^{p-1}\\)︎✘ If this were true, the derivative of the identity function (\\(p=1\\)) would be 0. Exercise 19.6: ykels Although we created an R function named slopeFun() for the purposes of demonstration, it’s better to use the R/mosaic operator D() which calculates the derivative, sometimes using symbolic methods and sometimes using a finite-difference method. As an example of the use of D(), here is some more R code that defines a function f() and finds \\(\\partial_x f()\\), calling it d_f(). Then a slice plot is made of both f() and d_f(). f &lt;- makeFun(sqrt(exp(x)) - x^2 ~ x) d_f &lt;- D(f(x) ~ x) slice_plot(f(x) ~ x, domain(x=c(0, 5))) %&gt;% slice_plot(d_f(x) ~ x, color = &quot;orange3&quot;) ::: For each of the following functions, write a brief comparison of the function to it’s differenced version. You can combine phrases such as “same shape”, “different shape. larger in amplitude”, “smaller in amplitude”, “same period”, “shorter period”, “longer period”, or whatever seems appropriate. For instance, for the original example in the sandbox, a reasonable comparison might be, “f() is concave down but Diff(f) is concave up.” Essay question tmp-1: A. For the function \\(f(x) \\equiv 3 x\\), compare \\(f()\\) to \\(\\partial_x f\\). Essay question tmp-2: B. For the function \\(f(x) \\equiv x^2\\), compare \\(f()\\) to \\(\\partial_x f\\). Essay question tmp-3: C. For the function \\(f(x) \\equiv e^x\\), compare \\(f()\\) to \\(\\partial_x f\\). Essay question tmp-4: D. For the function \\(f(x) \\equiv e^{-0.3 x}\\), compare \\(f()\\) to \\(\\partial_x f\\). Essay question tmp-5: E. For the function \\(f(x) \\equiv \\sin(x)\\), compare \\(f()\\) to \\(\\partial_x f\\). Essay question tmp-6: F. For the function \\(f(x) \\equiv \\sin(2 \\pi x)\\), compare \\(f()\\) to \\(\\partial_x f\\)). Essay question tmp-7: G. For the function \\(f(x) \\equiv \\sin(\\frac{2 \\pi}{20} x)\\), compare \\(f()\\) to \\(\\partial_x f\\)). Mathematically, one differentiates a function, not a tilde expression. But differentiation requires specification of the with-respect-to-input, so R/mosaic implements differentiation by using tilde-expression syntax.↩︎ "],["concavity-curvature.html", "Chapter 20 Concavity and curvature 20.1 Quantifing concavity and curvature 20.2 Concavity 20.3 Curvature 20.4 Exercises", " Chapter 20 Concavity and curvature Looking at the graph of a function, our eyes immediately register the slope at any point we focus on. A glance shows whether the slope at that point is positive or negative. Comparing the slopes at two locales is also an automatic visual task: most people have little difficulty saying which slope is steeper. One consequence of this visual ability is that it’s easy to recognize whether a line that touches the graph at a point is tangent to the graph. 2230 There are other aspects of functions, introduced in Section ??, that are also readily discerned from a glance at the function graph. 2235 Concavity: We can tell within each locale whether the function is concave down, concave up, or not concave. Curvature: Generalizing the tangent line capability a bit, we can do a pretty good job of eyeballing the tangent circle recognizing whether a circle has much too large or much too small a radius.. Smoothness: We can distinguish smooth functions from non-smooth ones. Or, as you will see, there are some kinds of smoothness that we can discern and others that are not apparent to the eye. The following exercises are simply meant to test your visual acuity in spotting concavity, tangency, and smoothness. Then we’ll move on to the calculations involved. 20.1 Quantifing concavity and curvature It often happens in building models that the modeler (you!) knows something about the concavity and/or curvature of a function. For example, concavity is important in classical economics; the curve for supply is concave down while the curve for demand is concave up. For a train, car, or plane, there are forces that depend on the curvature of the track, road, or trajectory. If you are designing a road, you’ll need to calculate the curvature in order to know if the road is safe at the indicated speed. It turns out that quantifying these properties of functions or shapes is naturally done by calculating derivatives. 2240 Imagine designing a highway. Due to the terrain, part of the road is oriented east-west and another part north-south. For vehicles to use the road, those two parts need to be connected together! (In math-speak, we might say that the road has to be continuous, but this is just common sense.) From your experience with highways, you know the connection will be a smooth curve. If the curve is part of a circle, then the design needs to specify the radius of curvature of the circle. Too tight a radius and the traffic won’t be able to handle the centrifugal force and will drift or skid off the road. A big radius is needed for safety, but making the radius bigger than required adds additional cost to road construction. It’s not as simple as finding the radius of the curve. The radius needs to change at the entry and exit of the curve. Why? Here’s an explanation from the American Association of State Highway and Transportation Officials *Policy on Geometric Design of Highways and Streets (1994): Any motor vehicle follows a transition path as it enters or leaves a circular horizontal curve. The steering change and the consequent gain or loss of centrifugal force cannot be effected instantly. For most curves the average driver can effect a suitable transition path within the limits of normal lane width. However, with combinations of high speed and sharp curvature the resultant longer transition can result in crowding and sometimes actual occupation of adjoining lanes. In such instances transition curves would be appropriate because they make it easier for a driver to confine the vehicle to his or her own lane. The employment of transition curves between tangents and sharp circular curves and between circular curves of substantially different radii warrants consideration. Later in this chapter, you’ll see the calculus concepts that relate to designing a road with a gently changing curvature. (Hint, but don’t get scared: It’s the third derivative, not the first or the second.) Let’s frame the calculations in terms of a function \\(f(x)\\). Depending on the setting, \\(x\\) might be the price of a product and \\(f(x)\\) the demand for that product. Or the graph of \\(f(x)\\) might be the path of a road drawn in \\((x,y)\\) coordinates or the reach of a robot arm as a function of time. Remember that \\(f()\\) is just a pronoun that I’m using instead of a proper descriptive name. I use such pronouns (also, \\(g()\\), \\(h()\\), the “she” and “he” of mathematical language) when writing about the general properties of functions. 20.2 Concavity Recall that to find the slope of a function \\(f(x)\\) at any input \\(x\\), you compute the derivative of that function, which we’ve been writing \\(\\partial_x\\,f(x)\\). Plug in some value for the input \\(x\\) and the output of \\(\\partial_x\\, f(x)\\) will be the slope of \\(f(x)\\) at that input. (Chapter 19 introduced some techniques for computing the derivative of any given function.) Now we want to show how differentiation can be used to quantify the concavity of a function. It will help if we augment our nomenclature a bit. When we speak of the “derivative” of a function, we mean something that might be more completely expressed as the first derivative of the function. Just that name naturally suggests that there will be a second derivative, a third derivative, and so on. Figure 20.1 shows a simple function that is concave down. Figure 20.1: A function that is concave down. Notice that the concavity is not about the slope. The curve in Figure 20.1 is concave down everywhere in the domain \\(0 \\leq x \\leq 4\\), but the slope is positive for \\(0 \\leq x \\leq 1\\) and negative for larger \\(x\\). Slope and concavity are two different aspects of a function. As introduced in Chapter ??, the concavity of a function depends not on the slope, but on the change in the slope. Figure 20.2 adds some annotations on top of the graph in Fig. 20.1. In the subdomain marked A, the function slope is positive while in the subdomain B, the function slope is negative. It is this transition from the slope in A to the slope in B that corresponds to the concavity of the function between A and B. ## Warning in validate_domain(domain, free_args): Missing domain names: x Figure 20.2: Convexity is about how the slope changes from one place in the domain to another. Similarly, the concavity of the function between B and C, reflects the transition in the slope from B to C. Even though the slope is negative in both B and C, the change in slope tells us about the concavity. Let’s look at this using symbolic notation. Keep in mind that the function graphed is \\(f(x)\\) while the slope is the function \\(\\partial_x\\,f(x)\\). We’ve seen that the concavity is indicated by the change in slope of \\(f()\\), that is, the change in \\(\\partial_x\\, f(x)\\). We’ll go back to our standard way of describing the rate of change near an input \\(x\\): \\[\\text{concavity.of.f}(x) \\equiv\\ \\text{rate of change in}\\ \\partial_x\\, f(x) = \\partial_x [\\partial_x f(x)] \\\\ \\\\ = \\lim_{h\\rightarrow 0}\\frac{\\partial_x f(x+h) - \\partial_x f(x)}{h}\\] We’re defining the concavity of a function \\(f()\\) at any input \\(x\\) to be \\(\\partial_x [\\partial_x f(x)]\\). We create the concavity_of_f(x) function by applying differentiation twice to the function \\(f()\\). Such a double differentiation of a function \\(f(x)\\) is called the second derivative of \\(f(x)\\). The second derivative is so important in applications that it has it’s own compact notation: \\[\\text{second derivative of}\\ f()\\ \\text{is written}\\ \\partial_{xx} f(x)\\] Look carefully to see the difference between the first derivative \\(\\partial_x f(x)\\) and the second derivative \\(\\partial_{xx} f(x)\\): it’s all in the double subscript \\(_{xx}\\). Computing the second derivative is merely a matter of computing the first derivative \\(\\partial_x f(x)\\) and then computing the (first) derivative of \\(\\partial_x f(x)\\). In R this process looks like: dx_f &lt;- D( f(x) ~ x) # First deriv. of f() dxx_f &lt;- D(dx_f(x) ~ x) # Second deriv. of f() As a shortcut for the two-step process above, for the second derivative you can use a notation which doubles up on the x on the right-hand side of the tilde: dxx_f &lt;- D(f(x) ~ x &amp; x) 20.3 Curvature As you see from Section 20.2, it’s easy to quantify the concavity of a function \\(f(x)\\): just evaluate the second derivative \\(\\partial_{xx} f(x)\\). But it turns out that people are very poor at estimating the quantitative value of concavity by eye. To illustrate, consider the square function, \\(f(x) \\equiv x^2\\). (See Figure 20.3.) Figure 20.3: Does the concavity of the square function vary with \\(x\\)? Clearly, the square function is concave up. Now a test: Looking at the graph of the square function, where is the concavity the largest? Don’t read on until you’ve pointed where you think the concavity is largest. With your answer to the test question in mind, let’s calculate the concavity of the square function using derivatives. \\[f(x) \\equiv x^2\\ \\text{ so }\\ \\partial_x f(x) = 2 x\\ \\text{ and therefore }\\ \\partial_{xx} f(x) = 2\\] The second derivative of \\(f(x)\\) is positive, as you would expect for a function that is concave up. What you might not expect, however, is that the second derivative is constant. The concavity-related property that the human eye reads from the graph of a function is not the concavity itself, but the curvature of the function. The curvature of \\(f(x)\\) at a point \\(x_0\\) is defined to be the radius of the circle that is tangent to the function at \\(x_0\\). Figure 20.4 illustrates the changing curvature of \\(f(x) \\equiv x^2\\) by inscribing tangent circles at several points on the function graph, marked with dots. You can see the tangency of the circle to the function graph; the function’s thin black line goes right down the middle of the broader lines used to draw the circles. Figure 20.4: At any point on the graph of a smooth function, a circle tangent to the graph can be drawn. The radius of this circle is \\(1/{\\cal K}\\). Black dots have been put along the graph at the points where the graph of the function is tangent to the inscribed circle. The visual sign of tangency is that the graph of the function goes right down the center of the circle. The inscribed circle at \\(x=0\\) is tightest. The circle at \\(x=1\\) has a somewhat larger radius. The radius of the circle at \\(x=-1.5\\) is the largest of all. Whereas the concavity is the same at all points on the graph, the visual impression that the function is most highly curved near \\(x=0\\) is better captured by the radius of the inscribed circle. The radius of the inscribed circle at any point is the reciprocal of a quantity \\({\\cal K}\\) called the curvature. The curvature \\({\\cal K}\\) of a function \\(f(x)\\) depends on both the first and second derivative. The formula for curvature \\(K\\) is somewhat off-putting; you are not expected to memorize it. But you can see where \\(\\partial x f()\\) and \\(\\partial_{xx}f()\\) come into play. \\[{\\cal K}_f \\equiv \\frac{\\left|\\partial_{xx} f(x)\\right|}{\\ \\ \\ \\ \\left|1 + \\left[\\strut\\partial_x f(x)\\right]^2\\right|^{3/2}}\\] Mathematically, the curvature \\(\\cal K\\) corresponds to the reciprocal of the radius of the tangent circle. When the tangent circle is tight, \\(\\cal K\\) is large. When the tangent circle has a very large radius, that is, the function is very close to approximating a straight line, \\(\\cal K\\) is very small. 2245 Returning to the highway design example earlier in the chapter … The Policy on geometric design of highways and streets called for the curvature of a road to change gently, giving the driver time to adjust the steering and accomodate the centrifugal force of the car going around the curve. Changing curvature implies that \\(\\partial_x {\\cal K}\\) is non-zero. Since \\({\\cal K}\\) depends on the first and second derivatives of \\(f(x)\\), the Policy on gradual change means that the third derivative of \\(f(x)\\) is non-zero. 20.4 Exercises Exercise 20.1: ykIBCb ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A Glance at the graph. In which boxes is the slope negative?     A, B, C︎✘        B, C, D︎✘        A, C, D\\(\\heartsuit\\ \\) Exercise 20.2: QnpuMN ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A Consider the slope of the function in the domains marked by the boxes. What is the order of boxes from least steep to steepest?     A, B, C︎✘        C, A, B︎✘        A, C, B\\(\\heartsuit\\ \\)       none of these︎✘ Exercise 20.3: kq3t86 ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A Which of the line segments is tangent to the curve at the point marked with a dot?     A︎✘        B︎✘        C︎✘        all of them\\(\\heartsuit\\ \\)       none of them︎✘ Exercise 20.4: BVRjhF ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A Which of the line segments is tangent to the curve at the point marked with a dot?     A\\(\\heartsuit\\ \\)       B︎✘ too shallow       C︎✘ too steep       all of them︎✘        none of them︎✘ Exercise 20.5: 5ddB5r ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A In which of the boxes is the function concave up?     A and E\\(\\heartsuit\\ \\)       B and D︎✘        C and D︎✘ Exercise 20.6: 4nsU0z ## Warning in validate_domain(domain, free_args): Missing domain names: x Question A In which boxes is the function smooth?     A and B︎✘        B and C︎✘        A and C\\(\\heartsuit\\ \\)       none of them︎✘        all of them︎✘ ## Warning in validate_domain(domain, free_args): Missing domain names: x Question B In which boxes is the function smooth?     A and B︎✘        B and C︎✘        A and C︎✘        none of them︎✘        all of them\\(\\heartsuit\\ \\) ## Warning in validate_domain(domain, free_args): Missing domain names: x Question C In which boxes is the function smooth?     A︎✘        B\\(\\heartsuit\\ \\)       neither of them︎✘        both of them︎✘ Exercise 20.7: iIXOYZ We introduced concavity graphically and used the terms “concave up” and “concave down.” Now we can compute the concavity quantitatively using the second derivative. In a sandbox, create this function and plot it. (Note: rfun() generates random functions in the same way you might by moving a pencil smoothly on a piece of paper. The seed = 8427 effectively chooses which one of infinitely many functions is being generated. Different seeds give different functions. ) f &lt;- rfun( ~ z, seed = 8427) slice_plot(f(x) ~ x, domain(x=c(-5,5))) You can see that in the region near \\(x = -1\\) the function is concave down. While near \\(x=2.5\\) the function is concave up. In your sandbox, compute the second derivative of \\(f(x)\\) and evaluate it at \\(x=-1\\) and \\(x=2.5\\). dxx_f &lt;- D(f(x) ~ x &amp; x) dxx_f(-1) dxx_f(2.5) Using these results, and perhaps experimenting a little with different values of \\(x\\), you should be able to answer this question: Question A Which of these is a correct statement of “concave up” in terms of the value of \\(\\partial_{xx} f(x)\\)? A function is concave-up at input \\(x_0\\) when \\(\\partial_{xx} f(x_0) &gt; 0\\)Nice!  A function is concave-up at input \\(x_0\\) when \\(\\partial_{xx} f(x_0) &lt; 0\\)︎✘ A function is concave-up at input \\(x_0\\) when \\(\\partial_{xx} f(x_0) &lt; 0\\) and \\(\\partial_x f(x_0) &lt; 0\\)︎✘ The first derivative has nothing to do with it. A function is concave-up at input \\(x_0\\) when \\(\\partial_{xx} f(x_0) &gt; 0\\) and \\(\\partial_x f(x_0) &gt; 0\\)︎✘ The first derivative has nothing to do with it. Recall that an inflection point is a value for the input \\(x\\) at which \\(f(x)\\) changes from concave up to concave down, or vice versa. Add a statement to your sandbox to graph \\(\\partial_{xx} f(x)\\). Question B From reading the graph of \\(\\partial_{xx} f(x)\\), say which of these is nearest to an inflection point for \\(f(x)\\). \\(x = 0.0\\)︎✘ \\(x = -4.0\\)Nice! The inflection point nearest \\(x=-4\\) occurs at \\(x = -4.156\\). \\(x = 2.5\\)︎✘ \\(x = -3\\)︎✘ Question C How many inflection points are there for \\(f(x)\\) in the domain \\(-5 \\leq x \\leq 5\\)? 1︎✘ 2︎✘ 3Excellent! You can see this by graphing \\(\\partial_{xx} f(x)\\) and counting the zero crossings. 4︎✘ 5︎✘ Exercise 20.8: bHacc6 The graph of the function \\(g(x) \\equiv \\sqrt{\\strut R^2 - x^2}\\) has the shape of a semi-circle of radius \\(R\\), e.g. g &lt;- makeFun(sqrt(R^2 - x^2) ~ x, R=2) slice_plot(g(x) ~ x, domain(x=c(-2, 2)), npts=300) Intuition suggests that the radius of an enscribed circle for \\(g()\\) should match the radius of the graph of the function. In a SANDBOX, create a function to calculate the curvature of \\(g()\\) at any input \\(x\\). Then plot that curvature function over the domain \\(-2 &lt; x &lt; 2\\). Is the curvature of \\(g()\\) indeed constant? To help you get started, here is a bit of scaffolding for your sandbox. g &lt;- makeFun(sqrt(R^2 - x^2) ~ x, R = 2) # define g() dg &lt;- D(g(x) ~ x) # first derivative of g() ddg &lt;- D(g(x) ~ x &amp; x) # second derivative of g() curvature &lt;- makeFun(abs(ddg(x)) / abs(__fill_in_the_formula__)^(3/2) ~ x) slice_plot(curvature(x) ~ x, domain(x=c(-2, 2))) We set the default value of the parameter \\(R\\) to be 2. Question A What is the curvature of \\(g(x)\\)?     0︎✘                0.5\\(\\heartsuit\\ \\)       1︎✘                1.5︎✘               2︎✘ Exercise 20.9: jyXhPM Here is a graph of \\(\\sin(x)\\) with points marked at \\(x=-\\pi/2\\), \\(x=0.923\\), and \\(x = \\pi/2\\). At each of those points, an inscribed circle has been drawn, tangent to the function at that point. You’re task is to calculate the curvature \\(\\cal K\\) at each of those three input points. This is a matter of calculating the first and second derivatives of the sine function, evaluating those derivatives at the input values, and plugging them in to the formula in Section 20.3. Question A What is the curvature \\(\\cal K\\) of \\(\\sin(x=-\\pi/2)\\)?     -1︎✘ \\(\\cal K\\) can be negative!       0︎✘                     0.5︎✘                       1\\(\\heartsuit\\ \\)            2︎✘ Question B What is the curvature \\(\\cal K\\) of \\(\\sin(x=-0.923)\\)?     -1︎✘ \\(\\cal K\\) can be negative!       0︎✘                     0.5\\(\\heartsuit\\ \\)           1︎✘             2︎✘ Question C What is the curvature \\(\\cal K\\) of \\(\\sin(x=\\pi/2)\\)?     -1︎✘ \\(\\cal K\\) can be negative!       0︎✘                     0.5︎✘            1\\(\\heartsuit\\ \\)            2︎✘ Question D What is the curvature \\(\\cal K\\) of \\(\\sin(x=0)\\)? (Hint: You can tell straight from the graph, even though no enscribed circle has been drawn. 0Excellent! The graph is straight at \\(x=0\\), so no curvature 0.5︎✘                     1︎✘                                2︎✘ Exercise 20.10: rou0yO The function road(x) has been constructed to correspond to a curved road of gradually tighter radius from left to right R &lt;- makeFun(3 - x/2 ~ x) road &lt;- makeFun(sqrt(R(x)^2 - x^2) ~ x) slice_plot(road(x) ~ x, domain(x=c(-2, 2)), npts=500) Using a SANDBOX, calculate the curvature of this road for each value of \\(x\\). Question A What is the curvature of the road at \\(x=-1\\)?     0.22\\(\\heartsuit\\ \\)       0.24︎✘                     0.27︎✘                     0.31︎✘                     0.35︎✘ Question B What is the curvature of the road at \\(x=1\\)?     0.22︎✘                     0.24︎✘                     0.27\\(\\heartsuit\\ \\)       0.31︎✘                     0.35︎✘ Question C What is the curvature of the road at \\(x=0\\)?     0.22︎✘                     0.24\\(\\heartsuit\\ \\)       0.27︎✘                     0.31︎✘                     0.35︎✘ "],["cont-and-smooth.html", "Chapter 21 Continuity and smoothness 21.1 Continuity 21.2 Discontinuity 21.3 Smoothness 21.4 Exercises", " Chapter 21 Continuity and smoothness You’ve seen how various properties of a function—whether it is monotonic, how it slopes, whether it is concave up or down (or not at all), curvature, etc.—can be related to the first and second derivatives of the function. 2300 In this chapter, we’ll elaborate on continuity, one of the ideas introduced in Section ??, and use the concept of continuity to characterize functions in a new way: their smoothness. 2305 21.1 Continuity The intuition behind continuity is simple: If you can draw the graph of a function without lifting the pencil from the paper, the function is continuous. Continuity can be an important attribute of a modeling function. Often, we are modeling phenomena where a small change in input is expected to produce a small change in output. For instance, if your income changes by one penny, you would expect your lifestyle not to change by much. If the temperature of an oven changes by 1 degree, you don’t expect the quality of the cake you are baking to change in any noticeable way. 2310 All of our basic modeling functions are continuous over their entire input domain.11 To illustrate discontinuity we’ll consider piecewise functions, as introduced in Chapter ??. The Heaviside function, graphed in Figure 21.1 is discontinuous. 2315 Figure 21.1: The Heaviside function is piecewise constant with a discontiuity at \\(x=0\\). Drawing the graph of the Heaviside function \\(H(x)\\) involves lifting the pencil at \\(x=0\\). In contrast, the piecewise ramp function (Figure 21.2) is continuous; you don’t need to lift the pencil from the paper in order to draw the ramp function. Figure 21.2: The ramp function is a continuous piecewise function. Imagine that you were constructing a model of plant growth as a function of the amount of water (in cc) provided each day. The plant needs about 20 cc of water to thrive. Let’s say that you use the Heaviside function for the model, say \\(H(W-20)\\), where an output of 1 means the plant thrives and a output 0 means the plant does not. The model implies that if you provide 20.001 cc of water, the plant will thrive. But if you are stingy, and provide only 19.999 cc of water, the plant will die. In other words, a very small change in the input can lead to a large change in the output. 2320 Common sense suggests that a change of 0.002 cc in the amount of water—that’s a small fraction of a drop, 2 cubic millimeters of volume, is not going to lead to a qualitative change in output. So you might prefer to use a sigmoidal function as your model rather than a Heaviside function. 2325 On the other hand, sometimes a very small change in input does lead to a large change in output. For instance, a sensible model of the hardness of water as a function of temperature would include a discontinuity at \\(32^\\circ\\)F, the temperature at which water turns to ice. 2330 One of author Charles Dickens’s famous characters described the relationship between income, expenditure, and happiness this way: \"Annual income 20 pounds, annual expenditure 19 [pounds] 19 [shillings] and six [pence], result happiness. Annual income 20 pounds, annual expenditure 20 pounds ought and six, result misery.\" — the character Wilkins Micawber in David Copperfield Macawber was referring to the common situation in pre-20th century England of putting debtors in prison, regardless of the size of their debt. Macawber’s statement suggests he would model happiness as a Heaviside function \\(H(\\text{income}- \\text{expenditure})\\). 2335 Whenever the output of a function is a binary (yes-or-no) value, you can anticipate that a model will involve a discontinuous function. 21.2 Discontinuity Recall the logical path that led us to the idea of the derivative of a function. We started with the differencing operator, which takes as input a function and a “small” value of \\(h\\): \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] Then, through algebraic manipulation and numerical experiments we found that, once \\(h\\) is small enough, the graph of the slope function \\({\\cal D}_x f(x)\\) does not depend on \\(h\\). And so we defined a function \\(\\partial_x f(x)\\) where \\(h\\) doesn’t play a role, writing \\(\\lim_{h\\rightarrow 0}\\) to remember our care to never actually divide by zero. \\[\\partial_x f(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\ .\\] Conveniently, we found that the derivatives of the pattern-book functions can be written in terms of the pattern-book functions without making any reference to \\(h\\). For instance: \\(\\partial_x \\ln(x) = 1/x\\) No \\(h\\) appears. \\(\\partial_x e^x = e^x\\) No \\(h\\) appears \\(\\partial_x x^p = p\\, x^{p-1}\\) No \\(h\\) appears. and so on. With discontinuous functions, we have no such luck. Figure 21.3 shows what happens if we compute \\({\\cal D}_x H(x)\\), the derivative of the Heaviside function, for smaller and smaller \\(h\\). H &lt;- makeFun(ifelse(x &gt;=0, 1, 0) ~ x) DH01 &lt;- makeFun((H(x + 0.1) - H(x))/0.1 ~ x) DH001 &lt;- makeFun((H(x + 0.01) - H(x))/0.01 ~ x) DH0001 &lt;- makeFun((H(x + 0.001) - H(x))/0.001 ~ x) slice_plot(DH01(x) ~ x, domain(x=c(-0.02, 0.02)), npts=500, color=&quot;red&quot;, size=2) %&gt;% slice_plot(DH001(x) ~ x, color=&quot;darkgreen&quot;, npts=500, size=3, alpha=0.5) %&gt;% slice_plot(DH0001(x) ~ x, color=&quot;blue&quot;, npts=500, alpha=0.5, size=2) Figure 21.3: \\({\\cal D}_x H(x)\\), the slope function of the discontinuous Heaviside, function, depends on the value of \\(h\\) used for the slope function. (Red: \\(h=0.1\\); Green: \\(h=0.01\\); Blue \\(h=0.001\\)) Differencing the Heaviside function produces very different functions depending on the value of \\(h\\). The bump near \\(x=0\\) gets taller and taller as \\(h\\) gets smaller. Mathematicians would describe this situation as \\[\\lim_{h\\rightarrow0}{\\cal D}_x H(x=0) \\equiv \\lim_{h\\rightarrow 0} \\frac{H(0+h) - H(0)}{h}\\ \\ \\ \\text{does not exist}.\\] Of course, for any given value of \\(h\\), e.g. \\(h=0.000001\\), the function \\({\\cal D}_x H(x)\\) has a definite shape. But that shape keeps changing as \\(h \\rightarrow 0\\), so we can’t point to any specific shape as the “limit as \\(h \\rightarrow 0\\).” Since there is no convergence in the shape of \\({\\cal D}_x H(0)\\) as \\(h\\) gets smaller, it’s fair to say that the Heaviside function does not have a derivative at \\(x=0\\). But away from \\(x=0\\), the Heaviside function has a perfectly sensible derivative: \\(\\partial_x H(x) = 0\\) for \\(x\\neq 0\\). But there is no derivative at \\(x=0\\). 2365 21.3 Smoothness Smoothness is a different concept than continuity, although the two are related. Most simply, any discontinuous function is not smooth at any input where a discontinuity occurs. But even the continuous ramp function is not smooth at the start of the ramp. Intuitively, imagine you were sliding your hand along the ramp function. You would feel the crease at \\(x=0\\). 2340 A function is not smooth if the derivative of that function is discontinuous. For instance, the derivative of the ramp function is the Heaviside function, so the ramp is not smooth at \\(x=0\\). All of our basic modeling functions are smooth everywhere in their domain. In particular, the derivatives of the basic modeling functions are continuous, as are the second derivative, third derivative, and so on down the line. Such functions are called C-infinity, written \\(C^\\infty\\). The superscript \\(\\infty\\) means that every order of derivative is continuous. 2345 Figure 21.4: A function whose derivative is the ramp function (hence continous) and whose second derivative is the Heaviside function (discontinous). Since the first derivative is continuous, this function has \\(C^1\\) smoothness. You cannot tell from the plot that the second derivative is discontinuous. But if you were in a plane flying along that trajectory, you would feel a jerk as you crossed \\(x=0\\). Mathematicians quantify the “smoothness” of a function by looking at the continuity of the function and its derivatives. The mathematical definition of smoothness is straightforward and phrased in terms of derivatives. Suppose you are examining the smoothness of a function \\(f(x)\\). The smoothness is assessed on a scale \\(C^0, C^1, C^2, \\ldots, C^\\infty\\). \\(C^0\\): the function \\(f()\\) is continuous. Intuitively, this means that a graph of the function can be drawn without lifting the pencil from the paper. \\(C^1\\): the function \\(f()\\) has a derivative over its entire domain and that derivative \\(\\partial_x f(x)\\) is continuous. (See Figure 21.4 for an example.) \\(C^2\\): the function \\(\\partial_x f(x)\\) has a derivative over its entire domain and that derivative is continuous. In other words, \\(\\partial_{xx} f(x)\\) exists and is continuous. \\(C^n\\): Like \\(C^2\\), but we’re talking about the \\(n\\)th-derivative of \\(f(x)\\) existing and being continuous. \\(C^\\infty\\): Usually when we denote a sequence with an infinite number of terms, we write down something like \\(C^0, C^1, C^2, \\ldots\\). It would be entirely valid to do this in talking about the \\(C^n\\) sequence. But many of the mathematical functions we work with are infinitely differentiable, that is \\(C^\\infty\\). Examples of \\(C^\\infty\\) functions: \\(\\sin(x)\\): the derivatives are \\(\\partial_x \\sin(x) = \\cos(x)\\), \\(\\partial_{xx} \\sin(x) = -\\sin(x)\\), \\(\\partial_{xxx} \\sin(x) =-\\cos(x)\\), \\(\\partial_{xxxx} \\sin(x) =\\sin(x)\\), … You can keep going infinitely. \\(e^x\\): the derivatives are \\(\\partial_x e^x = e^x\\), \\(\\partial_{xx} e^x = e^x\\), and so on. \\(x^2\\): the derivatives are \\(\\partial_x x^2 = 2 x\\), \\(\\partial_{xx} x^2 = 2\\), \\(\\partial_{xxx} x^2 = 0\\), … Higher order derivatives are all simply 0. Boring, but still existing. Example of non-\\(C^2\\) functions: We see these often when we take two or more different \\(C^\\infty\\) functions and split their domain, using one function for one subdomain and the other(s) for other subdomain(s). \\(|x|\\), the absolute value function. \\(|x|\\) is a pasting together of two \\(C^\\infty\\) functions: \\[|x| \\equiv \\left\\{\\begin{array}{rcl}+x &amp; \\text{for} &amp; 0 \\leq x\\\\-x&amp;\\text{for}&amp; \\text{otherwise}\\end{array} \\right.\\ .\\] The domain is split at \\(x=0\\). For engineering and design problems, smoothness means something substantially different than described by the mathematical concepts above. Later in the course we’ll introduce cubic splines which are continuous functions defined by a finite set of coordinate pairs, as in a data frame. Each line of the data frame corresponds to a dot in a scatter plot, but in a cubic spline it is called a “knot point.” The spline consists of cubic polynomials drawn between consecutive knot points. The domain is split at each of the knot points. Between any two adjacent knot points, the function is an ordinary cubic polynomial. At a knot point, the cubics on either side have been arranged to have their first and second derivatives match. Thus, the first two derivatives are continuous. The function is at least \\(C^2\\). The second derivative of a cubic is a straight-line function, so the second derivative of a cubic spline is a series of straight-line functions connected at the knot points. The second derivative does not itself have a derivative at the knot points. So, a cubic spline cannot satisfy the requirements for being \\(C^3\\); it is \\(C^2\\). 21.4 Exercises Exercise 21.1: yJKLEb Draw the graph of a function from your imagination over the domain \\(-5 \\leq x \\leq 5\\). The function should be continuous everywhere except at \\(x = -2, 1, 3\\). Draw the graph of a second function from your imagination over the same domain as in (1). The second function should be continuous everywhere in domain: no discontinuities. It should also have a derivative everywhere except at \\(x = -2, 1, 3\\). Exercise 21.2: 4Iv7t For the sketched functions below, decide what level of smoothness—\\(C^0, C^1, C^2, ...\\)— best describes the function. (We make no tricks in the drawings. Where a function looks like it’s broken–that is, the function locally has a V-shape or a \\(\\Lambda\\)-shape–it is meant to be broken.) Question A What’s the smoothness level of function A(x)? (Hint: A quadratic function has a first derivative that changes with x but a second derivative that is constant for all x.)     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)\\(\\heartsuit\\ \\)       \\(C^2\\) or higher︎✘ Question B What’s the smoothness level of function B(x)?     discontinuous\\(\\heartsuit\\ \\)       \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\) or higher︎✘ Question C What’s the smoothness level of function C(x)?     discontinuous︎✘        \\(C^0\\)\\(\\heartsuit\\ \\)       \\(C^1\\)︎✘        \\(C^2\\) or higher︎✘ Question D What’s the smoothness level of function D(x)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\) or higher\\(\\heartsuit\\ \\) Exercise 21.3: UNeewh The ramp function is defined algebraically as \\[\\text{ramp}(x) \\equiv \\left\\{\\begin{array}{ll}0 &amp; \\text{for}\\ x &lt; 0\\\\ x &amp; \\text{otherwise} \\end{array}\\right. \\] or in R as ramp &lt;- makeFun(ifelse(x&lt;0, 0, x) ~ x) Evaluate these three different forms for the definition of the instantaneous rate of change at \\(x=0\\) using \\(h=0.1\\). version 1: \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] version 2: \\[{\\cal D}_x f(x) \\equiv \\frac{f(x) - f(x-h)}{h}\\] version 3: \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x-h)}{2 h}\\] Question A Do the three versions give different numerical results at \\(x=0\\) for \\(h=0.01\\)? They all give the same result.︎✘ Versions 1 and 3 give the same result, but 2 is different.︎✘ Versions 1 and 2 give the same result, but 3 is different.︎✘ All three are differentCorrect.  Question B For much smaller \\(h\\) (say, \\(h=0.0001\\)), do the three versions give different numerical results at \\(x=0\\)?     Yes\\(\\heartsuit\\ \\)       No︎✘ Exercise 21.4: jG4rie Consider this function, defined piecewise: Write the R command to create this function. (Hint: Remember ifelse from Chapter ??.) Using a SANDBOX, plot \\(h(x)\\) over the domain \\(-1 \\leq x \\leq 1\\), then sketch a copy of the graph on your paper. Create the function \\(\\partial_x h(x)\\) by differentiating separately each piece of the piecewise function \\(h()\\). Write down \\(\\partial_x h(x)\\) using mathematical notation similar to the definition of \\(h(x)\\) given above. Sketch a graph of \\(\\partial_x h(x)\\) over the domain \\(-1 \\leq x \\leq 1\\). You’re welcome to use a SANDBOX, but you may be able to figure out the shape of the graph yourself. The shape of the function you sketched in (4) has a name, given in the text in Section 21.1. What is that name? Now you are going to do much the same as in items (3), (4), and (5), but instead of the first derivative \\(\\partial_x h(x)\\), create, sketch, and name the second derivative \\(\\partial_{xx} h(x)\\). Create and write down \\(\\partial_{xx} h(x)\\) in mathematical notation. Sketch \\(\\partial_{xx} h(x)\\) Classify the smoothness of \\(h(x)\\) using the following table: Smoothness Criterion \\(C^0\\) \\(\\partial_x h(x)\\) is discontinuous \\(C^1\\) \\(\\partial_x h(x)\\) is continuous \\(C^2\\) \\(\\partial_{xx} h(x)\\) is continuous \\(C^3\\) \\(\\partial_{xxx} h(x)\\) is continuous \\(\\vdots\\) and so on. \\(C^\\infty\\) All orders of derivative of \\(h(x)\\) are continous. Exercise 21.5: 2beqIv Consider the following functions \\(f_A(x)\\), \\(f_B(x)\\), … all of which involve a domain split at \\(x=0\\) and the pasting together of two individually \\(C^\\infty\\) functions: Question A How smooth is \\(f_A(x)\\)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)\\(\\heartsuit\\ \\)       \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ Question B How smooth is \\(f_B(x)\\)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\)\\(\\heartsuit\\ \\)       \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ \\[f_C(x) \\equiv \\left\\{\\begin{array}{cl}{x^3} &amp; \\text{for}\\ 0 \\leq x\\\\x^3&amp; \\text{otherwise}\\end{array} \\right.\\] Question C How smooth is \\(f_C(x)\\)?)     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)\\(\\heartsuit\\ \\) Question D How smooth is \\(f_D(x)\\)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)\\(\\heartsuit\\ \\)       \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ Question E How smooth is \\(f_E(x)\\)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)\\(\\heartsuit\\ \\)       \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ Question F How smooth is \\(f_F(x)\\)?     discontinuous\\(\\heartsuit\\ \\)       \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ Question G How smooth is \\(f_G(x)\\)?     discontinuous︎✘        \\(C^0\\)\\(\\heartsuit\\ \\)       \\(C^1\\)︎✘        \\(C^2\\)︎✘        \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ Question H How smooth is \\(f_H(x)\\)?     discontinuous︎✘        \\(C^0\\)︎✘        \\(C^1\\)︎✘        \\(C^2\\)\\(\\heartsuit\\ \\)       \\(C^3\\)︎✘        \\(C^\\infty\\)︎✘ The domain of the function \\(1/x\\) is the whole number line, except 0, where the positive and negative branches fail to meet up.↩︎ "],["prod-comp-rules.html", "Chapter 22 Derivatives of assembled functions 22.1 Using the rules 22.2 Differentiating linear combinations 22.3 Product rule for multiplied functions 22.4 Chain rule for function composition 22.5 Derivatives of the basic modeling functions 22.6 Exponentials and logarithms (optional) 22.7 Exercises", " Chapter 22 Derivatives of assembled functions In Section 19.4 we used the rules associated with \\(\\lim_{h\\rightarrow 0}\\) to confirm our claims about the derivatives of many of the pattern-book functions. We’ll call these rules h-theory for short. In this chapter, we’re going to use h-theory to find algebraic rules to calculate the derivatives of linear combinations of functions, products of functions, and composition of functions. Remarkably, we can figure out these rules without having to say specifically which functions are being combined. So the rules can be written in terms of abstractions: \\(f()\\), \\(g()\\), and \\(h()\\). Later, we’ll apply those rules to specific functions, to show how the rules are used in practical work. 22.1 Using the rules When you encounter a function that you want to differentiate, you first have to examine the function to decide which rule you want to apply. In the following, we’ll to use the names \\(f()\\) and \\(g()\\), but in practice the functions will often be basic modeling functions, for instance \\(e^{kx}\\) or \\(\\sin\\left(\\frac{2\\pi}{P}t\\right)\\), etc. Step 1: Identify f() and g() We will write the rules in terms of two function names, \\(f()\\) and \\(g()\\), which can stand for any functions whatsoever. It’s rare to see the product or the composition written explicitly as \\(f(x)g(x)\\) of \\(f(g(x))\\). Instead, you are given something like \\(e^x \\ln(x)\\). The first step in differentiating the product or composition is to identify what are \\(f()\\) and \\(g()\\) individually. 2400 In general, \\(f()\\) and \\(g()\\) might be complicated functions, themselves involving linear combinations, products, and composition. But to get started, we’ll practice with cases where they are simple, pattern-book functions. 2405 Step 2: Find f’() and g’() For differentiating either products or compositions, you will need to identify both \\(f()\\) and \\(g()\\) (the first step) and then compute the derivatives \\(\\partial_x f()\\) and \\(\\partial_x g()\\). That is, you’ll write down four functions. 2410 Step 3: Apply the relevant rule Recall from Chapter ?? that will will be working with three important forms for creating new functions out of existing functions: Linear combinations, e.g. \\(a f(x) + bg(x)\\) Products of functions, e.g. \\(f(x) g(x)\\) Compositions of functions, e.g. \\(f\\left(g(x)\\right)\\) 22.2 Differentiating linear combinations Linear combination is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves scaling functions and then adding the scaled functions as in \\(a f(x) + b g(x)\\), alinear combination of \\(f(x)\\) and \\(g(x)\\). We can easily use \\(h\\) to show what is the result of differentiating a linear combination of functions. First, let’s figure out what is \\(\\partial_x\\, a f(x)\\), Going back to writing \\(\\partial_x\\) in terms of a slope function: \\[\\partial_x\\, a\\,f(x) = \\frac{a\\, f(x + h) - a\\,f(x)}{h}\\\\ \\ \\\\ = a \\frac{f(x+h) - f(x)}{h} = a\\, \\partial_x f(x)\\] In other words, if we know the derivative \\(\\partial_x\\, f(x)\\), we can easily find the derivative of \\(a\\, f()\\). Notice that even though \\(h\\) was used in the derivation, it appears nowhere in the result \\(\\partial_x\\, b\\,f(x) = b\\, \\partial_x\\, f(x)\\). The \\(h\\) is solvent to get the paint on the wall and evaporates once its job is done. Now consider the derivative of the sum of two functions, \\(f(x)\\) and \\(g(x)\\): \\[\\partial_x\\, \\left[f(x) + g(x)\\right] =\\\\ \\ \\\\ =\\frac{\\left[f(x + h) + g(x + h)\\right] - \\left[f(x) + g(x)\\right]}{h} \\\\ \\ \\\\ = \\frac{\\left[f(x+h) -f(x)\\right] + \\left[g(x+h) - g(x)\\right]}{h}\\\\ \\ \\\\ = \\frac{\\left[f(x+h) -f(x)\\right]}{h} + \\frac{\\left[g(x+h) - g(x)\\right]}{h}\\\\ \\ \\\\ = \\partial_x\\, f(x) + \\partial_x\\, g(x)\\] Because of the way that \\(\\partial_x\\) can be “passed through” a linear combination, mathematicians say that differentiation is a linear operator. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a product of two functions or the composition of two functions. We’ll lay out the \\(h\\)-theory based algebra of this in the next two sections. 2390 We can summarize the h-theory result for linear combinations this way: The derivative of a linear combination is the linear combination of the derivatives. That is: \\[\\partial_x \\left[\\strut \\color{magenta}{a} \\color{brown}{f(x)} + \\color{magenta}{b} \\color{brown}{g(x)}\\right] = \\color{magenta}{a} {\\large\\color{brown}{f&#39;(x)}} + \\color{magenta}{b} {\\large\\color{brown}{g&#39;(x)}}\\] as well as \\[\\partial_x \\left[\\strut \\color{magenta}{a}\\, \\color{brown}{f(x)} + \\color{magenta}{b}\\, \\color{brown}{g(x)} + \\color{magenta}{c}\\, \\color{brown}{h(x)} + \\cdots\\right] = \\color{magenta}{a}\\, {\\large\\color{brown}{f&#39;(x)}} + \\color{magenta}{b}\\, {\\large\\color{brown}{g&#39;(x)}} + \\color{magenta}{c}\\, {\\large\\color{brown}{h&#39;(x)}} + \\cdots\\] Example 22.1 The derivative of a polynomial is a polynomial of a lower order. Consider the polynomial \\[h(x) = \\color{magenta}{a}\\color{brown}{x^0} + \\color{magenta}{b} \\color{brown}{x^1} + \\color{magenta}{c} \\color{brown}{x^2}\\] The derivative is \\[\\partial_x h(x) = \\color{brown}{0}\\, \\color{magenta}{a} + \\color{brown}{1}\\, \\color{magenta}{b} + \\color{magenta}{c}\\, \\color{brown}{2 x} = \\color{magenta}{b} + \\color{magenta}{2 c}\\ x\\] 22.3 Product rule for multiplied functions The question at hand is how to compute the derivative \\(\\partial_x f(x) g(x)\\). Of course, you can always use numerical differentiation. But let’s look at the problem from the point of view of symbolic differentiation. And since \\(f(x)\\) and \\(g(x)\\) are just pronoun functions, we’ll assume you are starting out already knowing the derivatives $_x f(x) \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This situation arises particularly when \\(f(x)\\) and \\(g(x)\\) are pattern-book functions for which you already have memorized \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\) or are basic modeling functions whose derivatives you will memorize in Section 22.5. The purpose of this section is to derive the formula for \\(\\partial_x f(x) g(x)\\) in terms of \\(f(x)\\), \\(g(x)\\), \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This formula is called the product rule. The point of showing a derivation of the product rule is to let you see how the logic of evanescent \\(h\\) plays a role. In practice, everyone simply memorizes the rule, which has a beautiful, symmetric form: \\[\\text{Product rule:}\\ \\ \\ \\ \\partial_x \\left[\\strut f(x)g(x)\\right] = \\left[\\strut \\partial_x f(x)\\right]\\, g(x) + f(x)\\, \\left[\\strut\\partial_x g(x)\\right]\\] and is even prettier in Lagrange notation (where \\(\\partial_x f(x)\\) is written \\(f&#39;\\)): \\[ \\left[\\strut f g\\right]&#39; = f&#39; g + g&#39; f\\] As with all derivatives, the product rule is based on the instantaneous rate of change \\[F&#39;(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h}\\] introduced in Section 17.3. We also need two other statements about \\(h\\) and functions: The derivative \\(F&#39;(x)\\) is the slope of of \\(F()\\) at input \\(x\\). Taking a step of size \\(h\\) from \\(x\\) will induce a change of output of \\(h F&#39;(x)\\), so \\[F(x+h) = f(x) + h F&#39;(x)\\ .\\] Any result of the form \\(h F(x)\\), where \\(F(x)\\) is finite, gives 0. More precisely, \\(\\lim_{h\\rightarrow 0} h F(x) = 0\\) As before, we’ll put the standard \\(\\lim_{h\\rightarrow 0}\\) disclaimer against dividing by \\(h\\) until there are no such divisions at all, at which point we can safely use the equality \\(h = 0\\). Suppose the function \\(F(x) \\equiv f(x) g(x)\\), a product of the two functions \\(f(x)\\) and \\(g(x)\\). \\[\\require{cancel} F&#39;(x) = \\partial_x \\left[\\strut f(x) g(x) \\right] \\equiv \\lim_{h\\rightarrow 0}\\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\\] We’ll replace \\(g(x_h)\\) with its equivalent \\(g(x) + h g&#39;(x)\\) giving \\[= \\lim_{h\\rightarrow 0} \\frac{f(x+h) \\left[\\strut g(x) + h g&#39;(x) \\right] - f(x) g(x)}{h} \\] \\(g(x)\\) appears in both terms in the numerator, once multiplied by \\(f(x+h)\\) and once by \\(f(x)\\). Collecting those terms give: \\[=\\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x+ h) - f(x)\\right] g(x) + \\left[\\strut f(x+h) h\\, g&#39;(x)\\right]}{h}\\] This has two bracketed terms added together over a common denominator. Let’s split them into separate terms: \\[=\\lim_{h\\rightarrow 0}\\underbrace{\\left[\\strut \\frac{f(x+h) - f(x)}{h}\\right]}_{f&#39;(x)} g(x) + \\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x) + h f&#39;(x)\\right]h\\,g&#39;(x)}{h}\\] The first term is \\(g(x)\\) multiplied by the familiar form for the derivative of \\(f(x)\\) \\[= f&#39;(x) g(x) + \\lim_{h\\rightarrow 0}\\frac{f(x) h g&#39;(x)}{h} + \\lim_{h\\rightarrow 0}\\frac{h f&#39;(x) h g&#39;(x)}{h}\\] In each of the last two terms there is an \\(h/h\\) involved. This is safely set to 1, since the \\(\\lim_{h\\rightarrow 0}\\) implies that \\(h\\) will not be exactly zero. There remain no divisions by \\(h\\) so we can drop the \\(\\lim_{h\\rightarrow 0}\\) in favor of \\(h=0\\): \\[= f&#39;(x) g(x) + f(x) g&#39;(x) + \\cancel{h f&#39;(x) g&#39;(x)}\\] \\[=f&#39;(x) g(x) + g&#39;(x) f(x)\\] The last step relies on statement (2) above. Some people find it easier to read the rule in Lagrange shorthand, where \\(f\\) and \\(g\\) stand for \\(f(x)\\) and \\(g(x)\\) respectivly, and \\(f&#39;\\) (“f-prime”) and \\(g&#39;\\) (“g-prime”) stand for \\(\\partial f()\\) and \\(\\partial g()\\). 2415 \\[\\large\\text{Lagrange shorthand:}\\ \\ \\partial[\\color{magenta}f \\times \\color{brown}g] = [\\color{magenta}f \\times \\color{brown}g]&#39; = \\color{magenta}{f&#39;}\\color{brown}g + \\color{brown}{g&#39;}\\color{magenta}f\\] Example 22.2 The expression \\(\\partial_x x^3\\) is exactly the same as \\(\\partial_x \\left[\\strut x\\ x^2\\right]\\). Since we already know \\(\\partial_x x\\) (it’s 1) and \\(\\partial_x x^2\\) (it’s \\(2x\\)) let’s apply the product rule to find \\(\\partial_x x^3\\): \\[\\large\\partial [\\color{magenta}x \\times \\color{brown}{x^2}] = \\color{magenta}{[\\partial x]} \\times \\color{brown}{x^2} \\ +\\ \\color{brown}{[\\partial x^2]} \\times \\color{magenta}x =\\color{magenta}1\\times \\color{brown}{x^2} + \\color{brown}{2x} \\times \\color{magenta}x = 3 x^2\\] Occasionally, mathematics gives us a situation where being more general produces simplicity. In the case of function products, the generalization is from products of two functions \\(f(x)\\cdot g(x)\\) to products of more than two functions, e.g. \\(u(x) \\cdot v(x) \\cdot w(x)\\). The chain rule here takes a form that makes the overall structure much clearer: \\[\\partial_x \\left[\\strut u(x) \\cdot v(x) \\cdot w(x)\\right] = \\\\ \\color{blue}{\\partial_x u(x)} \\cdot v(x) \\cdot w(x)\\ +\\ u(x) \\cdot \\color{blue}{\\partial_x v(x)} \\cdot w(x)\\ +\\ u(x) \\cdot v(x) \\cdot \\color{blue}{\\partial_x w(x)}\\] In the Lagrange shorthand, the pattern is even more evident: \\[\\left[ u\\cdot v\\cdot w\\right]&#39; = \\color{blue}{u&#39;}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v&#39;}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}&#39;\\] 22.4 Chain rule for function composition A function composition, as described in Section ??, involves inserting the output of one function (the “interior function”) as the input of the other function (the “exterior function”). As we so often do, we’ll be using pronouns a lot. A list might help keep things straight: There are two functions involved in a composition. We’ll call them \\(f(y)\\) and \\(g(x)\\). In the composition \\(f(g(x))\\), the exterior function is \\(f()\\) and the interior function is \\(g()\\). Each of the two functions \\(f()\\) and \\(g()\\) has an input. In our examples, we’ll use \\(y\\) to stand for the input to the exterior function and \\(x\\) as the pronoun for the input to the interior function. As with all rules for differentiation, we’ll need to compute the derivatives of the functions involved, each with respect to its own input. So these will be \\(\\partial_y f(y)\\) and \\(\\partial_x g(x)\\). A reason to use different pronouns for the inputs to \\(f()\\) and \\(g()\\) is to remind us that the output \\(g(x)\\) is in general not the same kind of quantity as the input \\(x\\). In a function composition, the \\(f()\\) function will take the output \\(g(x)\\) as input. But since \\(g(x)\\) is not necessarily the same kind of thing as \\(x\\), why would we want to use the same name for the input to \\(f()\\) as we use for the input to \\(g()\\). With this distinction between the names of the inputs, we can be even more explicit about the composition, writing \\(f(y=g(x))\\) instead of \\(f(g(x))\\). Had we used the pronound \\(x\\) for the input to \\(f()\\) but our explicit statement, although technically correct, would be confusing: \\(f(x = g(x))\\)! With all these pronouns in mind, here is the chain rule for the derivative \\(\\partial_x f(g(x))\\): \\[\\large\\partial_x \\left[\\strut \\color{magenta}{f\\left(\\strut\\right.}\\strut \\color{brown}{g(x)}\\color{magenta}{\\left.\\right)}\\right] = [\\color{magenta}{\\partial_y f}](\\color{brown}{g(x)}) \\times [\\color{brown}{\\partial_xg(x)}]\\] Or, using the Lagrange prime notation, where \\(&#39;\\) stands for the derivative of a function with respect to its input, we have \\[\\large\\text{Lagrange shorthand:}\\ \\ [\\color{magenta}f(\\color{brown}g)]&#39; = \\color{magenta}{f&#39;} (\\color{brown}g) \\times \\color{brown}{g}&#39;\\] In news and policy discussions, you will often hear about “inflation rate” or “birth rate” or “interest rate” or “investment rate of return.” In each case, there is a function of time combined with a derivative of that function: with the general form \\[\\frac{\\partial_t f(t)}{ f(t)}\\ .\\] Inflation rate: The function is cost_of_living(\\(t\\)). The derivative is the rate of change with respect to time in the cost of living: \\(\\partial_t\\,\\)cost_of_living(\\(t\\)). Birth rate: The function is population(\\(t\\)). The derivative is \\(\\partial_t\\,\\)population(\\(t\\)), or at least that component of the overall \\(\\partial_t\\,\\)population(\\(t\\)) that is related to births. (Other components are deaths and the balance of in-migration and out-migration.) Interest rate: The function is account_balance(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)account_balance(\\(t\\)). Investment returns: The function is net_worth(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)net_worth(\\(t\\)). In all these cases, The “rate” is not merely “per time” as would be the case for \\(\\partial_t f(t)\\). Instead the rate is “per unit of the whole per time.” Thus the birth rate is “births per capita per year.”12 Interest and return rates are “percent per year” where the “percent” understood to be the “change-in-value divided by the current value.” Thanks to the chain rule, there is a shortcut way of writing these sorts of “rates per time.” Exactly equivalent to the ratio \\(\\frac{\\partial_t f(t)}{ f(t)}\\) is \\[\\partial_t \\ln(f(t))\\ .\\] Such changes in logarithms are encountered in fields such as economics or finance, where it’s common to consider the logarithm of the economic quantity in order to render changes as percent of the whole. It’s also something to keep in mind when interpreting graphs of an amount versus time, as in Figure @ref{fig:italy-us-covid}. Source Figure 22.1: Growth in the number of Coronavirus cases in Italy and the US early in the pandemic. Look closely at the two graphs in Figure 22.1. They show exactly the same data about growing numbers of coronavirus cases, the left graph on linear axes, the right on the now-familiar semi-log axes. Most people are excellent at comparing slopes, even if they find it difficult or tedious to quantify a slope with a number and units. For instance, a glance suffices to show that in the left graph, well through mid-March the red curve (Italy) is steeper on any given date than the blue curve (US). This means that the number of people with coronavirus was growing faster (per day) in Italy. The right graph tells a different story: up until about March 1, the Italian cases were increasing faster than the US cases. Afterwards, the US sees a larger growth rate than Italy until, around March 19, the US growth rate is substantially larger than the Italy growth rate. The previous two paragraphs and their corresponding graphs may seem to contradict one another. But they are both accurate, truthful depictions of the same events. What’s different between the two graphs is that the left shows one kind of rate and the right shows another kind of rate. In the left, the slope is new-cases-per-day, the output of the derivative function \\[{\\mathbf{\\text{left graph:}}} \\ \\ \\ \\ \\partial_t \\text{daily_new_cases}(t)\\ .\\] On the right, the slope is the proportional increase in cases per day, that is, \\[{\\mathbf{\\text{right graph:}}}\\ \\ \\ \\ \\frac{\\partial_t \\text{daily_new_cases}(t)}{\\text{daily_new_cases}(t)}\\] From the chain rule, we know that \\[\\partial_t \\left[\\strut\\ln(f(t))\\right] = \\frac{\\partial_t f(t)}{f(t)}\\] Since the right graph is on semi-log axes, the slope we perceive visually is \\(\\partial_t \\left[\\strut\\ln(f(t))\\right]\\). That’s an obscure-looking bunch of notation until the chain rule reveals it to be the rate of change at time \\(t\\) divided by the value at time \\(t\\). The derivation of the chain rule relies on two closely related statements which are expressions of the idea that near any value \\(x\\) a function can be expressed as a linear approximation with the slope equal to the derivative of the function : \\(g(x + h) = g(x) + h g&#39;(x)\\) \\(f(y + \\epsilon) = f(y) + \\epsilon f&#39;(y)\\), which is the same thing as (1) but uses \\(y\\) as the argument name and \\(\\epsilon\\) to stand for the small quantity we usually write with an \\(h\\). We’ll now look at \\(\\partial_x f\\left({\\large\\strut} g(x)\\right)\\) by writing down the fundamental definition of the derivative. This, of course, involves the disclaimer \\(\\lim_{h\\rightarrow 0}\\) until we’re sure that there is no division by \\(h\\) involved. \\[\\partial_x \\left[{\\large\\strut} f\\left(\\strut g(x)\\right)\\right] \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] Let’s examine closely the expression \\(\\color{magenta}{f\\left(\\strut g(x+h)\\right)}\\). Applying rule (1) above turns it into \\[\\lim_{h\\rightarrow 0} f\\left(\\strut g(x) + \\color{blue}{h g&#39;(x)}\\right)\\] Now apply rule (2) but substituting in \\(g(x)\\) for \\(y\\) and \\(\\color{blue}{h g&#39;(x)}\\) for \\(\\epsilon\\), giving \\[\\lim_{h\\rightarrow 0} \\color{magenta}{f\\left(\\strut g(x+h)\\right)} = \\lim_{h\\rightarrow 0} \\color{brown}{\\left[{\\large\\strut} f\\left(g(x)\\right) + \\color{blue}{h g&#39;(x)}f&#39;\\left(g(x)\\right)\\right]}\\] We’ll substitute the \\(\\color{blue}{blue}\\) and \\(\\color{brown}{brown}\\) expression for the \\(\\color{magenta}{magenta}\\) expression in \\[\\partial_x f\\left(\\strut g(x)\\right) \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] giving \\[\\partial_x f\\left(\\strut g(x)\\right) \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{f\\left(g(x)\\right) + \\color{blue}{h g&#39;(x)}f&#39;\\left(g(x)\\right)} - f\\left(g(x)\\right)}{h}\\] In the denominator, \\(f\\left(g(x)\\right)\\) appears twice and cancels itself out. That leaves a single term with an \\(h\\) in the numerator and an \\(h\\) in the denominator. Those \\(h\\)’s cancel out, at the same time obviating the need for \\(\\lim_{h\\rightarrow 0}\\) and leaving us with the chain rule: \\[\\partial_x f\\left(\\strut g(x)\\right) \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{ \\color{blue}{h g&#39;(x)} f&#39;\\left(g(x)\\right)}}{h} = f&#39;\\left(g(x)\\right)\\ g&#39;(x)\\] Example 22.3 Use the chain rule to find the derivative \\(\\partial_x e^{2x}\\). Recognize that \\(g(x) \\equiv 2x\\) is the interior function in \\(e^{2x}\\) and \\(f(x) \\equiv \\exp(x)\\) is the exterior function. Thus \\[\\partial_x e^{2x} = f&#39;(g(x)) g&#39;(x) = \\exp(g(x)) 2 = 2 e^{2x}\\ .\\] Happily, this is exactly the same result as we got from using the product rule to find \\(\\partial_x e^{2x}\\). Recognizing \\(e^{2x}\\) as \\(e^x \\times e^x\\), we can apply the product rule. Example 22.4 The chain rule can be used in a clever way to find a formula for \\(\\partial_x \\ln(x)\\). We’ve already seen that the logarithm is the inverse function to the exponential, and vice versa. That is: \\[e^{\\ln(y)} = y \\ \\ \\ \\text{and}\\ \\ \\ \\ln(e^y) = x\\] Since \\(\\ln(e^y)\\) is the same function as \\(y\\), the derivative \\(\\partial_y \\ln(e^y) = \\partial_y y = 1\\). Let’s differentiate the second form using the chain rule: \\[\\partial_y \\ln(e^y) = \\left[\\partial_y \\ln\\right](e^y)\\, e^x = 1\\] giving \\[\\left[\\partial_y \\ln\\right](e^y) = \\frac{1}{e^y} = \\recip(e^y)\\] Whatever the function \\(\\partial_x \\ln()\\) might be, it takes its input and produces as output the reciprocal of that input. In other words: \\[\\partial_x \\ln(x) = \\frac{1}{x}\\ .\\] Example 22.5 Knowing that \\(\\partial_x \\ln(x) = 1/x\\) and the chain rule, we’re in a position to demonstrate the power-law rule \\(\\partial_x x^p = p\\, x^{p-1}\\). The key is to use the identity \\(e^{\\ln(x)} = x\\). \\[\\partial_x x^p = \\partial_x \\left[e^{\\ln(x)}\\right]^p\\] The rules of exponents allow us to recognize \\[\\left[e^{\\ln(x)}\\right]^p = e^{p \\ln(x)}\\] Thus, \\(x^p\\) can be seen as a composition of the exponential function onto the logarithm function. Applying the chain rule to this composition gives \\[\\partial_x e^{p \\ln(x)} = e^{p\\ln(x)}\\partial_x [p \\ln(x)] = e^{p\\ln(x)} \\frac{p}{x}\\ .\\] Of course, we already know that \\(e^{p \\ln(x)} = x^p\\), so we have \\[\\partial_x x^p = x^p \\frac{p}{x} = p x^{p-1}\\ .\\] \\(\\large\\partial_x [\\color{brown}\\sin(\\color{magenta}{a x + b})] = [\\partial_x \\color{brown}{\\sin}](\\color{magenta}{a x + b}) \\times \\partial_x [\\color{magenta}{ax + b}] = \\color{brown}{\\cos}(\\color{magenta}{ax + b}) \\times \\color{magenta}a\\). In 1734, famous philosopher George Berkeley (1685-1753) published a long-titled book: The Analyst: A Discourse Addressed to an Infidel Mathematician: Wherein It Is Examined Whether the Object, Principles, and Inferences of the Modern Analysis Are More Distinctly Conceived, or More Evidently Deduced, Than Religious Mysteries and Points of Faith. In The Analyst, Berkeley took issue with the arguments of that time that it is legitimate to divide by \\(h\\) when, ultimately, \\(h\\) will be replaced by zero. Calling \\(h\\) an “evanescent increment,” he asked, 2392 “And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?” Interesting, Berkeley believed that the ghost of \\(h\\) yielded correct results. His objection was that the framers of calculus had made two, canceling errors. “[B]y virtue of a two fold mistake you arrive, though not at science, yet truth.” Berkeley was saying that calculus had not yet been put on a solid logical foundation. It wasn’t until more than a century after Berkeley’s death that this work was accomplished. Once accomplished, the results that had been claimed true all along were confirmed. 2394 22.5 Derivatives of the basic modeling functions The basic modeling functions are the same as the pattern-book functions, but with bare \\(x\\) replaced by \\(\\line(x)\\). In other words, each of the basic modeling functions is a composition of the corresponding pattern-book function with \\(\\line(x)\\). As such, the derivatives of the basic modeling functions can be found using the chain rule. Suppose \\(f()\\) is one of our pattern-book functions. Then \\[\\large\\partial_x f(\\color{magenta}{ax + b}) = \\color{brown}{a} f&#39;(\\color{magenta}{ax + b})\\] where \\(\\color{brown}{a}\\) is the derivative with respect to \\(x\\) of \\(\\color{magenta}{ax + b}\\). Here are the steps for differentiating a basic modeling function \\(\\color{brown}{f}(\\color{magenta}{a x + b})\\) where \\(f()\\) is one of the pattern-book functions: Step 1: Identify the particular pattern-book function \\(\\color{brown}{f}()\\) and write down its derivative \\(\\color{brown}{f&#39;}\\). For example, if \\(f()\\) is \\(\\sin()\\), then \\(f&#39;()\\) is \\(\\cos()\\). Step 2: Find the derivative of the linear interior function. If the function is \\(\\color{magenta}{ax + b}\\), then the derivative is \\(\\color{magenta}{a}\\). If the interior function is \\(\\frac{2\\pi}{P}(t-t_0)\\), the derivative is \\(\\frac{2 \\pi}{P}\\). Step 3: Write down the original function \\(\\large\\color{brown}{f}(\\color{magenta}{a x + b})\\) but replace \\(\\large\\color{brown}{f}\\) with \\(\\large \\color{brown}{f&#39;}\\) and pre-multiply by the derivative of the interior function. For instance, \\[\\partial_x f(\\color{magenta}{ax + b}) = {\\large \\color{magenta}{a}}{\\large f&#39;}(\\color{magenta}{ax + b})\\] Another example: \\[\\partial_t \\color{brown}{\\sin}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0)} \\right) = {\\large \\color{magenta}{\\frac{2 \\pi}{P}}}\\color{brown}{\\large\\cos}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0) }\\right) \\] By convention, there are different ways of writing \\(\\line(x)\\) for the different pattern-book functions, for instance: 2396 \\[\\text{Pattern-book function} \\ \\ \\ \\longrightarrow\\ \\ \\ \\text{Basic modeling function}\\\\ \\ \\\\ \\sin(x)\\ \\ \\ \\longrightarrow\\ \\ \\ \\sin\\left(\\strut2 \\pi \\left[x-x_0\\right]/P\\right)\\\\ \\exp(x)\\ \\ \\ \\longrightarrow\\ \\ \\ \\exp(k x)\\\\ x^2 \\ \\ \\ \\longrightarrow\\ \\ \\ \\left[mx + b\\right]^2\\\\ 1/x \\ \\ \\ \\longrightarrow\\ \\ \\ 1/\\left[mx + b\\right]\\\\ \\ln(x) \\ \\ \\ \\longrightarrow\\ \\ \\ \\ln(a x + b)\\\\\\] The rule for the derivative of any basic modeling function \\(f(\\line(x))\\) is \\[\\partial_x f(\\line(x)) = \\partial_x \\line(x) \\times \\partial_x f\\left(\\strut\\line(x)\\right)\\] To illustrate: \\(\\partial_x e^{\\color{magenta}{kx}} = {\\large\\color{magenta}{k}}\\, e^{\\color{magenta}{kx}}\\) where \\(\\line(x) = kx\\). \\(\\partial_x \\sin(2\\pi (x-x_0)/P) = \\frac{2\\pi}{P} \\sin(2\\pi (x-x_0)/P)\\) where \\(\\line(x) = 2\\pi (x-x_0)/P)\\). \\(\\partial_x (mx + b)^2 = m\\, 2 (m x + b) = 2 m^2 x + m^2 b\\) where \\(\\line(x) = mx + b\\). \\(\\partial_x \\text{reciprocal}(mx + b) = \\partial_x \\frac{1}{mx + b} = - \\frac{m}{(mx + b)^2}\\) where \\(\\line(x) = mx + b\\) and we use the fact that \\(\\partial_x \\text{reciprocal}(x) = - 1/x^2\\) \\(\\partial_x \\ln(a x + b) = a/(ax+b)\\) \\(\\partial_x \\pnorm(x, \\text{mean}, \\text{sd}) = dnorm(x, \\text{mean}, \\text{sd})\\). \\(\\partial_x \\dnorm(x, \\text{mean}, \\text{sd}) = - \\frac{x-m}{\\text{sd}^2} \\dnorm(x, \\text{mean}, \\text{sd})\\) You will be using the derivatives of the basic modeling functions so often, that you should practice and practice until you can write the derivative at a glance. There are many possible implementations of the general concept of hump functions and sigmoidal functions. The one we use in this book is \\(\\dnorm()\\) for the hump and \\(\\pnorm()\\) for the sigmoid. The names \\(\\dnorm\\) and \\(\\pnorm\\) are worth remarking on. As we’ve said before, \\(\\dnorm()\\) is called the gaussian function in many fields of science and engineering. It is also a centrally important function in statistics, where it is usually called the normal function. (That’s how important it is: it’s just “normal.”) You may also have heard the normal function described as a “bell-shaped curve.” In statistical nomenclature, \\(\\dnorm()\\) is called the “normal probability density function (PDF)” and \\(\\pnorm()\\) is called the “normal cumulative density function (CDF).” That’s way too wordy for our purposes. So, for brevity, we have adopted the R name for those functions: dnorm() and pnorm(). Owing to the origin of the names \\(\\dnorm\\) and \\(\\pnorm\\), we are writing the parameters of the functions—mean and sd—using the computer language notation. The pattern-book functions are just \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\), without listing the parameters. But the basic modeling functions, with parameters, are written \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) and \\(\\dnorm(x, \\text{mean}, \\text{sd})\\). This violates the convention that the basic modeling functions are the composition of the pattern-book functions with \\(\\line(x)\\). But \\(\\dnorm()\\) doesn’t actually work this way because, by convention, the amplitude of the peak of \\(\\dnorm()\\) changes with the input parameter sd. That’s not true for any other basic modeling function. Composition or product? There is one family of functions for which function composition is the same thing as multiplying functions: the power-law family. Consider, for instance, the function \\(h(x) \\equiv \\left[3x\\right]^4\\). Let’s let \\(g(x) \\equiv 3x\\) and \\(f(y) \\equiv y^4\\). With these definitions, \\(h(x) = f(g(x))\\). Recognizing that \\(\\partial_y f(y) = 4 y^3\\) and \\(\\partial_x g(x) = 3\\), the chain rule gives \\[\\partial_x h(x) = \\underbrace{4 g(x)^3}_{f&#39;(g(x))} \\times \\underbrace{3}_{g&#39;(x)} = \\underbrace{4 (3 x)^3}_{f&#39;(g(x))} \\times 3 = 4\\cdot 3^4 \\times x^3 = 324\\ x^3\\] Another way to look at the same function is \\(g(x)\\) multiplied by itself 3 times: \\[h(x) = g(x)\\cdot g(x) \\cdot g(x) \\cdot g(x)\\] This is a product of 4 terms. Applying the product rule gives \\[\\partial_x h(x) = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\\color{blue}{g&#39;(x)}\\cdot g(x)\\cdot g(x) \\cdot g(x) +\\\\ g(x)\\cdot \\color{blue}{g(x)}&#39;\\cdot g(x) \\cdot g(x) +\\\\ g(x)\\cdot g(x)\\cdot \\color{blue}{g(x)&#39;} \\cdot g(x) +\\\\ g(x)\\cdot g(x)\\cdot g(x) \\cdot \\color{blue}{g&#39;(x)}\\ \\ \\ \\ \\] Since multiplication is commutative, all of these four terms are the same, each being \\(3^4 x^3\\). The sum of all four is therefore \\(4 \\times 3^4 x^3 = 324 x^3\\). These are two long-winded ways of getting to the result. For most people, differentiating power-law functions algebraically is simplified by using the rules of exponentiation rather than the product or chain rule. Here, \\[h(x) \\equiv \\left[3x\\right]^4 = 3^4 x^4\\]so \\(\\partial_x h(x)\\) is easily handled as a scalar (\\(3^4\\)) times a function \\(x^4\\). Consequently, applying the rule for differentiating power laws, \\[\\partial_x h(x) = 3^4 \\times \\partial_x x^4 = 3^4 \\times 4 x^3 = 324 x^3\\] As another example, take \\(h(x) \\equiv \\sqrt[4]{\\strut x^3}\\). This is, of course, the composition \\(f(g(x))\\) where \\(f(y) \\equiv y^{1/4}\\) and \\(g(x) \\equiv x^3\\). Applying the chain rule to find \\(\\partial_x h(x)\\) will work (of course!), but is more work than applying the rules of exponentiation followed by a simple power-law differentiation. \\[h(x) = \\sqrt[4]{\\strut x^3} = x^{3/4}\\ \\ \\text{so}\\ \\ \\partial_x h(x) = \\frac{3}{4} x^{(3/4 - 1)} = \\frac{3}{4} x^{-1/4}\\] 22.6 Exponentials and logarithms (optional) The natural logarithm function, \\(\\ln(x)\\), is one of our basic modeling functions. As you know, there are other logarithmic functions. The one most often used is the logarithm-base-10, written \\(\\log_{10}(x)\\) or log10(x). Ten is an integer, and a nice number to use in arithmetic. So in practice, it’s sensible to use \\(\\log_{10}()\\). (Indeed, \\(\\log_{10}()\\) is the digit() function, introduced in Chapter ??. The “natural” in the “natural logarithm” means something different. 2430 The base of the natural logarithm is the number called Euler’s constant and written \\(e\\). As a celebrity number, \\(e\\) is right up there with \\(\\pi\\) and \\(i\\). Just as \\(\\pi\\) has a decimal expansion that is infinitely long, the familiar \\(\\pi = 3.14159265358979...\\), Euler’s constant has an infinitely long decimal representation: \\(e = 2.71828182845905...\\) 2435 It’s not obvious at first glance why \\(e = 2.71828182845905...\\) should be called “natural” by mathematicians. The reason is not the number itself, but \\(\\ln(x)\\) is the inverse of \\(e^x\\), which is special for being invariant under differentiation: \\(\\partial_x e^x = e^x\\). The derivative \\(\\partial_x \\ln(x)\\) which has a particularly simple form, namely, \\(1/x\\). Let’s look at the log-base-10 and it’s computer-savvy cousin log-base-2. The very definition of logarithms means that both 10 and 2 can be written \\[10 = e^{\\ln(10)}\\ \\ \\ \\text{and}\\ \\ \\ 2 = e^{\\ln(2)}\\] This implies that the base-10 and base-2 exponential functions can be written \\[10^x = \\left[\\strut e^{\\strut\\ln(10)}\\right]^x = e^{\\ln(10)x} \\ \\ \\ \\text{and}\\ \\ \\ 2^x = \\left[\\strut e^{\\strut\\ln(2)}\\right]^x = e^{\\ln(2) x}\\] Calculating \\(\\partial_x 10^x\\) or \\(\\partial_x 2^x\\) is a matter of applying the chain rule: \\[\\partial_x [10^x] = \\partial_x [e^{\\ln(10)x}] = e^{\\ln(10)x} \\times \\ln(10) \\ =\\ 10^x \\times 2.3026\\] and \\[\\partial_x [2^x] = \\partial_x [e^{\\ln(2)x}] = e^{\\ln(2)x} \\times \\ln(2) \\ = \\ 2^x \\times 0.6931\\] Like \\(e^x\\), the derivatives of \\(10^x\\) and \\(2^x\\) are proportional to themselves. For \\(e^x\\) the constant of proportionality is 1, a very natural number indeed. 22.7 Exercises Exercise 22.01: jYXYTF Section 22.1 explains that in differentiating a linear combination of two functions, or a product of two functions, or one function composed with another, your first task is to identify the two functions \\(f()\\) and \\(g()\\) involved. Second, compute the derivative of each of those functions on its own: \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). Carry out these two tasks for each of the combined functions shown in the table. (The first row has been done for you as an example.) Combination \\(f()\\) \\(g()\\) \\(\\partial_x f()\\) \\(\\partial_x g()\\) \\(e^x \\ln(x)\\) \\(\\ln(x)\\) \\(e^x\\) \\(\\recip\\) (that is \\(1/x\\)) \\(e^x\\) \\(sin(e^x)\\) \\(x + x^2\\) \\(1/\\sin(x)\\) \\(\\pnorm(x)^2\\) \\(\\sqrt{\\pnorm(x)}\\) \\(\\pnorm(x^2)\\) \\(pnorm(sin(x))\\) Exercise 22.02: 3ICCCz For each of the following, say whether the function is a composition \\(f(g(x))\\) or a product \\(f(x) g(x)\\), or neither. Question A What sort of combination is \\(h_1(x)\\equiv \\ln(x) e^x\\)?     product\\(\\heartsuit\\ \\)       composition︎✘        neither︎✘ Question B What sort of combination is \\(h_2(x)\\equiv \\sin(x) \\cos(x)\\)?     product\\(\\heartsuit\\ \\)       composition︎✘        neither︎✘ Question C What sort of combination is \\(h_3(x) \\equiv \\sin(\\ln(x))\\)?     product︎✘        composition\\(\\heartsuit\\ \\)       neither︎✘ Question D What sort of combination is \\(h_4(x) \\equiv e^{\\ln(x)}\\)?     product︎✘        composition\\(\\heartsuit\\ \\)       neither︎✘ Question E What sort of combination is \\(h_5(x) \\equiv \\sin(x) - \\dnorm(x)\\)?     product︎✘        composition︎✘        neither\\(\\heartsuit\\ \\) Question F What sort of combination is is \\(h_6(x) \\equiv e^{x^2}\\)?     product︎✘        composition\\(\\heartsuit\\ \\)       neither︎✘ Question G What sort of combination is \\(h_7(x) \\equiv \\pnorm(x^2)\\)?     product︎✘        composition\\(\\heartsuit\\ \\)       neither︎✘ Question H What sort of combination is \\(h_8(x) \\equiv \\pnorm(x) \\dnorm(x)\\)?     product\\(\\heartsuit\\ \\)       composition︎✘        neither︎✘ Question I What sort of combination is \\(h_9(x) \\equiv 1/\\sin(x)\\)? product︎✘ compositionNice! Remember, \\(1/\\sin(x)\\) is the same as \\(\\recip(\\sin(x))\\). neither︎✘ Exercise 22.03: JvtMuz Consider this function, \\(F(t)\\), which is a linear combination of three time-shifted sigmoids. As you know, the derivative of a sigmoid \\(\\text{pnorm}(t)\\) is a gaussian with the same center and standard deviation. Question A How many gaussians will be in \\(\\partial_t F(t)\\).     2︎✘        3\\(\\heartsuit\\ \\)       6︎✘        none︎✘ The following figure shows several functions. One of them is \\(\\partial_t F(t)\\). Question B Which function is the actual derivative of \\(F(t)\\)? (Hints: The vertical axis is important as is the value of \\(\\text{dnorm}(0)\\).)     A︎✘        B︎✘        C︎✘        D\\(\\heartsuit\\ \\) Question C Which function is the actual second derivative of \\(F(t)\\)? (Hints: The vertical axis is important as is the value of \\(\\text{dnorm}(0)\\).) A︎✘ The problem is with the first event. B︎✘ The first and second events go in the same direction. The third event is not as big as the second. CGood.  D︎✘ The first event isn’t the second derivative of a sigmoid. Exercise 22.04: QFzIOl In function compositions of the form \\(f\\left(\\strut g(x)\\right)\\), the function \\(f()\\) is called the exterior function and \\(g()\\) is called the interior function. Question A In \\(\\cos(\\ln(x))\\) which is the interior function? \\(\\ln()\\)Nice!  \\(\\cos()\\)︎✘ This is the exterior function. \\(\\sin()\\)︎✘ Sine has nothing to do with it! None of the above︎✘ It’s not a function composition︎✘ Question B In \\(1/\\sin(x)\\) which is the exterior function? \\(\\recip()\\)Excellent!  \\(\\cos()\\)︎✘ \\(\\sin()\\)︎✘ This is the interior function. None of the above︎✘ It’s not a function composition︎✘ Question C In \\(\\sin\\left(\\frac{2 \\pi}{P} (t-t_0)\\right)\\) which is the exterior function? \\(t-t_0\\)︎✘ \\(\\frac{2\\pi}{P}\\)︎✘ \\(\\frac{2\\pi}{P} t\\)︎✘ \\(\\frac{2\\pi}{P} (t-t_0)\\)︎✘ \\(\\sin()\\)Nice!  None of the above︎✘ It’s not a function composition︎✘ Question D In \\(\\sin\\left(2 \\pi (t-t_0)/P\\right)\\) which is the interior function? \\(t-t_0\\)︎✘ That’s part of it. \\(2\\pi /P\\)︎✘ That’s part of it. \\(2\\pi t /P t\\)︎✘ That’s part of it. \\(2\\pi (t-t_0)/P\\)Correct.  \\(\\sin()\\)︎✘ This is the interior function. None of the above︎✘ It’s not a function composition.︎✘ Question E In \\(\\sin(x)\\dnorm(x^2)\\), which is the interior function?     \\(x^2\\)︎✘        \\(x\\)︎✘        \\(\\dnorm(x^2)\\)︎✘        None of the above︎✘        It’s not a function composition.\\(\\heartsuit\\ \\) Exercise 22.06: QIELS Compare the functions \\(f_1 \\equiv \\dnorm(x, mn, sd)\\) and \\(f_2 \\equiv \\dnorm\\left(\\left[x-mn\\right]/sd\\right)\\) by plotting them out in a SANDBOX. In order to construct the plot, you’ll have to pick specific values for \\(mn\\) and \\(sd\\). Make sure that you use the same \\(sd\\) and \\(mn\\) when constructing \\(f_1()\\) and \\(f_2()\\). For instance: f1 &lt;- makeFun(dnorm(x, mn, sd) ~ x, mn=2, sd=3) f2 &lt;- makeFun(dnorm( (x-mn) / sd) ~ x, mn=2, sd=3) Question A When \\(\\text{sd} = 1\\), are the two functions the same?     Yes\\(\\heartsuit\\ \\)       Yes, but only if \\(\\text{mn}=1\\)︎✘        Yes, but only if \\(\\text{mn}=0\\)︎✘        No︎✘ Question B When \\(\\text{sd} \\neq 1\\), for any given mean, the two functions are not the same. What’s the relationship between \\(f_1(x)\\) and \\(f_2(x)\\)? \\(f_2(x) = sd\\, f_1(x)\\)Good.  \\(f_1(x) = sd\\, f_2(x)\\)︎✘ \\(f_1(x) = sd^2 f_2(x)\\)︎✘ \\(f_2(x) = sd^2 f_1(x)\\)︎✘ Exercise 22.08: HCILW Pilots of commercial passenger aircraft consider the comfort of their passengers into account when flying. In transitioning from level flight onto the descent path for landing, for example, pilots take care that the vertical component of acceleration isn’t so great that passengers feel the plane “falling out from under them.” A simple model of the descent path is a sigmoid function. Suppose that the descent starts from an altitude of \\(A = 20,000\\) feet at a distance of 30,000 feet from the end of the runway. A reasonable model for the vertical component of the flight path is \\[\\text{altitude}(x) \\equiv A\\,\\pnorm(x,\\ \\ mn=30000/2,\\ \\ sd=30000/6)\\] Notice that the parameter “mean” is set to be half the distance to the runway, and the parameter “sd” is set to be a third of that. This ensures that the start and end of the descent will involve flight that is close to level. The vertical acceleration is the second derivative of alt() with respect to time: \\(\\partial_{tt} \\text{altitude}(t)\\). But notice that alt() is a function of distance from the runway, not time. In order to treat alt() as a function of time, we need to write “distance from the runway” as a function of time. Let’s set \\(t=0\\) to be the time when the plane begins its descent, when it’s 30,000 feet from the end of the runway. Distance from the runway will be \\[x(t) = 30000 - v\\, t\\] where \\(v\\) is the plane’s velocity. Composing altitude() onto \\(x(t)\\) gives a new function \\[\\text{alt}(t) \\equiv \\text{altitude}(x(t)) = \\text{altitude}(30000-v\\, t)\\] Suppose that the aircraft is flying at \\(v = 200\\) miles-per-hour, which is \\[200 \\frac{\\bcancel{\\text{miles}}}{\\cancel{\\text{hour}}} \\frac{\\cancel{1\\, \\text{hour}}}{3600\\, \\text{s}} \\frac{5280 \\text{ft}}{\\bcancel{1\\, \\text{mile}}} = 293.3 \\frac{\\text{ft}}{\\text{s}}\\]. At that speed, it will take a little more than 100 seconds for the aircraft to reach the runway. Using a sandbox, plot out the function alt(\\(t\\)) function, choosing a domain for \\(t\\) that lets you see the whole descent path. alt &lt;- makeFun(20000 * pnorm(30000 - v * t, 30000/2, 30000/6) ~ t, v = 293.3) slice_plot(alt(t) ~ t, domain(t=c(0, 110))) Compute the second derivative \\(\\partial_{tt} \\text{alt}(t)\\) to find the vertical component of acceleration of the aircraft. (Important note: Due to a bug in R, use numD() rather than D() to compute the second derivative.) Graph the second derivative over the appropriate domain and look for the most extreme values of acceleration. dd_alt &lt;- numD(alt(t) ~ t + t) slice_plot(dd_alt(t) ~ t, domain(t=c(0,110))) From the graph, read off the maximum vertical acceleration during the descent. Question A What are the units of vertical acceleration shown in the graph? feet-per-second︎✘ That’s a unit of velocity, not acceleration. feet-per-second-squaredGood.  miles-per-hour-squared︎✘ This is a unit of acceleration, but since we used feet and seconds in defining the alt() function, the second derivative will also be in feet and seconds. A rule of thumb is that a vertical acceleration up to \\(5\\, \\text{ft}\\, \\text{s}^{-2}\\) is acceptable in terms of passenger comfort. Regrettably, the descent path we described doesn’t meet the standard! So we have to re-design the descent path. Since both the altitude and velocity are set, the only parameter you can change is the distance from the foot of the runway where descent commences. Of course, for the parameters “mean” and “sd” need to be set accordingly. Question B How far from the foot of the runway should descent begin in order to stay within the \\(5\\, \\text{ft}\\, \\text{s}^{-2}\\) acceleration constraint? Pick the shortest distance that satisfies the constraint.     40,000 ft︎✘        50,000 ft︎✘        60,000 ft\\(\\heartsuit\\ \\)       70,000 ft︎✘        80,000 ft︎✘ For reflection: A new hire at the airline’s operations center proposes to model the descent as a straight-line function rather than a sigmoid. He points out that the second derivative of a straight-line function is always 0, so the passengers would feel no acceleration at all! Explain to this newbie what’s wrong with his idea. Exercise 22.09: Ao6v5n In Exercise none yet, E9e7c6 you constructed models \\(D(t)\\) of the availability of a drug in the bloodstream for three different pill-taking regimens: every six hours, every eight hours, and a double dose to start followed by a single dose every eight hours. The model for from a single, isolated pill is a zero before the pill is taken, then exponential decay from the level of the pill dose after the pill is taken. Like this: pill &lt;- makeFun(ifelse(t &lt; 0, 0, exp(-k * t)) ~ t, k = log(2)/3) The parameter \\(k\\) has been set to represent a drug with a half-life of three hours. The model for the entire regiment is a linear combination of time-shifted single pills, e.g.: regimen8 &lt;- makeFun(A*pill(t) + A*pill(t-8) + A*pill(t-16) + A*pill(t-24) + A*pill(t-32) ~ t, A=1) From graphs of the functions themselves it is easy to check whether the availability ever falls below the therapeutic threshold (which we stipulated is 0.25). For instance, the eight-hour regiment with a dose of A=1 does fall below the threshold during the first day. So a larger dose is needed than A=1. The derivative \\(\\partial_t \\text{regimen8}(t)\\) tells the instantaneous rate at which the drug is being administered to and eliminated from the patient’s body. For each of the three regimens, construct \\(\\partial_t \\text{regimen}(t)\\). Ignoring the glitches due to discontinuity at the times the pills are consumed, which of the three regimens has the lowest average rate of drug elimination? Exercise 22.10: ed6q4v Recall from Section ?? the Lorenz curve used to describe income inequality. The Lorenz curve shows the fraction of total income versus population fraction. Figure 22.2: A Lorenz curve (blue) fitted to income data from the US in 2009. (See Figure ??.) Since the population is arranged from poorest to richest along the horizontal axis, Lorenz curves must be both monotonically increasing and concave up. That is, any Lorenz function \\(L(P)\\), where \\(P\\) is the population fraction, must satisfy these criteria: \\(L(0) = 0\\) \\(L(1) = 1\\) that is, the aggregate fraction of income earned by the entire population is 100%. \\(\\partial_P L(P) \\ &gt; \\ 0\\) that is, monotonically increasing \\(\\partial_{PP} L(P) \\ &gt; \\ 0\\) that is, concave up. Consider a function \\(H(P) \\equiv L_1(L_2(P))\\) which is the composition of two Lorenz curves. A. Use the composition rule to show that \\(H(P)\\) is monotonically increasing. (Hint, calculate \\(\\partial_P H(P)\\) and show that it must be positive.) B. Using both the composition and product rules, calculate \\(\\partial_{PP} H(P)\\) and show that \\(H(P)\\) must be concave up. Exercise 22.12: 0yrIXe The formula for the function \\(\\dnorm(x)\\) is \\[\\dnorm(x) \\equiv \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(\\frac{x^2}{2}\\right)\\ .\\] Use the chain rule to find \\(\\partial_x \\dnorm(x)\\). Confirm from your answer to (1) that there is another formula for \\(\\partial_x \\dnorm(x)\\), namely \\[\\partial_x \\dnorm(x) = - x \\dnorm(x)\\ .\\] Use the product rule to find \\(\\partial_{xx} \\dnorm(x)\\). From your answer to (3), compute the 3rd derivative \\(\\partial_{xxx} \\dnorm(x)\\): Ans: \\[2 x \\dnorm(x) - x \\left[{\\large\\strut}x^2 - 1\\right] \\dnorm(x) = \\dnorm(x) \\left[{\\large\\strut} (3 x - x^3)\\right] \\ .\\] Let’s generalize the pattern. Each of the previous derivatives has been a polynomial—let’s call it \\(p_n(x)\\) for the \\(n\\)th derivative—times \\(\\dnorm(x)\\). Knowing \\(p_n(x)\\), we can easily find \\(p_{n+1}(x)\\): \\[p_{n+1}(x) = -x p_n(x) + \\partial_x p_n(x)\\] We know \\(p_1(x) = -x\\) so \\(p_2(x) = x^2 - 1\\). In turn, this tells us \\(p_3(x) = 3x - x^3\\). Find: \\(p_4(x)\\) \\(p_5(x)\\) \\(p_6(x)\\) Exercise 22.14: 2JJoS6 Confirm using algebraic manipulation the differentiation rule for a product of three functions: \\[\\left[ u\\cdot v\\cdot w\\right]&#39; = \\color{blue}{u&#39;}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v&#39;}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}&#39;\\] Here, \\(u\\) is shorthand for \\(u(x)\\), and \\(u&#39;\\) is shorthand for \\(\\partial_x u(x)\\), and similarly for \\(v\\) and \\(w\\). Hint: \\(\\left[ u\\cdot v\\cdot w\\right] = u \\cdot \\left[ v \\cdot w\\right]\\). So a product of three functions can be seen as a product \\(u\\cdot h\\) where \\(h \\equiv v \\cdot w\\). “Per capita” is Latin. Literally, it translates to “by head.” but its modern sense is “per unit of population.” The “unit of population” is, of course, a person.↩︎ "],["optim-and-shape.html", "Chapter 23 Optimization 23.1 Structure of the problem 23.2 Interpreting the argmax 23.3 Derivatives and optimization 23.4 Be practical! 23.5 Exercises", " Chapter 23 Optimization To “optimize” means to make something as good as possible with the available resources. Optimization problems are common in science, logistics, industry, and any other area where one seeks the best solution to a problem. Some everyday examples: 2440 When to harvest trees being grown for lumber. Harvest too soon and you might be losing out on the prime growing years. Wait too long and trees will have settled in to slow growth, if any. Walking up too steep a slope is tiring and slows you down; that’s why hiking trails have switchbacks. When the switchbacks are too shallow, it takes a long time to cover the distance. What’s the most efficient angle to enable hikers to get up the hill in the shortest time. How much salt to add to a stew. Stews can be too salty, or they can be not salty enough. Somewhere in the middle is the optimum.2445 23.1 Structure of the problem In an optimization problem, there is one or more input quantities whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We’ll call this the decision quantity. 2450 Similarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the amount of usable wood harvested; the time it takes to walk up the hill. The output quantities are called the objectives. 2455 In this chapter, we will deal with optimization problems that involve only a single objective. Problems with multiple objectives are among the most interesting and important in real-world decision making. Single-objective optimization techniques are a component of the more complex decision making, but they are a good place to get started. The model that relates inputs to the objective output is called the objective function. Solving an optimization problem—once the modeling phase is complete—amounts to finding a value for the decision quantity (the input to the objective function) that produces the best output from the objective function. 2460 Sometimes the objective is something that you want to minimize, make as small as possible. In the hiking trail problem, we seek to minimize the amount of time it takes to walk up the trail. Sometimes you want to maximize the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year. 2465 Mathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we’ll imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, “best” can mean minimization. 2470 Recall from Section ?? that there are two components to the task of maximization or minimization. The argmax is the input to the objective function which produces the largest output. The maximum is the value of that output.13 Argmin and minimum are the words used in a situation where you seek the smallest value of the objective function.2475 Once you have found the argmax you can plug that value into the objective function to find the value of the output. That value is the maximum. People often talk about “finding the maximum.” This is misleading. The setup for an optimization problem is: Construct (that is, model) the objective function. Now that you know the objective function, find the input to that function—that is, the argmax—that produces the maximum output. To illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot’s cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball. 2480 The objective is to maximize the distance traveled by the ball. The objective function models the distance travelled as a function of the quantities you can control, for instance the vertical angle of launch or the amount by which you pull back the slingshot. For simplicity, we’ll imagine that the slingshot is pulled back by a standard amount, producing a velocity of the ball at release of \\(v_0\\). Since \\(v_0\\) is fixed, you’ll win or lose based on the angle of launch you choose. 2485 Before you head out into the field to experiment, let’s do a bit of preparation for constructing the objective function. Using some principles of physics and mathematics (which you may not yet understand), we’ll model how far the ball will travel (horizontally) as a function of the angle of launch \\(\\theta\\) and the initial velocity \\(v_0\\). 2490 The mathematics of such problems involves an area called differential equations, an important part of calculus which we’ll come to later in the course. Since you don’t have the tools yet, we’ll just state a simple model of how long the ball stays in the air. \\[\\text{duration}(v_0, \\theta) = 2 v_0 \\sin(\\theta)/g\\] \\(g\\) is the acceleration due to gravity, which is about \\(9.8 \\text{m}\\text{s}^{-2}\\), assuming that the contest is being held on Earth. The horizontal distance travelled by the tennis ball will be \\[\\text{hdist}(v_0, \\theta) = \\cos(\\theta) v_0\\, \\text{duration}(v_0, \\theta) = 2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\] Our objective function is hdist(), and we seek to find the argmax. The input \\(v_0\\) is (we have assumed) fixed, so the only decision quantity is the angle \\(\\theta\\). The best choice of \\(\\theta\\) will make the quantity \\(\\cos(\\theta)\\sin(\\theta)\\) as large as possible. So in finding the argmax, we don’t need to be concerned with \\(v_0\\) or \\(g\\). Finding the argmax can be accomplished simply by plotting the function \\(\\cos(\\theta)\\sin(\\theta)\\). We’ll implement the function so that the input is in units of degrees. Figure 23.1: In the simple model of a tennis ball launched at an angle \\(\\theta\\) from the horizontal, the distance travelled is \\(2 v_0^2 / g\\) times \\(\\cos(\\theta)\\sin(\\theta)\\). You can see that the maximum value is about 0.5 and that this occurs at an argmax \\(\\theta\\) that’s a little bit less than 50\\(^\\circ\\). Zooming in on the \\(\\theta\\) axis let’s you find the argmax with more precision: Figure 23.2: Zooming in on the argmax of the objective function. It’s important to look at the scale of the vertical axis. Any value of \\(\\theta\\) between about 40 and 50 gives a very close approximation to the maximum. From the graph, especially the zoomed-in version, you can read off the argmax as \\(\\theta = 45^\\circ\\). Finding the argmax solves the problem. You may also want to present your solution by saying what the value of the output of hdist() is when the argmax is given as input. You can read off the graph that the maximum of \\(\\cos(\\theta)\\sin(\\theta)\\) is 0.5 at \\(\\theta = 45^\\circ\\), so overall the distance will be \\(v_0^2 / g\\) 2495 23.2 Interpreting the argmax The graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends of course on whether the model we built for the objective function is correct. There are potentially important things we have left out, such as air resistence. 2500 Solving the optimization problem has prepared us to go out in the field and test the result. Perhaps we’ll find that the real-world optimum angle is somewhat steeper or shallower than \\(\\theta = 45^\\circ\\). 2505 Besides the argmax, another important quantity to read from the graph in Figure 23.1 is the precision of the argmax. In strict mathematical terms, the argmax for the tennis-ball problem is exactly 45 degrees at which point \\(\\cos(\\theta)\\sin(\\theta) = 0.5\\). Suppose, however, that the ball were launched at only 40 degrees. Five degrees difference is apparent to the eye, but the result will be essentially the same as for 45 degrees: \\(\\cos(\\theta)\\sin(\\theta) = 0.492\\). The same is true for a launch angle of 50 degrees. For both “sub-optimal” launch angles, the output is within 2 percent of the 45-degree result. It’s easy to imagine that a factor outside the scope of the simple model—the wind, for instance—could change the result by as much or more than 2 percent, so a practical report of the argmax should reasonable be “40 to 50 degrees” rather than “exactly 45 degrees.” 2510 Contests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, \\(45^\\circ\\) is the argmax of the model. Reality may deviate from the model. For instance, suppose that air resistance or wind might have an effect of about 1% on the distance. Since the real-world function might deviate by as much as 1% of the model value, we shouldn’t expect the real-world argmax to be any closer to 45\\(^\\circ\\) than \\(\\pm 5^\\circ\\), since anywhere in that input domain generates an output that is within 1% of the maximum output for the model. 2515 23.3 Derivatives and optimization We’re now going to reframe the search for the argmax and it’s interpretation in terms of derivatives of the objective function with respect to the decision quantity (\\(\\theta\\) in the slingshot problem). For a function of one variable, this will not be an improvement from the look-at-the-graph technique to find the argmax. A genuine reason to use derivatives is to set us up in the future to solve problems with more than one variable, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax. 2520 With a graph such as Figure 23.1, it’s easy to find the argmax; common sense carries the day. So it won’t be obvious at first why we are going to take the following approach: Let’s denote an argmax of the objective function \\(f(x)\\) by \\(x^\\star\\). Let’s look at the derivative \\(\\partial_x f(x)\\) in the neighborhood of \\(x^\\star\\). Referring to Figure 23.1, where \\(x^\\star = 45^\\circ\\), you may be able to see that \\(\\partial_x f(x^\\star)\\) is zero; the line tangent to the function’s graph at \\(x^\\star\\) is horizontal. 2525 Seen another way, the slope of \\(f(x)\\) to the left of \\(x^\\star\\) is positive. Move a tiny bit to the right (that is, increase \\(x\\) by a very small amount) leads to an increase in the output \\(f(x)\\). Just to the right of \\(x^\\star\\), the slope of \\(f(x)\\) is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of \\(x^\\star\\) and negative on the other, suggesting that it crosses zero at the argmax. 2530 Common sense is correct: Walk uphill to get to the peak, walk downhill to move away from the peak. When you come to the top of a smooth hill, the terrain is level. (Since our modeling functions are smooth, so must be the hills that we visualize the functions with.) Inputs \\(x^\\star\\) such that \\(\\partial_x f(x^\\star) = 0\\) are called critical points. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it’s even possible to have the slope be zero at a point that’s neither an argmin or an argmax. 2535 At this point, we know that values \\(x^\\star\\) that give \\(\\partial_x f(x^\\star) = 0\\) are “critical points,” but we haven’t said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of \\(\\partial_x f(x)\\) near \\(x=x^\\star\\) is important. If \\(x^\\star\\) is an argmax, then \\(\\partial_x f(x)\\) will be positive to the left of \\(x^\\star\\) and negative to the right of \\(x^\\star\\); walk up the hill to get to \\(x^\\star\\), at the top the hill is flat, and just past the top the hill has a negative slope. 2540 For an argmin, changing \\(x\\) from less than \\(x^\\star\\) to greater than \\(x^\\star\\); you will be walking down into the valley, then level at the very bottom \\(x=x^\\star\\), then back up the other side of the valley after you pass \\(x=x^\\star\\). Figure 23.3 shows the situation. 2545 Figure 23.3: Top row: An objective function near an argmax (left) and an argmin (right). Bottom row: The derivative of the objective function. A horizontal line (orange) has been added to mark zero on the vertical axis. The bottom row of graphs in Figure 23.3 shows the derivative of the objective function \\(f(x)\\), that is, \\(\\partial_x f(x)\\). You can see that for the argmax of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is positive to the left and negative to the right. Similarly, near the argmin of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is negative to the left and positive to the right. 2550 Stated another way, the derivative \\(\\partial_x f(x)\\) has a negative slope just to the left of an argmin and a positive slope to the left of an argmax. 2555 The second derivative of the objective function \\(f(x)\\) at a critical point \\(x^\\star\\) is what tells us whether the critical point is an argmax, an argmin, or neither. Critical point \\(x^\\star\\) \\(\\partial_x f(x^\\star)\\) \\(\\partial_{xx} f(x^\\star)\\) argmax 0 negative argmin 0 positive neither 0 0 Throughout Block 2, we have translated features of functions that are evident on a graph into the language of derivatives: The slope of a function \\(f(x)\\) at any input \\(x\\) is the value of the derivative function \\(\\partial_x f(x)\\) at that same \\(x\\). The concavity of a function \\(f(x)\\) at any input is the slope of the derivative function, that is, \\(\\partial_{xx} f(x)\\). Putting (i) and (ii) together, we get that the concavity of a function \\(f(x)\\) at any input \\(x\\) is the value of the second derivative function, that is, \\(\\partial_{xx} f(x)\\). At an argmax \\(x^\\star\\) of \\(f(x)\\), the value of the derivative function \\(\\partial_x f(x^\\star)\\) is zero and the value of the second derivative function \\(\\partial_{xx} f(x^\\star)\\) is negative. The situation at an argmin is along the same lines, the derivative of the objective function is zero and the second derivative is positive. 2575 What’s the critical point? You’re familiar with the quadratic polynomial: \\[g(x) = a_0 + a_1 x + a_2 x^2\\] The graph of a quadratic polynomial is a parabola, which might be concave up or concave down. As you know, a parabola has only one critical point, which might be an argmin or an argmax. 2580 Let’s find the critical point. We know that the critical point is \\(x^\\star\\) such that \\(\\partial_x g(x^\\star) = 0\\). Since we know how to differentiate a power law, we can see that \\[\\partial_x g(x) = a_1 + 2 a_2 x\\] and, more specifically, at the critical point \\(x^\\star\\) the derivative will be \\[a_1 + 2 a_2 x^\\star = 0\\] The above is an equation, not a definition. It says that whatever \\(x^\\star\\) happens to be, the quantity \\(a_1 + 2 a_2 x^\\star\\) must be zero. Using plain old algebra, we can find the location of the critical point \\[x^\\star = -\\frac{a_1}{2 a_2}\\] 2585 In economics, a monopoly or similar arrangement can set the price for a good or commodity. Monopolists can set the price at a level that generates the most income for themselves. Figure 23.4: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. Source) In 1836, early economist Antoine-Augustin Cournot published a theory of revenue versus demand based on his conception that demand will be a monotonically decreasing function of price. (That is, higher price means lower demand.) We’ll write as \\(\\text{Demand}(p)\\) demand as a function of price. The revenue generated at price \\(p\\) is \\(R(p) \\equiv p \\text{Demand}(p)\\): price times demand. To find the revenue-maximizing demand, differentiate \\(R(p)\\) with respect to \\(p\\) and find the argmax \\(p^\\star\\) at with \\(\\partial_p R(p^\\star) = 0).\\) This can be done with the product rule. \\[\\partial_p R(p) = p \\ \\partial_p \\text{Demand}(p) + \\text{Demand}(p)\\] At the argmax \\(p^\\star\\) we have: \\[p^\\star \\partial_p \\text{Demand}(p^\\star) + \\text{Demand}(p^\\star) = 0 \\ \\ \\stackrel{\\text{solving for}\\ p^\\star}{\\Longrightarrow} \\ \\ p^\\star = - \\frac{\\text{Demand}(p^\\star)}{\\partial_p \\text{Demand}(p^\\star)}\\] If the monopolist knows the demand function \\(D(p)\\), finding the revenue maximizing price is a simple matter. But in general, the monopolist does not know the demand function in advance. Instead, an informed guess is made to set the initial price \\(p_0\\). Measuring sales \\(D(p_0)\\) gives one point on the demand curve. Then, try another price \\(p_1\\). This gives another point on the demand curve as well as an estimate \\[\\partial_p D(p_0) = \\frac{D(p_1) - D(p_0)}{p_1 - p_0}\\] Now the monopolist is set to model the demand curve as a straight-line function and easily to find \\(p^\\star\\) for the model. For instance, if the demand function is modeled as \\(D_1 (p) = a + b p\\), the optimal price will be \\(p^\\star_1 = - \\frac{a + b p^\\star}{b}\\) which can be solved as \\(p^\\star_1 = - a/2b\\). \\(p^\\star_1\\) is just an estimate of the optimum price. Still, the monopolist can try out that price, giving a third data point for the demand function from which a better model of the demand function can be constructed. With the better estimate, find a new a argmax \\(p^\\star_2\\). This sort of iterative process for finding an argmax of a real-world function is very common in practice. 23.4 Be practical! Decision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding a value for an input variable that will produce the “best” outcome. For those who have studied calculus, it’s natural to believe that calculus-based techniques for optimization are the route to making the decision. We emphasize that the optimization techniques covered in this chapter are only part of a broader set of techniques for real-world decision-making problems. In particular, most policy contexts involve multiple objectives. For example, in designing a car one goal is to make it cheap to manufacture, another to make it attractive, and still another to make it safe. These different objectives are often at odds with one another. In Block 4 of this text, we’ll discuss some calculus techniques that help policy-makers in multi-objective settings. For now, sticking with the idealized (and often unrealistic) setting of maximizing a single objective, with one or more inputs. Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, \\(f(x)\\) or \\(g(x,y)\\) or, often, \\(h(x, y, z, \\ldots)\\) where \\(\\ldots\\) might be standing for tens or hundreds or thousands of variables or more. If you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The calculus-based techniques were developed for situations where such graphing is not possible and, instead, you have a formula for the function. (Such occasions are of great theoretical interest but not all that common in practice.) The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0. As an example, consider a style problem that often appears in calculus textbooks. You have been tasked to design a container for a large volume V of liquid. It is desired to make the weight of the container as little as possible. (This is a minimization problem, then.) In classical textbook fashion, you are told that the container is to be a cylinder made out of a particular metal of a particular thickness. This is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question. Using the notation in the diagram, the volume and surface area of the cylinder is \\[V(r, h) \\equiv \\pi r^2 h \\ \\ \\ \\text{and}\\ \\ \\ A(r, h) \\equiv 2 \\pi r^2 + 2 \\pi r h\\] Minimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume \\(V\\) is given (according to the problem statement), we want to re-write the area function to use volume: \\[h(r, V) \\equiv V / \\pi r^2 \\ \\ \\ \\implies\\ \\ \\ A(r, V) = 2 \\pi r^2 + 2 \\pi r V/\\pi r^2 = 2 \\pi r^2 + 2 V / r\\] Suppose \\(V\\) were specified as 1000 liters. A good first step is to choose appropriate units for \\(r\\) to make sure the formula for \\(A(r, V)\\) is dimensionally consistent. Suppose we choose \\(r\\) in cm. Then we want \\(V\\) in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function: A &lt;- makeFun(2*pi*r^2 + 2*V/r ~ r, V=1000000) slice_plot(A(r) ~ r, domain(r=c(10, 100))) %&gt;% gf_labs(x = &quot;radius (cm)&quot;, y = &quot;Surface area of container (square cm)&quot;) As always, the function’s derivative is zero at the optimal \\(r\\). In the graph, the argmin is near \\(r=50\\) cm at which point the minimum is about 50,000 cm\\(^2\\). Since \\(h(r,V) = V/\\pi r^2\\), the required height of cylinder will be near \\(10^6 / \\pi 50^2 = 127\\)cm. In calculus courses, the goal is often to find a formula for the optimal radius as a function of \\(V\\). So we differentiate the objective function—that is, the area function for any \\(V\\) and \\(r\\) with respect to \\(r\\), \\[\\partial_r A(r, V) = 4 \\pi r - 2 V / r^2\\] Setting this to zero (which will be true at the optimal \\(r^\\star\\)) we can solve for \\(r^\\star\\) in terms of \\(V\\): \\[4 \\pi r^\\star - 2 \\frac{V}{\\left[r^\\star\\right]^2} = 0 \\ \\ \\ \\Longrightarrow\\ \\ \\ 4\\pi r^\\star = 2\\frac{V}{\\left[r^\\star\\right]^2} \\Longrightarrow\\ \\ \\ \\left[r^\\star\\right]^3 = \\frac{1}{2\\pi} V \\ \\ \\ \\Longrightarrow\\ \\ \\ r^\\star = \\sqrt[3]{V/2\\pi}\\] For \\(V = 1,000,000\\) cm\\(^3\\), this gives \\(r^\\star = 54.1926\\) cm which in turn implies that the corresponding height \\(h^\\star = V/\\pi (r^\\star)^2 = 108.3852\\) cm. We’ve presented the optimum \\(r^\\star\\) and \\(h^\\star\\) to the nearest micron. (Does that make sense to you? Think about it for a moment before reading on.) A good rule of thumb in modeling is this: “If you don’t know what a sensible precision is for reporting your result, you don’t have a complete grasp of the problem.” Here are two reasonable ways to sort out a suitable precision. Solve a closely related problem which for many practical purposes would have been equivalent. Look at how big a change in the output of the objective function is produced by a change from the argmax. Approach (2) is always at hand, since you already know the objective function. Let’s graph the objective function near \\(r = 54.1926\\) … Look carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units out of 55,000. In other words, about 0.7%. It’s true that \\(r^\\star = 54.1926\\) cm gives the “best” outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: often the difference between silver and gold! What’s different is that you know exactly what is the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your “optimal” tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the designers of the aircraft want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. To save that much drag, it’s probably well worth increasing weight by 0.7%. In reporting the results from an optimization problem, you ought to give the decision maker all relevant information. Here, that might be as simple as including the above graph in your report. We mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. This often requires some insight and creativity to frame the new problem. Here, we note that large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends. Using \\(h\\) for the length of the cylindrical portion of the tank, and \\(r\\) for the radius, the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Again, \\(V\\) was specified as 1000 liters. As detailed in Exercise 23.18, the surface area of this 1000-liter tank is about 48,400 cm\\(^2\\). This is more than 10% less than for the cylindrical tank. 23.5 Exercises Exercise 23.02: 0zyL02 The simple model of the distance travelled by a tennis ball after launch from a slingshot is \\[\\text{hdist}(v_0, \\theta) = 2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\] where \\(\\theta\\) is the launch angle, measured from the horizontal, \\(v_0\\) is the initial velocity, and \\(g\\) is the acceleration due to gravity. Question A What is the dimension of \\(v_0\\)?     L T︎✘        L / T\\(\\heartsuit\\ \\)As in miles/hour or meters/second.       L / T\\(^2\\)︎✘        L\\(^2\\) / T\\(^2\\)︎✘ Question B What is the dimension of \\(g\\)?     M L︎✘        M L / T︎✘        L / T\\(^2\\)\\(\\heartsuit\\ \\)       M L / T\\(^2\\)︎✘ This is force, not acceleration. Question C What is the dimension of \\(\\theta\\)? L︎✘ L^2︎✘ T/L︎✘ It’s dimensionlessGood. An angle is measured by distance along the circumference of a circle divided by the radius of the circle, so L/L. The L dimensions cancel out. Question D What is the dimension of \\(\\sin(\\theta\\))? L︎✘ L^2︎✘ T/L︎✘ It’s dimensionlessExcellent! The output of a sinusoid is always dimensionless. You would have to multiply by a dimensionful amplitude (\\(A \\sin(\\theta)\\)) to attach a dimension to the output. Question E What is the dimension of \\(2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\)?     L\\(\\heartsuit\\ \\)       L^2︎✘        L/T︎✘        L/T\\(^2\\)︎✘ Question F Suppose the initial velocity of the ball is \\(v_0 = 10\\) meters/second. Since we’re on the surface of Earth, \\(g=9.8\\) meters/second-squared. At the optimal launch angle \\(\\theta\\), how far does the model predict the ball will travel?     10 meters\\(\\heartsuit\\ \\)       25 meters︎✘        50 meters︎✘        75 meters︎✘ Exercise 23.04: FbJynV Question A Consider the function \\(f(x) \\equiv x^3\\). Confirm that the value of the derivative \\(\\partial_x f(x = 0)\\) and so \\(x^\\star = 0\\) is a critical point. Which sort of critical point is \\(x^\\star=0\\)? (Hint: Draw the graph of \\(f(x)\\) near \\(x=0\\) to see what’s going on.) An argmax︎✘ But \\(f(0) &lt; f(x &gt; 0)\\), so \\(x^\\star=0\\) can’t be an argmax. An argmin︎✘ But \\(f(x &lt; 0) &lt; f(0)\\), so \\(x^\\star=0\\) can’t be an argmin. NeitherRight!  Question B Still working with the function \\(f(x) \\equiv x^3\\), find the value of the second-derivative \\(\\partial_{xx} f(x^\\star)\\) evaluated at the critical point \\(x = x^\\star = 0\\). Which of these is \\(\\partial_{xx} f( x=0)\\)? Negative︎✘ But you established in the previous exercise that the critical point \\(x^\\star=0\\) is neither an argmin nor wan argmax. Positive︎✘ But you established in the previous exercise that the critical point \\(x^\\star=0\\) is neither an argmin nor wan argmax. ZeroExcellent!  Exercise 23.06: ykelx The graph shows three different functions labeled (A), (B), and (C). Question A Function (A) is     concave down\\(\\heartsuit\\ \\)       non-concave︎✘        concave up︎✘ Question B Function (B) is     concave down︎✘        non-concave︎✘        concave up\\(\\heartsuit\\ \\) Question C Function (C) is     concave down︎✘        non-concave\\(\\heartsuit\\ \\)       concave up︎✘ Question D The negative of function (A) is     concave down︎✘        non-concave︎✘        concave up\\(\\heartsuit\\ \\) The graph shows a function \\(\\text{wave}(t) \\equiv \\sin(2 \\pi t/4)\\) and labels four input values \\(t\\). Question E For what values of the input \\(t\\) is the function concave up?     \\(t = A\\) and \\(t=D\\)︎✘        \\(t = A\\) and \\(t=C\\)︎✘        \\(t = C\\) and \\(t = D\\)\\(\\heartsuit\\ \\)       none of the above︎✘ Question F For what values of the input \\(t\\) is the function non-concave?     \\(t = A\\)\\(\\heartsuit\\ \\)       \\(t=B\\)︎✘        \\(t=C\\)︎✘        none of the above︎✘ Question G Where is the function steepest?     \\(t = A\\)\\(\\heartsuit\\ \\)       \\(t=B\\)︎✘        \\(t=C\\)︎✘        \\(t=D\\)︎✘ Exercise 23.08: lexkd Here is a smooth function marked at a few points. Your task is, at each point, to estimate the value of the derivative, the sign of the second derivative, and the radius of the circle that would nicely match the function in a small region around each point. (Remember, we’re asking for the radius of the circle, which is half the diameter.) To simplify things, here is a table giving seven different combinations of the quantities you are to estimate. Some of them correctly match one of the labeled points, some do not. All you need to do is choose which is the correct set of quantities for each labeled point. row value of 1st deriv sign of 2nd deriv radius                                        i -0.3 pos 0.25 ii 2.1 near 0 2000 iii -1.4 neg 12 iv 0.3 neg 0.3 v 2.1 pos 0.1 vi 1.3 neg 3 vii 0.5 pos 1 Question A Which row from the table best matches the function at point A?     i︎✘        ii︎✘        iii︎✘        iv︎✘        v︎✘        vi︎✘        vii\\(\\heartsuit\\ \\) Question B Which row from the table best matches the function at point B?     i︎✘        ii\\(\\heartsuit\\ \\)       iii︎✘        iv︎✘        v︎✘        vi︎✘        vii︎✘ Question C Which row from the table best matches the function at point C?     i︎✘        ii︎✘        iii︎✘        iv\\(\\heartsuit\\ \\)       v︎✘        vi︎✘        vii︎✘ Question D Which row from the table best matches the function at point D?     i\\(\\heartsuit\\ \\)       ii︎✘        iii︎✘        iv︎✘        v︎✘        vi︎✘        vii︎✘ Question E Which row from the table best matches the function at point E?     i︎✘        ii︎✘        iii︎✘        iv︎✘        v︎✘        vi\\(\\heartsuit\\ \\)       vii︎✘ Exercise 23.10: dwes You and your pet dog Swimmer often go to the beach and walk along the water’s edge. You throw a ball down the beach, but at an angle so it lands in the water. Swimmer goes to work. She runs down the beach (fast) and then plunges into the water, heading toward the ball. She can run fast on the beach: 400 m/minute. But she swims rather slower: 50 m/min. Suppose you threw the ball to a point about 50 meters down the beach and 10 meters out in the water. The overall distance to the ball is therefore \\(\\sqrt{50^2 + 10^2} \\approx 51\\) meters. If Swimmer entered the water immediately, she would take about a minute to reach the ball (51 m / 50 m/min). Swimmer can get to the ball faster by running down the beach a bit and then turning into the water. If Swimmer ran all 50 meters down the beach and then turned to swim the 10 meters, it would take her (50/400 + 10/50) minutes, about one-third of a minute. Figure 23.5: Swimmer’s optimal path to the ball consists of running \\(x\\) meters along the shore, then swimming diagonally to the ball. Can Swimmer do better? You can set up the calculation like this. Imagine \\(x\\) to be the distance down the beach that Swimmer runs. The time to run this distance will be \\(x/400\\). The distance remaining to the ball can be found by the Pythagorean theorem. One leg of the triangle has length \\((50-x)\\), the other has length 10 m. So, the length of the third side is \\(\\sqrt{\\strut (50-x)^2 + 10^2}\\). For instance, if \\(x\\) were 45, the distance to swim in the water would be \\(\\sqrt{(50-45)^2 + 10^2} = 11.2\\) m. Divide this distance by 50 m/min to get the time spent in the water. distance_in_water &lt;- makeFun( __your_pythagorean_formula ~ x) time_to_ball &lt;- makeFun(x/400 + distance_in_water(x)/50 ~ x) Time_to_ball() takes one argument, the distance \\(x\\) Swimmer runs down the beach before turning into the water. Use a SANDBOX to find the distance that calculus-savvy Swimmer runs down the beach before turning into the water, if Swimmer’s goal is to get to the ball as fast as possible. Question A What’s the optimal running distance for Swimmer?     46.75︎✘        47.5︎✘        48.75\\(\\heartsuit\\ \\)       49.75︎✘ Here’s a news story about a mathematician’s dog on the shore of Lake Michigan. It’s not plausible that Swimmer has been trained in calculus. Perhaps the way Swimmer solves the running distance problem is simply to graph time_to_ball(x) ~ x over a suitable domain and find the argmax by eye! Exercise 23.12: whfrt If you’re skeptical that a dog might do a calculus problem before running to fetch a ball, consider the path taken by a photon. “Fermat’s Principle” is that light takes the path of least time. To illustrate, consider the problem of a photon travelling from a point A to a point B, as in the diagram. The shortest path between the two points is a straight line. Along this straight-line path, the time taken by the photon will be the distance divided by the speed of light. The diagram shows another path consisting of two segments, one of length \\(l_1\\) and the other \\(l_2\\). Obviously, the two-segment path is longer than the straight-line path. But according to Fermat’s principle, light “prefers” the longer path if the time taken to traverse it is shorter. This phenomenon is called refraction. The reason the indirect path might be shorter is that the speed of light differs in different physical media. Light traveling in a vacuum famously has a speed of about 300,000 km per second. In air, the speed is smaller by a factor of 1/1.003. In water, the speed is smaller still: the factor is 1/1.3. Imagine that the blue zone of the diagram is water and the clear zone air. The time for the photon to travel from point A to B is proportional to \\(1.003\\ l_1 + 1.3\\ l_2\\). To see the path actually taken by light, let’s imagine that point A is \\((x=0, y=10)\\) and point B is \\((x=20, y=-10)\\), and that the boundary between water and air is at \\(y=0\\). We’ll place the point P at \\((x, 0)\\). The total time taken for light to traverse this path is 1.003 dist(A to P) + 1.3 dist(P to B). Question A Which of these formulas gives the total time it takes for light to traverse the path from A to P at relative speed 1/1.003 and then the path from P to B at relative speed 1/1.3? A is located at \\((0, 10)\\), B is located at \\((20,-10)\\), and P is located at \\((x, 0)\\) \\(1.003 \\sqrt{(x-0)^2 +(0-10)^2}+ 1.3\\sqrt{(20-x)^2 + (-10 - 0)^2}\\)Nice!  \\(\\sqrt{(x-0)^2 +(0-10)^2}/1.003+ \\sqrt{(20-x)^2 + (-10 - 0)^2}/1.3\\)︎✘ It’s true that you divide distance by speed to get time, but here the relative speeds are \\(1/1.003\\) and \\(1/1.3\\). \\(1.003 \\sqrt{(x-10)^2 +(0-0)^2}+ 1.3\\sqrt{(-10 -x)^2 + (50 - 0)^2}\\)︎✘ This mixes up the x and y coordinates. The distance from A to P is \\(\\sqrt{(x_P - x_A)^2 + (y_P - y_A)^2}\\). In this problem, point P is at \\((50-x, 0)\\). Implement the calculation of total_time() in R, then use a graph to find the argmin. total_time &lt;- makeFun( your_formula ~ x) slice_plot(total_time(x) ~ x, domain(x=c(0, 20))) # For the next problem dx_time &lt;- D(total_time(x) ~ x) dxx_time &lt;- D(total_time(x) ~ x &amp; x) Question B What value of \\(x\\) (that is, the argmin) minimizes the travel time of light between points A and B? (Choose the best answer)     10.52︎✘        11.02︎✘        12.22︎✘        12.50\\(\\heartsuit\\ \\)       13.21︎✘        14.94︎✘ Question C Suppose that instead of being water, the blue area was glass. The speed of light in glass is roughly 1/1.5 times as big as in vacuum. What value of \\(x\\) minimizes the travel time of light between points A and B? (Choose the best answer)     13.60\\(\\heartsuit\\ \\)       14.58︎✘        14.85︎✘        15.54︎✘ Exercise 23.14: jgtrds Turn this into a problem about algebra. Later in accumulation, figure out the mile-markers on the road. There is a tradition in mathematics education of using geometrical, distance-related problems to illustrate optimization. As it happens, some such problems can provide some valuable insight into physical situations. Suppose you have a function \\(f(x)\\) whose graph represents the path of a road through the jungle. There is a lion at coordinates \\((3, 2)\\) in the jungle. At what point as you travel along the road will you be closest to the lion. The trick here is to see that your position on the road at any value of \\(x\\) is \\((x, f(x))\\). The distance to the lion as a function of \\(x\\) is \\[ \\text{lion.dist}(x) \\equiv \\sqrt{(x - 3)^2 + (f(x)-2)^2} .\\] The road and the lion are graphed in the sandbox for the road \\(f(x) \\equiv x^2 e^{-x}\\). You can see that the road comes closest to the lion at roughly \\(x=3\\). When you implement the lion.dist\\((x)\\) function, you’ll be able to find the argmin, that is, the value of the input \\(x\\) for which the output lion_dist\\((x)\\) is minimal. The value of the function lion_dist\\((x)\\) at the argmin is the min of the function. slice_plot(x^2 * exp(-x) ~ x, domain(x=c(0,5))) %&gt;% gf_label(2 ~ 3, label=&quot;lion&quot;) %&gt;% gf_refine(coord_fixed()) To find the argmin of the distance to the lion more precisely, implement the \\(\\text{lion.dist}(x)\\) function in the sandbox. Plot this out and look for the minimum. Alternatively, find \\(\\partial_x \\text{lion.dist}(x)\\) and find the \\(x\\) of the zero crossing. Question A Using the sandbox above, find the argmin for which the graph of the function \\(g(x) \\equiv x^2 \\exp(-x)\\) comes closest to the point \\((3, 2)\\)? (Choose the best answer.)     2.03︎✘        2.07︎✘        2.21︎✘        2.27︎✘        2.28︎✘        2.33︎✘        2.37︎✘        2.39︎✘        2.43︎✘        2.48︎✘        2.58︎✘        2.71︎✘        2.80\\(\\heartsuit\\ \\)       2.94︎✘ Exercise 23.18: ph92lT Return to the problem of finding the optimal radius of a cylindrical tank with spherical ends. The point is to choose the sphere radius \\(r\\) and the cylinder length \\(h\\) that minimize the surface area of the tank while enclosing a specified volume. In terms of \\(h\\) and \\(r\\), the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Question A Which of these is correct? (Hint: Only one of the answers is dimensionally consistent.) \\(h(r, V) = (V-4\\pi r^3/3)/ \\pi r^2\\)Good.  \\(h(r, V) = (V-4\\pi r^2/2)/ \\pi r^2\\)︎✘ \\(h(r, V) = (V-4\\pi r^3/3)/ \\pi r^3\\)︎✘ \\(h(r, V) = \\sqrt{(V-4\\pi r^3/2)/\\pi r^3}\\)︎✘ Question B Which of these is the correct expression for \\(A(r, V)\\) \\(A(r, V) = 2 V/r + \\frac{8 \\pi}{3} r^2\\)︎✘ \\(A(r, V) = 2 V/r + \\frac{4 \\pi}{3} r^2\\)Good.  \\(A(r, V) = V/r + \\frac{20 \\pi}{3} r^2\\)︎✘ \\(A(r, V) = V/r + 3 \\pi r^2\\)︎✘ Question C Find \\(\\partial_r A(r, V)\\) and set to zero. Solve for \\(r^\\star\\) in terms of \\(V\\). Which of these is correct? \\(r^\\star = \\sqrt[3]{\\frac{3}{4\\pi} V}\\)Excellent!  \\(r^\\star = \\sqrt[3]{\\frac{4}{3\\pi} V}\\)︎✘ \\(r^\\star = \\sqrt[3]{\\frac{3\\pi}{4} V}\\)︎✘ \\(r^\\star = \\sqrt[3]{\\frac{3}{2\\pi} V}\\)︎✘ Find the optimum value of \\(r\\) to minimize \\(A(r,V)\\) when \\(V = 1000\\) liters. Question D What is the optimal value of \\(r\\) in cm to a precision of one micron?     6.2035︎✘ Check that you were using the right units for \\(V\\).       46.0351︎✘        52.0351︎✘        62.0351\\(\\heartsuit\\ \\) Use a sandbox to plot a graph of \\(A(r, V)\\) versus r, setting \\(V = 1000\\) liters. Question E From the graph of \\(A(r, V)\\) versus \\(r\\) at \\(V=1000\\) liters, read off a range of \\(r\\) that produces \\(A\\) no worse than 1% greater than the minimum. How wide is that range, approximately?     \\(\\pm 0.1\\)cm\\(&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;\\)\\(cm\\)︎✘        \\(\\pm 5\\)cm\\(&lt;span class=&#39;mcanswer&#39;&gt;\\) \\(&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;\\)\\(cm\\)︎✘ Another word for an “input” is “argument.” Argmax is the contraction of argument producing the maximum output.↩︎ "],["partial-change.html", "Chapter 24 Partial change 24.1 Calculus on two inputs 24.2 All other things being equal … 24.3 Gradient vector 24.4 Total derivative (optional) 24.5 Differentials 24.6 Exercises", " Chapter 24 Partial change This is a good time to point out something we have been doing all along, but which has likely been such a persistent component of your mathematics education that you may not have realized that it is a construction. 2600 We have two ways by which we represent functions: As a computational algorithm for generating the output from an input(s), typically involving arithmetic and such. As a geometrical entity, specifically the graph of a function which can be a curve or, for functions of two inputs, a surface. These two modes are sometimes intertwined, as when we use the name “line” to refer to a computational object: \\(\\line(x) \\equiv a x + b\\). Unfortunately for functions of two inputs, a surface is hard to present in the formats that are most easily at hand: a piece of paper, a printed page, a computer screen. That’s because a curved surface is naturally a 3-dimensional object, while paper and screens provide two-dimensional images. Consequently, the graphics mode we prefer for presenting functions of two variables is the contour plot, which is not a single geometrical object but a set of many objects: contours, labels, colored tiles. 2605 We’ve been doing calculus on functions of one variable because it is so easy to exploit both the computational mode and the graphical mode. And it might fairly be taken as a basic organizing theme of calculus that 2610 a line segment approximates a curve in a small region around a point. When figuring out the derivative function \\(\\partial_x f(x)\\) from a graph of \\(f(x)\\), we find the tangent to the graph at each of many input values, record the slope of the line (and throw away the intercept) and then write down the series of slopes as a function of the input, typically by representing the slope by position along the vertical axis and the corresponding input by position along the horizontal axis. Figure 24.1 shows the process. 2615 Figure 24.1: (A) The graph of a smooth function annotated with small line segments that approximate the function locally. The color of each labeled segment corresponds to the value of \\(x\\) for that segment. The slope of each segment is written numerically below the segment. (B) The labeled dots show the slope of each segment from (A). The slope is encoded using vertical position (as usual) and carries over the numerical label from (A). Connecting the dots sketches out the derivative of the function in (A). Panel (A) in Figure 24.1 shows a smooth function \\(f(x)\\) (thin black curve). To find the function \\(\\partial_x f(x)\\), we take the slope of \\(f(x)\\) at many closely spaced inputs. In Panel (A), we’ve highlighted short, tangent line segments at the closely-spaced points labeled A through V. The slope of each tangent line segment can be calculated by the usual rise-over-run method; the numerical value of the slope is written underneath the segment. To plot the derivative \\(\\partial_x f(x)\\), I have taken the slope information from (A) and plotted it as a function of \\(x\\). 2620 To restate what you already know, in the neighborhood of any input value \\(x\\), the slope of any local straight-line approximation to \\(f(x)\\) is given by the value of of \\(\\partial_x f(x)\\). 24.1 Calculus on two inputs Although we use contour plots for good practical reasons, the graph of a function \\(g(x,y)\\) with two inputs is a surface, as described in Section ??. The derivative of \\(g(x,y)\\) should encode the information needed to approximate the surface at any input \\((x,y)\\). In particular, we want the derivative of \\(g(x,y)\\) to tell us the orientation of the tangent plane to the surface. A tangent plane is infinite in extent. Let’s use the word facet to refer to a little patch of the tangent plane centered at the point of contact. Each facet is flat. (It’s part of a plane!) Figure 24.2 shows some facets tangent to a familiar curved surface. No two of the facets are oriented the same way. Figure 24.2: A melon as a model of a curved surface such as the graph of a function of two inputs. Each tangent facet has its own orientation. (Disregard the slight curvature of the small pieces of paper. Summer humidity has interfered with my attempt to model a flat facet with a piece of Post-It paper! Better than a picture of a summer melon, pick up a hardcover book and place it on a curved surface such as a basketball. The book cover is a flat surface: a facet. The orientation of the cover will match the orientation of the surface at the point of tangency. Change the orientation of the cover and you will find that the point of tangency will change correspondingly. 2625 If melons and basketballs are not your style, you can play the same game on an interactive graph of a function of two variables. The snapshot below is a link to an applet that shows the graph of a function as a blue surface. You can specify a point on the surface by setting the value of the (x, y) input using the sliders. Display the tangent plane (which will be green) at that point by check-marking the “Tangent plane” input. (Acknowledgments to Alfredo Sánchez Alberca who wrote the applet using the GeoGebra math visualization system.) 2630 2635 For the purposes of computation by eye, a contour graph of a surface can be easier to deal with. Figure 24.3 shows the contour graph of a smoothly varying function. Three points have been labeled A, B, and C. 2640 Figure 24.3: A function of 2 inputs with 3 specific inputs marked A, B, and C Zooming in on each of the marked points presents a simpler picture for each of them, although one that is different for each point. Each zoomed-in plot contains almost parallel, almost evenly spaced contours. If the surface had been exactly planar over the entire zoomed-in domain, the contours would be exactly parallel and exactly evenly spaced. We can approach such exact parallelness by zooming in more closely around the labeled point. 2645 Figure 24.4: Zooming in on the neighborhoods of A, B, and C in Figure 24.3 shows a simple, almost planar, local landscape. Just as the function \\(\\line(x) \\equiv a x + b\\) describes a straight line, the function \\(\\text{plane}(x, y) \\equiv a + b x + c y\\) describes a plane whose orientation is specified by the value of the parameters \\(b\\) and \\(c\\). (Parameter \\(a\\) is about the vertical location of the plane, not it’s orientation.) 2650 In Figure 24.5, the facets tangent to the original surface at A, B, and C are displayed. Comparing Figures 24.4 and 24.5 you can see that each facet has the same orientation as the surface; the contours face in the same way. 2655 Figure 24.5: The facets around the points are linear functions, each aligned with the contours near that point in Figure 24.3 Remember that the point of constructing such facets is to generalize the idea of a derivative from a function of one input \\(f(x)\\) to functions of two or more inputs such as \\(g(x,y)\\). Just as the derivative \\(\\partial_x f(x_0)\\) reflects the slope of the line tangent to the graph of \\(f(x)\\) at \\(x=x_0\\), our plan for the “derivative” of \\(g(x_0,y_0)\\) is to represent the orientation of the facet tangent to the graph of \\(g(x,y)\\) at \\((x=x_0, y=y_0)\\). The question for us now is what information is needed to specify an orientation. 2660 One clue comes from the formula for a function whose graph is a plane oriented in a particular direction: \\[\\text{plane}(x,y) \\equiv a + b x + cy\\] To explore the roles of the parameters \\(b\\) and \\(c\\) in setting the orientation of the line, open a SANDBOX. The scaffolding code generates a particular instance of \\(\\text{plane}(x,y)\\) and plots it in two ways: a contour plot and a surface plot. Change the numerical values of \\(b\\) and \\(c\\) and observe how the orientation of the planar surface changes in the graphs. You can also see that the value of \\(a\\) is irrelevant to the orientation of the plane, just as the intercept of a straight-line graph is irrelevant to the slope of that line. 2665 plane &lt;- makeFun(a + b*x + c*y ~ x + y, a = 1, b = -2.5, c = 1.6) if (knitr::is_html_output()) { interactive_plot(plane(x, y) ~ x + y, domain(x=c(-2, 2), y=c(-2, 2))) } else { knitr::include_graphics(normalizePath(&quot;www/plane-3d.png&quot;)) } contour_plot(plane(x, y) ~ x + y, domain(x=c(-2, 2), y=c(-2, 2))) %&gt;% gf_refine(coord_fixed()) As always it can be difficult to extract quantitative information from a surface plot. For the example here, you can see that the high-point on the surface is when \\(x\\) is most negative and \\(y\\) is most positive. Compare that to the contour plot to verify that that two modes are displaying the same surface. 2670 (Note: The gf_refine(coord_fixed()) part of the contour-plot command makes numerical intervals on the horizontal and vertical axes have the same length.) An instructive experience is to pick up a rigid, flat object, for instance a smartphone or hardcover book. Hold the object level with pinched fingers at the mid-point of each of the short ends, as shown in Figure 24.6 (left). 2675 Figure 24.6: Combining two simple movements can tip a plane to all sorts of different orientations. You can tip the object in one direction by raising or lowering one hand. (middle picture) And you can tip the object in the other coordinate direction by rotating the object around the line joining the points grasped by the left and right hands. (right picture) By combining these two motions, you can orient the surface of the object in a wide range of directions.14 2680 The purpose of this lesson is to show that two-numbers are sufficient to dictate the orientation of a plane. In terms of Figure 24.6 these are 1) the amount that one hand is raised relative to the other and 2) the angle of rotation around the hand-to-hand axis. 2685 Similarly, in the formula for a plane, the orientation is set by two numbers, \\(b\\) and \\(c\\) in \\(\\text{plane}(x, y) \\equiv a + b x + c y\\). How do we find the right \\(b\\) and \\(c\\) for the tangent facet to a function \\(g(x,y)\\) at a specific input \\((x_0, y_0)\\)? Taking slices of \\(g(x,y)\\) provides the answer. In particular, these two slices: \\[\\text{slice}_1(x) \\equiv g(x, y_0) = a + b\\, x + c\\, y_0 \\\\ \\text{slice}_2(y) \\equiv g(x_0, y) = a + b x_0 + c\\, y\\] Look carefully at the formulas for the slices. In \\(\\text{slice}_1(x)\\), the value of \\(y\\) is being held constant at \\(y=y_0\\). Similarly, in \\(\\text{slice}_2(y)\\) the value of \\(x\\) is held constant at \\(x=x_0\\). 2690 The parameters \\(b\\) and \\(c\\) can be read out from the derivatives of the respective slices: \\(b\\) is equal to the derivative of the slice\\(_1\\) function with respect to \\(x\\) evaluated at \\(x=x_0\\), while \\(c\\) is the derivative of the slice\\(_2\\) function with respect to \\(y\\) evaluated at \\(y=y_0\\). Or, in the more compact mathematical notation: \\[b = \\partial_x \\text{slice}_1(x)\\left.\\strut\\right|_{x=x_0} \\ \\ \\text{and}\\ \\ c=\\partial_y \\text{slice}_2(y)\\left.\\strut\\right|_{y=y_0}\\] These derivatives of slice functions are called partial derivatives. The word “partial” refers to examining just one input at a time. In the above formulas, the \\({\\large |}_{x=x_0}\\) means to evaluate the derivative at \\(x=x_0\\) and \\({\\large |}_{y=y_0}\\) means something similar. 2695 You don’t need to create the slices explicitly in order to calculate the partial derivatives. Simply differentiate \\(g(x, y)\\) with respect to \\(x\\) in order to get parameter \\(b\\) and differentiate \\(g(x, y)\\) with respect to \\(y\\) to get parameter \\(c\\). To demonstrate, we’ll make use of the sum rule: \\[\\partial_x g(x, y) = \\underbrace{\\partial_x a}_{=0} + \\underbrace{\\partial_x b x}_{=b} + \\underbrace{\\partial_x cy}_{=0} = b\\] Similarly, \\[\\partial_y g(x, y) = \\underbrace{\\partial_y a}_{=0} + \\underbrace{\\partial_y b x}_{=0} + \\underbrace{\\partial_y cy}_{=c} = c\\] Get in the habit of noticing the subscript on the differentiation symbol \\(\\partial\\). When taking, for instance, \\(\\partial_y f(x,y,z, \\ldots)\\), all variables other than \\(y\\) are to be held constant. Some examples: 2700 \\[\\partial_y 3 x^2 = 0\\ \\ \\text{but}\\ \\ \\ \\partial_x 3 x^2 = 6x\\\\ \\ \\\\ \\partial_y 2 x^2 y = 2x^2\\ \\ \\text{but}\\ \\ \\ \\partial_x 2 x^2 y = 4 x y \\] 24.2 All other things being equal … Recall that the derivative of a function of one variable, say, \\(\\partial_x f(x)\\) tells you, at each possible value of the input \\(x\\), how much the output will change proportional to a small change in the value of the input. Now that we are in the domain of multiple inputs, writing \\(h\\) to stand for “a small change” is not entirely adequate. Instead, we’ll write \\(dx\\) for a small change in the \\(x\\) input and \\(dy\\) for a small change in the \\(y\\) input. With this notation, we write the first-order polynomial approximation to a function of a single input \\(x\\) as \\[f(x+dx) = f(x) + \\partial_x f(x) \\times dx\\] Applying this notation to functions of two inputs, we have: \\[g(x + \\color{magenta}{dx}, y) = g(x,y) + \\color{magenta}{\\partial_x} g(x,y) \\times \\color{magenta}{dx}\\\\ \\text{and} \\\\ g(x, y+\\color{brown}{dy}) = g(x,y) + \\color{brown}{\\partial_y} g(x,y) \\times \\color{brown}{dy}\\] Each of these statements is about changing one input while holding the other input(s) constant. Or, as the more familiar expression goes, \"The effect of changing one input all other things being equal or all other things held constant.15 2705 Everything we’ve said about differentiation rules applies not just to functions of one input, \\(f(x)\\), but to functions with two or more inputs, \\(g(x,y)\\), \\(h(x,y,z)\\) and so on. 2710 24.3 Gradient vector For functions of two inputs, there are two partial derivatives. For functions of three inputs, there are three partial derivatives. We can, of course, collect the partial derivatives into Cartesian coordinate form. This collection is called the gradient vector. 2715 Just as our notation for differences (\\(\\cal D\\)) and derivatives (\\(\\partial\\)) involves unusual typography on the letter “D,” the notation for the gradient involves such unusual typography although this time on \\(\\Delta\\), the Greek version of “D.” For the gradient symbol, turn \\(\\Delta\\) on its head: \\(\\nabla\\). That is, \\[\\nabla g(x,y) \\equiv \\left(\\stackrel\\strut\\strut\\partial_x g(x,y), \\ \\ \\partial_y g(x,y)\\right)\\] Note that \\(\\nabla g(x,y)\\) is a function of both \\(x\\) and \\(y\\), so in general the gradient vector differs from place to place in the function’s domain. The graphics convention for drawing a gradient vector for a particular input, that is, \\(\\nabla g(x_0, y_0)\\), puts an arrow with its root at \\((x_0, y_0)\\), pointing in direction \\(\\nabla g(x_0, y_0)\\), as in Figure 24.7. 2720 Figure 24.7: The gradient vector \\(\\nabla g(x=1,y=2)\\). The vector points in the steepest uphill direction. Consequently, it is perpendicular to the contour passing through its root. A gradient field (see Figure 24.8) is the value of the gradient vector at each point in the function’s domain. Graphically, in order to prevent over-crowding, the vectors are drawn at discrete points. The lengths of the drawn vectors are set proportional to the numerical length of \\(\\nabla g(x, y)\\), so a short vector means the surface is relatively level, a long vector means the surface is relatively steep. 2725 Figure 24.8: A plot of the gradient field \\(\\nabla g(x,y)\\). 24.4 Total derivative (optional) The name “partial derivative” suggests the existence of some kind of derivative that’s not just a part, but the whole thing. The total derivative is such a whole and gratifyingly made up of it’s parts, that is, the partial derivatives. Suppose you are modeling the temperature of some volume of the atmosphere, given as \\(T(t, x, y, z)\\). This merely says that the temperature depends on both time and location, something that is familiar from everyday life. The partial derivatives have an easy interpretation: \\(\\partial_t T()\\) tells how the temperature is changing over time at a given location, perhaps because of the evaporation or condensation of water vapor. \\(\\partial_x T()\\) tells how the temperature changes in the \\(x\\) direction, and so on. The total derivative gives an overall picture of the changes in a parcel of air, which you can thnk of as a tiny balloon-like structure but without the balloon membrane. The temperature inside the “balloon” may change with time (e.g. condensation or evaporation of water), but as the ballon drifts along with the motion of the air (that is, the wind), the evolving location can change the temperature as well. Think of a balloon caught in an updraft: the temperature goes down as the balloon ascends. For an imaginary observer located in the balloon, the temperature is changing with time. Part of this change is the instrinsic change measured by \\(\\partial_t T\\) but we need to add to that the changes induces by the evolving location of the balloon. The partial change in temperature due to a change in altitude is \\(\\partial_z T\\), but it’s important to realize that the coordinates of the location are themselves functions of time: \\(x(t), y(t), z(t)\\). Seeing the function \\(T()\\) for the observer in the balloon as a function of \\(t\\), we have \\(T(t, x(t), y(t), z(t))\\). This is a function composition: \\(T()\\) composed with each of \\(x()\\), \\(y()\\), and \\(z()\\). Recall in the chain rule \\(\\partial_v f(g(v)) = \\partial_v f(g(v)) \\partial_v g(v)\\) that the derivative of the composed quantity is the product of two derivatives. Likewise, the total derivative of temperature with respect to the observer riding in the balloon will be add together the parts due to changes in time (holding position constant), x-coordinate (holding time and the other space coordinates constant), and the like. Signifying the total differentiation with a capital \\(D\\), we have \\[D\\, T(t) = \\partial_t T() + \\partial_x T() \\cdot\\partial_t x + \\partial_y T()\\cdot \\partial_t y + \\partial_z T() \\cdot\\partial_t z\\] Note that \\(\\partial_t x\\) is the velocity of the balloon in the x-direction, and similarly for the other coordinate directions. Writing these velocities as \\(v_x, v_y, v_z\\), the total derivative for temperature of a parcel of air embedded in a moving atmosphere is \\[D\\ T(t) = \\partial_t T + v_x\\, \\partial_x T + v_y\\, \\partial_y T + v_z\\, \\partial_z T\\] Formulations like this, which put the parts of change together into a whole, are often seen in the mathematics of fluid flow as applied in meteorology and oceanology. 24.5 Differentials A little bit of this, a little bit of that. — Stevie Wonder, “The Game of Love” We have framed calculus in terms of functions: transformations that take one (or more!) quantities as input and return a quantity as output. This was not the original formulation. In this section, we will use the original style in order to demonstrate how you can sometimes skip the step of constructing a function before differentiating to answer a question of the sort: “If this quantity changes by a little bit, how much will another, related quantity change?” As an example, consider the textbook-style problem of a water skier being pulled along the water by a rope pulled in from the top of a tower of height \\(H\\). The skier is distance \\(x\\) from the tower. As the rope is winched in at a constant rate, does the skier go faster or slower as she approaches the tower. In the function style of approach, we can write the position function \\(x(t)\\) with input the length of the rope \\(L(t)\\). Using the diagram, you can see that \\[x(t) = \\sqrt{\\strut L(t)^2 - H^2}\\ .\\] Differentiate both sides with respect to \\(t\\) to get the velocity of the skier: \\(\\partial_t x(t)\\) through the chain rule: \\[\\underbrace{\\partial_t x(t)}_{\\partial_t f(g(t))} = \\underbrace{\\frac{1}{2\\sqrt{\\strut L(t)^2 - H^2}}}_{\\left[ \\partial_t f \\right](g(t)) } \\times \\underbrace{\\left[2 \\partial_t L(t)\\right]}_{\\partial_t g(t)} = \\frac{\\partial_t L(t)}{\\strut\\sqrt{L(t)^2 - H^2}}\\] Now to reformulate the problem without defining a function. Newton referred to “flowing quantities” or “fluents” and to what today is universally called derivatives as “fluxions.” Newton did not have a notion of inputs and output.16 At about the same time as Newton’s inventions, very similar ideas were being given very different names by mathematicians on the European continent. There, an infinitely small change in a quantity was called a “differential” and the differential of \\(x\\) was denoted \\(dx\\). The first calculus textbook was subtitled, Of the Calculus of Differentials, in other words, how to calculate differentials. (See Figure 24.9.) Section I of this 1696 text is entitled, “Where we give the rules of this calculation,” those rules being recognizably the same as presented in Chapter 22 of this book. Figure 24.9: From the start of the first calculus textbook, by le marquis de l’Hôpital, 1696. Definition I of Section I states, \"We call quantities variable* that grow or decrease continuously; and to the contrary constant quantities are those that remain the same while the others change. … The infinitely small amount by which a continuous quantity increases or decreases is called the differential.*\" The differential is not a derivative. The differential is an infinitely small change in a quantity and a derivative is a rate of change. The differential of a quantity \\(x\\) is written \\(dx\\) in the textbook.17 The point of Section I of de l’Hôpital’s textbook is to present the rules by which the differentials of complex quantities can be calculated. You’ll recognize the product rule in de l’Hôpital’s notation: Figure 24.10: The differential of \\(x\\,y\\) is \\(y\\,dx + x\\,dy\\) The Pythagorean theorem relates the various quantities this way: \\[L^2 = x^2 + H^2\\] The differential of each side of the equation refers to “a little bit” of increase in the quantity on that side of the equation: \\[d(L^2) = d(x^2)\\ \\ \\ \\implies\\ \\ \\ 2 L\\, dL = 2 x\\, dx\\] where we’ve used one of the “rules” for calculating differentials. This gives us \\[dx = \\frac{L}{x} dL\\] Think of this as a recipe for calculating \\(dx\\). If you tell me \\(L\\), \\(x\\), and \\(dL\\) then you can calculate the value of \\(dx\\). For instance, suppose the tower is 52 feet tall and that there is \\(L=173\\) feet of tow-rope extending to the skier. The Pythagorean theorem tells us the skier is \\(x=165\\) feet from the base of the tower. The rope is, let us suppose, being pulled in at the top of the tower at \\(dL = 10\\) feet per second. How fast is \\(x\\) changing? \\[dx = \\frac{173\\ \\text{ft}}{165\\ \\text{ft}} \\times 10 \\text{ft s}^{-2} = 10.05\\ \\text{ft s}^{-1}\\] We’ll return to “a little bit of this” when we explore how to add up little bits to get the whole in Chapter ??. 24.6 Exercises Exercise 24.01: xdkw It’s relatively easy to assess partial derivatives when you know the gradient. After all, the gradient is the vector of \\((\\partial_x\\,f(x,y), \\partial_y f(x,y))\\). To train your eye, here’s a contour plot and a corresponding gradient plot. Question A What is the rule for determining \\(\\partial_x f(x,y)\\) from the direction of the gradient vector? If the vector has a component pointing right, \\(\\partial_x f\\) is positive.Right!  If the vector has a component pointing left, \\(\\partial_x f\\) is positive︎✘ If the gradient points left, then uphill is to the left. So the function is increasing from right to left. That’s a negative partial derivative. If the vector has a vertical component pointing up, \\(\\partial_x f\\) is positive.︎✘ This would be true for the partial derivative with respect to \\(y\\), but that has to relevance to the partial with respect to \\(x\\). If the vector has a component pointing downward, the partial derivative \\(\\partial_x f\\) is positive.︎✘ No, but the partial with respect to \\(y\\) would be negative. Question B What is the rule for determining \\(\\partial_y f(x,y)\\) from the direction of the gradient vector? If the vector has a component pointing right, \\(\\partial_y f\\) is positive.︎✘ Left and right are about \\(\\partial_x f\\), not \\(\\partial_y f\\). If the vector has a component pointing left, \\(\\partial_y f\\) is positive.︎✘ Left and right are about \\(\\partial_x f\\), not \\(\\partial_y f\\). If the vector has a vertical component pointing up, \\(\\partial_y f\\) is positive.Right!  If the vector has a component pointing downward, the partial derivative \\(\\partial_y f\\) is positive.︎✘ The partial with respect to \\(y\\) would be negative. Exercise 24.02: 9iowVB Open a sandbox and use the following commands to make a contour plot of the function \\(g(x)\\) centered on the reference point \\((x_0\\!=\\!0,\\, y_0\\!=\\!0)\\). g &lt;- rfun( ~ x + y, seed = 802, n = 15) x0 &lt;- 0 y0 &lt;- 0 size &lt;- 5 contour_plot(g(x, y) ~ x + y, domain(x = x0 + size*c(-1, 1), y = y0 + size*c(-1, 1))) By making size smaller, you can zoom in around the reference point. Zoom in gradually (say, size = 1.0, 0.5, 0.1, 0.05, 0.01) until you reach a point where the surface plot is (practically) a pretty simple inclined plane. From the contour plot, zoomed in so that the graph shows an inclined plane, figure out the sign of \\(\\partial_x g(0,0)\\) and \\(\\partial_y g(0,0)\\). Question A Which answer best describes the signs of the partial derivatives of \\(g(x,y)\\) at the reference point \\((x_0=0, y_0=0)\\)? \\(\\partial_x g(0,0)\\) is pos, \\(\\partial_y g(0,0)\\) is pos ︎✘ \\(\\partial_x g(0,0)\\) is pos, \\(\\partial_y g(0,0)\\) is neg ︎✘ \\(\\partial_x g(0,0)\\) is neg, \\(\\partial_y g(0,0)\\) is neg ︎✘ \\(\\partial_x g(0,0)\\) is neg, \\(\\partial_y g(0,0)\\) is pos.Good.  \\(\\partial_x g(0,0)\\) is 0, \\(\\partial_y g(0,0)\\) is pos︎✘ Exercise 24.04: qdkw Consider this close up of a function around a reference point at the center of the graph. By eye, estimate these derivatives of the function at the reference point \\((x_0=-2, y_0=-5)\\). Question A What is the numerical value of \\(\\partial_x g(x,y)\\) at the reference point?     -1︎✘        -0.50︎✘        -0.25︎✘        0︎✘        0.25︎✘        0.50\\(\\heartsuit\\ \\)       1︎✘ Question B What is the numerical value of \\(\\partial_y g(x,y)\\) at the reference point?     -1︎✘        -0.50︎✘        -0.25︎✘        0︎✘        0.25︎✘        0.50︎✘        1\\(\\heartsuit\\ \\) The next questions ask about second-order partial derivatives. As you know, the second derivative is about how the first derivative changes with x or y. Insofar as the function is a simple inclined plane, where the contours would be straight, parallel, and evenly spaced, the second derivatives would all be zero. But you can see that it is not such a plane: the contours curve a bit. In determining the second derivatives by eye from the graph, you are encouraged to compare first derivatives at the opposing edges of the graph, as opposed to at very nearby points. Question C What is the sign of \\(\\partial_{xx} g(x,y)\\) at the reference point?     negative\\(\\heartsuit\\ \\)       positive︎✘ Question D What is the sign of \\(\\partial_{yy} g(x,y)\\) at the reference point?     negative︎✘        positive\\(\\heartsuit\\ \\) Question E What is the sign of \\(\\partial_{xy} g(x,y)\\) at the reference point?     negative︎✘        positive\\(\\heartsuit\\ \\) Question F What is the sign of \\(\\partial_{yx} g(x,y)\\) at the reference point?     negative︎✘        positive\\(\\heartsuit\\ \\) Exercise 24.06: uekses At numerous occasions in your professional life, you will be in one or both of these positions: You are a decision-maker being presented with the results of analysis conducted by a team of unknown reliability, and you need to figure out whether what they are telling you is credible. You are a member of the analysis team needing to demonstrate to the decision-maker that your work should be believed. As an example, consider one of the functions presented in a comedy book, Geek Logic: 50 Foolproof Equations for Everyday Life (2006), by Garth Sundem. The particular function we’ll consider here is Dr(), intended to help answer the question, “Should you go to the doctor?” \\[\\text{Dr}(d, c, p, e, n, s) = \\frac{\\frac{s^2}{2} + e(n-e)}{100 - 3(d + \\frac{p^3}{70} - c)}\\] where \\(d\\) = How many days in the past month have you been incapacitated? \\(d_0 \\equiv 3\\) \\(c\\) = Does the issue seem to be getting better or worse. (-10 to 10 with -10 being “circling the drain” and 10 being “dramatic improvement”) \\(c_0 \\equiv -2\\) \\(p\\) = How much pain or discomfort are you currently experiencing? (1-10 with 10 being “currently holding detached toe in Ziploc bag”) \\(p_0 = 3\\) \\(e\\) = How embarrassing is this issue? (1-10 with 10 being “slipped on ice and fell on 1972 Mercedes-Benz hood ornament, which is now part of my body”) \\(e_0 = 4\\) \\(n\\) = How noticeable is the issue? (1-10 with 10 being “fell asleep on waffle iron”) \\(n_0 = 5\\) \\(s\\) = How serious does the issue seem? (1-10 with 10 being “may well have nail embedded in frontal lobe [of brain]”) \\(s_0 = 3\\) Although the function is offered tongue-in-cheek, let’s examine it to see if it even roughly matches common sense. The tool we will use relates to low-order polynomial approximation around a reference point and examining appropriate partial derivatives. To save time, we stipulate a reference point for you, noted in the description of variables above. The code creates an R implementation of the function that is set up so that the default values of the variables are those at the given reference point. You can use this in a sandbox to try different changes in each of the input quantities. Dr &lt;- makeFun( ((s^2)/2 + e*(n-e)) / (100 - 3*(d + (p^3/70) - c)) ~ d+p+e+c+n+s, s=3, n=5, e=4, p=3, d=3, c=-2) Dr() According to the instructions in the book, if Dr()\\(&gt; 1\\), you should go to the doctor. Essay 1: The value of Dr() at the reference point is 0.10, indicating that you shouldn’t go to the doctor. But we don’t yet know whether 0.10 is very close to the decision threshold of 1 or very far away. Describe a reasonable way to figure this out. Report your description and the results here. Essay 2: There are six inputs to the function. Go through the list of all six and (without thinking too hard about it) write down for all of them your intuitive sense of whether an increase of one point in that input should raise or lower the output of Dr() at the reference point. Also write down whether you think the input should be a large or small determinant of whether to go to the doctor. (You don’t need to refer to the Dr() function itself, just to your own intuitive sense of what should be the effect of each of the inputs.) The operator D() can calculate partial derivatives. You can calculate the value of a partial derivative very easily at the reference point, using an expression like this, which gives the value of the partial of Dr() with respect to input \\(s\\) at the reference point: D(Dr(s = s) ~ s)(s=3) We’re now going to use these partial derivatives to compare your intuition about going to the doctor to what the function has to say. Of course, we don’t know yet whether the function is reasonable, so don’t be disappointed if your intuition conflicts with the function. Essay 3: Calculate the numerical value of each of the partial derivatives at the reference point. List them here and say, for each one, whether it accords with your intuition. Exercise 24.07: RbuhMy Each of the figures below shows a contour plot and a gradient field. For some of the figures, the contour plot and the gradient field show the same function, for others they do not. Your task is to identify whether the contour plot and the gradient field are of the same or different functions. Question A For Figure A, do the contour plot and the gradient field show the same function?     Yes\\(\\heartsuit\\ \\)       No︎✘ Question B For Figure B, do the contour plot and the gradient field show the same function?     Yes\\(\\heartsuit\\ \\)       No︎✘ Question C For Figure C, do the contour plot and the gradient field show the same function?     Yes︎✘        No\\(\\heartsuit\\ \\) Question D For Figure D, do the contour plot and the gradient field show the same function?     Yes\\(\\heartsuit\\ \\)       No︎✘ Question E For Figure E, do the contour plot and the gradient field show the same function?     Yes︎✘        No\\(\\heartsuit\\ \\) Question F For Figure F, do the contour plot and the gradient field show the same function?     Yes\\(\\heartsuit\\ \\)       No︎✘ Exercise 24.08: lkwciw Here is a contour plot of a function \\(g(x,y)\\). You will be presented with several gradient fields. Your task is to determine whether the gradient field corresponds to the contour plot and, if not, say why not. ## Warning: Removed 19 rows containing missing values (geom_segment). Question A What’s wrong with gradient field 1? arrows point down the hill instead of up it︎✘ magnitude of arrows are wrong, but direction is rightRight!  arrows don’t point in the right direction︎✘ nothing is wrong︎✘ ## Warning: Removed 18 rows containing missing values (geom_segment). Question B What’s wrong with gradient field 2? arrows point down the hill instead of up itExcellent!  magnitude of arrows are wrong, but direction is right︎✘ arrows don’t point in the right direction︎✘ nothing is wrong︎✘ ## Warning: Removed 19 rows containing missing values (geom_segment). Question C What’s wrong with gradient field 3? arrows point down the hill instead of up it︎✘ magnitude of arrows are wrong, but direction is right︎✘ arrows don’t point in the right direction︎✘ nothing is wrongCorrect.  ## Warning: Removed 19 rows containing missing values (geom_segment). Question D What’s wrong with gradient field 4? arrows point down the hill instead of up it︎✘ magnitude of arrows are wrong, but direction is right︎✘ arrows don’t point in the right directionGood.  nothing is wrong︎✘ Exercise 24.10: wkd83 Here are contour maps and gradient fields of several functions with input \\(x\\) and \\(y\\). But any row of graphs may show two different functions. Your job is to match the contour plot with the gradient field, which may be in another row. Question A Which contour plot matches gradient field 1?     A︎✘        B︎✘        C︎✘        D︎✘        E︎✘        F\\(\\heartsuit\\ \\) Question B Which contour plot matches gradient field 2?     A\\(\\heartsuit\\ \\)       B︎✘        C︎✘        D︎✘        E︎✘        F︎✘ Question C Which contour plot matches gradient field 3?     A︎✘        B︎✘        C︎✘        D\\(\\heartsuit\\ \\)       E︎✘        F︎✘ Question D Which contour plot matches gradient field 4?     A︎✘        B︎✘        C\\(\\heartsuit\\ \\)       D︎✘        E︎✘        F︎✘ Question E Which contour plot matches gradient field 5?     A︎✘        B︎✘        C︎✘        D︎✘        E\\(\\heartsuit\\ \\)       F︎✘ Question F Which contour plot matches gradient field 6?     A︎✘        B\\(\\heartsuit\\ \\)       C︎✘        D︎✘        E︎✘        F︎✘ Exercise 24.12: tPsfoR Using the gradient field depicted below, figure out the sign of the partial derivatives at the labeled points. We’ll use “neg” to refer to negative partial derivatives, “pos” to refer to positive partial derivatives, and “zero” to refer to partials that are so small that you can’t visually distinguish them from zero. Question A Which is \\(\\partial_y f\\) at point A?     neg\\(\\heartsuit\\ \\)       zero︎✘        pos︎✘ Question B Which is \\(\\partial_x f\\) at point A?     neg︎✘        zero︎✘        pos\\(\\heartsuit\\ \\) Question C Which is \\(\\partial_x f\\) at point B?     neg︎✘        zero︎✘        pos\\(\\heartsuit\\ \\) Question D Which is \\(\\partial_x f\\) at point C?     neg︎✘        zero\\(\\heartsuit\\ \\)       pos︎✘ Question E Which is \\(\\partial_y f\\) at point E?     neg︎✘        zero︎✘        pos\\(\\heartsuit\\ \\) Question F Which is \\(\\partial_x f\\) at point E?     neg\\(\\heartsuit\\ \\)       zero︎✘        pos︎✘ Question G At which letter are both the partial with respect to \\(x\\) and the partial with respect to \\(y\\) negative.?     A︎✘        B︎✘        C︎✘        D︎✘        E︎✘        F︎✘        none of them\\(\\heartsuit\\ \\) Exercise 24.14: vkdlw For almost everyone, a house is too expensive to buy with cash, so people need to borrow money. The usual form of the loan is called a “mortgage”. Mortgages extend over many years and involve paying a fixed amount each month. That amount is calculated so that, by paying it each month for the duration of the mortgage, the last payment will completely repay the amount borrowed plus the accumulated interest. The monthly mortgage payment in dollars, \\(P\\), for a house is a function of three variables, \\[P(A, r, N)\\] where \\(A\\) is the amount borrowed in dollars, \\(r\\) is the interest rate (percentage points per year), and \\(N\\) is the number of years before the mortgage is paid off. A studio apartment is selling for $220,000. You will need to borrow $184,000 to make the purchase. Question A Suppose \\(P(184000,4,10) = 2180.16\\). What does this tell you in financial terms? The monthly cost of borrowing $184,000 for 10 years at 4% interest per year.Excellent!  The monthly cost of borrowing $184,000 for 4 years at 10% interest per year.︎✘ You’ve got the order of the argument wrong. The annual cost of the mortgage at 4% interest for 10 years.︎✘ The output of the function \\(P()\\) is the monthly mortgage payment. The annual cost of the mortgage at 10% interest for 4 years︎✘ The output of the function \\(P()\\) is the monthly mortgage payment. The next two questions involve what happens to the monthly mortgage payments if you change either the amount or duration of the mortgage. (Hint: Common sense works wonders!) Question B What would you expect about the quantity \\(\\partial P / \\partial A\\), the partial derivative of the monthly mortgage payment with respect to the amount of money borrowed? It’s positiveNice! If you borrow more money, holding mortgage duration and interest rate constant, you are going to have to pay more each month. It’s zero︎✘ It’s negative︎✘ Question C What would you expect about the quantity \\(\\partial P / \\partial N\\), the partial derivative of the monthly mortgage payment with respect to the number of years the mortgage lasts? It’s positive︎✘ It’s zero︎✘ It’s negativeNice! If you borrow the same amount of money at the same interest rate, but have more years to pay it back, your monthly payment will be smaller. Question D Suppose \\(\\partial_r P (184000,4,30) =\\) $145.65. What is the financial significance of the number $145.65?? If the interest rate \\(r\\) went up from 4 to 5, the monthly payment would increase by $145.65.︎✘ This is a perfectly reasonable answer, but … recall that the derivative gives the rate of increase in the output of the function when the input changes by a tiny amount. It turns out that a 1 percentage point increase in interest rate is a very large amount of change. If the interest rate \\(r\\) went up from 4 to 4.001, the monthly payment would increase by $145.65.︎✘ This is indeed a small change in interest rate, but the value of the derivative is the rate of increase, not the increase itself. If the interest rate \\(r\\) went up from 4 to 4.001, the monthly payment would increase by $0.001 imes $145.65.Nice! You might think that nobody would be concerned about such a small increase in interest rate. But knowing the result for each very small increase allows us to calculate what would be the impact of a large increase by a process called integration. Exercise 24.16: 72ldw In economic theory, the quantity of the demand for any good is a decreasing function of the price of that good and an increasing function of the price of a competing good. The classical example is that apple juice competes with orange juice. The demand for orange juice is in units of thousands of liters of orange juice. The price is in units of dollars per liter. Here’s a graph with the input variables unlabeled. The contour labels indicate the demand for orange juice. The concept of partial derivatives makes it much easier to think about the situation. There are two partial derivative functions relevant to the function in the graph. Well denote the input variables apple and orange, but remember that these are the prices of those commodities in dollars per liter. \\(\\partial_\\text{apple} \\text{demand}()\\) – how the demand changes when apple-juice price goes up, holding orange-juice price constant. (Another notation that is more verbose but perhaps easier to read \\(\\frac{\\partial\\, \\text{demand}}{\\partial\\,\\text{apple}}\\)) \\(\\partial_\\text{orange} \\text{demand}()\\) – how the demand changes when orange-juice price goes up, holding apple-juice price constant. (Another notation: \\(\\frac{\\partial\\, \\text{demand}}{\\partial\\,\\text{orange}}\\)) Notice that the notation names both the output and the single input which is to be changed–the other inputs will be held constant. The first paragraph of this problem gives the economic theory which amounts to saying that one of the partial derivatives is positive and the other negative. Question A What is the proper translation of the notation \\(\\partial_\\text{apple}\\text{demand}()\\)? The partial derivative of orange-juice demand with repect to apple-juice priceGood.  The partial derivative of apple-juice price with respect to demand for orange juice︎✘ Derivatives are always with respect to an input variable. Demand for orange juice is the output variable. The partial derivative of apple-juice demand with respect to price of apple juice︎✘ This problem is about orange-juice demand, not apple-juice demand. The partial derivative of orange-juice price with respect to apple-juice price.︎✘ Both of these are input variables to the demand function. A partial derivative is always about the change in output when one of the inputs changes, not the change in one input when another input changes. Question B According to the economic theory described above, one of the partial derivatives will be positive and the other negative. Which will be positive. \\(\\partial_\\text{apple} \\text{demand}()\\)Excellent!  \\(\\partial_\\text{orange} \\text{demand}()\\)︎✘ If orange juice prices went up, would you drink more orange juice? Question C What does the vertical axis measure? Price of orange juiceNice!  Quantity of apple juice︎✘ Quantity of orange juice︎✘ Price of apple juice︎✘ Question D Consider the magnitude (absolute value) of the partial derivative of demand with respect to orange-juice price. Is this magnitude greater toward the top of the graph or the bottom? topCorrect.  bottom︎✘ The horizontal distance between contour lines is larger at the bottom of the graph than at the top. Far-apart contours mean that the function is flatter. neither︎✘ Remember, we’re talking about the partial derivative with respect to orange juice price. That corresponds to the slope when moving in a horizontal direction. Exercise 24.18: oekse The contour plot of function \\(g(y, z)\\) is overlaid with vectors. The black vector is a correct representation of the gradient (at the root of the vector). The other vectors are also supposed to represent the gradient, but might have something wrong with them (or might not). You’re job is to say what’s wrong with each of those vectors. Question A What’s wrong with the red vector? nothing︎✘ too longCorrect. The red vector is located in a place where the function is almost level. You can tell this because the contour lines are spaced far apart. The magnitude of the gradient will be small in such an area. But here, the red vector is even longer than the black vector, even though black is in a very steep area (with closely spaced contours). too short︎✘ points downhill︎✘ points uphill︎✘ Pointing uphill is what gradient vectors do! No problem with this. wrong direction entirely︎✘ Question B What’s wrong with the green vector? nothing︎✘ too long︎✘ Note that the vector reflects the steepness at the point where the root of the vector is drawn. The root of the green vector is in an area where the contour lines are spaced similarly to the area near the root of the black vector. So it’s correct that the length of the green vector is similar to the length of the black vector. too short︎✘ points downhillRight! The root of the green vector is near contour=4, the head at contour=2. So the vector is incorrectly pointing downhill. Gradients point in the steepest direction uphill. points uphill︎✘ Pointing uphill is what gradient vectors do! No problem with this. wrong direction entirely︎✘ Question C What’s wrong with the blue vector? nothingNice!  too long︎✘ too short︎✘ The blue vector is in a very flat area of the function. That’s why it’s so short. points downhill︎✘ points uphill︎✘ Pointing uphill is what gradient vectors do! No problem with this. wrong direction entirely︎✘ Question D What’s wrong with the orange vector? nothing︎✘ too long︎✘ too short︎✘ points downhill︎✘ points uphill︎✘ wrong direction entirelyRight! Gradient vectors should be perpendicular to nearby contours and should point uphill. The orange vector is neither Question E What’s wrong with the gray vector? nothing︎✘ too long︎✘ too shortNice! The function is practically as steep at the root of the gray vector as it is at the root of the black vector. (You can tell this from the spacing of the contour lines.) So the magnitude of the gray vector should be just about the same as the magnitude of the black vector. points downhill︎✘ points uphill︎✘ Exercise 24.20: ZQlUv4 Let’s return to the water skier in Section 24.5. When we left her, the rope was being pulled in at 10 feet per second and her corresponding speed on the water was 10.05 feet per second. The relationship between the rope speed and the skier’s speed was \\[dx = \\frac{L}{x} dL\\] and, due to the right-angle configuration of the tow system, \\(x^2 = L^2 - H^2\\). What happens to \\(dx\\) as \\(L\\) gets smaller with \\(dL\\) being the same? We need to keep in mind that \\(dx\\) depends on three things: \\(dL\\), \\(L\\), and \\(x\\). But we can substitute in the Pythagorean relationship between \\(x\\), \\(L\\), and (fixed) \\(H\\) to get \\[dx = \\frac{L}{\\strut\\sqrt{L^2 - H^2}}\\ dL\\ .\\] This is bad news for the skier who holds on too long! As \\(L\\) approaches \\(H\\), the rope becomes more and more vertical and the skier’s water speed becomes greater and greater, approaching \\(\\infty\\) as \\(L \\rightarrow H\\). You, an engineer brought in to solve this dangerous possibility, have proposed to have the winch slow down as the rope is reeled in. How should the speed of the rope be set so that the skier’s water speed remains safely constant? Question A What formula for \\(dL\\) will allow \\(dx\\) to stay constant at a value \\(v\\)? \\(dL = dx\\)︎✘ This is possible only for one value of \\(L\\), when \\(\\sqrt{L^2 - H^2} = L\\). So this isn’t a way to keep \\(dx\\) constant. \\(dL = v \\sqrt{L^2 - H^2}{L}\\)Good. The roop speed will get slower and slower, coming to zero when \\(L = H\\) \\(dL = v \\sqrt{L^2 - H^2}\\)︎✘ If \\(dL\\) were set according to this rule, \\(dx = L\\), which isn’t going to be constant as \\(L\\) gets smaller. There is no such formula.︎✘ In describing the orientation of aircraft and ships, three parameters are used: pitch, roll, and yaw. For a geometrical plane (as opposed to an aircraft or ship, which have distinct front and back ends), yaw isn’t applicable.↩︎ The Latin phrase for this is ceteris paribus, often used in economics.↩︎ The meaning of “output” as “to produce” dates from more than 100 years after Newton’s death.↩︎ A “warning” is given in the textbook that the symbol \\(d\\) will always be used to mark the differential of a variable quantity and that \\(d\\) will never be used to indicate a parameter.↩︎ "],["local-approximations.html", "Chapter 25 Local approximations 25.1 Eight simple shapes 25.2 Low-order polynomials 25.3 The low-order polynomial with two inputs 25.4 Finding coefficients from data 25.5 Exercises", " Chapter 25 Local approximations We have focused in this book on a small set of basic modeling functions and three operations for assembling new functions out of old ones: linear combination, multiplication, and composition. All of these have a domain that is the whole number line, or the positive half of the number line, or perhaps the whole number line leaving out zero or some other isolated point. Consider such domains to be global. 2750 We also discussed the components of piecewise functions. Each component is a function defined on a limited domain, an interval \\(a \\leq x \\leq b\\). In contrast to the global domains, we’ll call the limited domains local. 2755 In this chapter, we’ll explore a simple and surprisingly powerful method to approximate any function locally, that is, over a small domain. Why would you want to approximate a function? Why not just use the function itself? It’s often the case that we know about or hypothesize about relationships only from data. We believe there is a definite functional form for the relationship, but it’s unknown and unknowable to us. Still, we can approximate even an unknown function, matching the approximation to the data that is the visible manifestation of the unknown function. Local approximations provide a general-purpose method for creating functions that can represent a wide range of relationship patterns, even ones that are not otherwise known to us. In fields such as physics or engineering, there are often theories that dictate a particular form of function. For example, Newton’s universal law of gravitation posits an inverse square law for the force of gravity as a function of distance. Mechanical engineers use power laws to describe the shape of a beam under load, and communications engineers (and others) make extensive use of sinusoids. Textbooks in those fields rightfully emphasize those particular function forms. The utility of the local approximation method is that you can move forward even in the absence of a detailed theory. You need only apply your insight to posit which quantities are related to each other and then apply the approximation methods to produce a functional form. This approach is ubiquitous in all fields. Sometimes, the local approximation becomes the theory. This is seen, for instance, in Newton’s law of cooling, in Hooke’s law relating force and extension, or the chemist’s law of mass action. 2760 The information that you have about the relationship often takes the form of a data table. Each row records one trial in which the values of the inputs have been measured and the corresponding output value recorded. We’ll discuss the methods of constructing functions to match such data in Block 5 of this course. 2765 Another common form for the information about the relationship is about derivatives. That is, you know something about the derivative of a relationship even though you don’t (yet) have a form for the function describing the relationship. As an example, think about building a model of the sustainable speed of a bicycle as a function of the gear selected and the grade of the road—up or down. 2770 Consider these three questions that any experienced bicyclist can likely answer: On a given grade of road, is there an optimal gear for the highest sustained speed? (Have in mind a particular rider, perhaps yourself.) Imagine that the grade of the road is described by a positive number for uphill and a negative number for downhill: that is, the slope of the road. For a positive (uphill) grade and at a fixed gear, will the bike’s sustained speed be higher or lower as a function of the grade?18 Assuming you answered “yes” to question (1): Does the optimal gear choice depend on the grade of the road? (In concrete terms, would you choose different gears for an uphill climb than for a level road or a downhill stretch?) 2775 Using the methods in this chapter, the answers to those three questions let you choose an appropriate form for the speed(gear, grade) function. Then, using methods in Block 5 of this text, you can make a few measurements for any given rider and construct a model customized to that rider. 2780 Note that the three questions all have to do with derivatives. An “optimal gear” is a gear at which \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade}) = 0\\). That you ride slower the higher the numerical value of the slope means that \\(\\partial_\\text{grade} \\text{speed}(\\text{gear}, \\text{grade}) &lt; 0\\). And we know that \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade})\\) depends on the grade; that’s why there’s a different optimal gear at each grade. 2785 25.1 Eight simple shapes In many modeling situations with a single input, you can get very close to a good modeling function \\(f(x)\\) by selecting one of eight simple shapes, shown in Figure 25.1. 2790 Figure 25.1: The eight simple shapes, locally, of functions with one input. (See Chapter 25.) To choose among these shapes, consider your modeling context: 2795 is the relationship positive (slopes up) or negative (slopes down) is the relationship monotonic or not is the relationship concave up, concave down, or neither Some examples, scenarios where the modeler knows about the derivative and concavity of the relationship being modeled. It’s often the case that your knowledge of the system comes in this form. 2800 The incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent. 2805 How many minutes can you run as a function of speed? Concave down and shallow-then-steep; you wear out faster if you run at high speed. How far can you walk as a function of time? Steep-then-shallow and concave down; your pace slows as you get tired. How does the stew taste as a function of saltiness. The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable. The temperature of cooling water or the emission of radioactivity as functions of time are concave up and steep-then-shallow. 2810 How much fuel is consumed by an aircraft as a function of distance? For long flights the function is concave up and shallow-then-steep; fuel use increases with distance, but the amount of fuel you have to carry also increases with distance and heavy aircraft use more fuel per mile. In micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price. 2815 As a rule, production increases with price and demand decreases with price. In the short term, production functions tend to be concave down, since it’s hard to squeeze increased production out of existing facilities. For demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. An example is the consumption of gasoline versus price: it’s hard in the short term to find another way to get to work. In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices for gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply. 2820 2825 25.2 Low-order polynomials There is a simple, familiar functional form that, by selecting parameters appropriately, can take on each of the eight simple shapes: the second-order polynomial. \\[g(x) \\equiv a + b x + c x^2\\] As you know, the graph of \\(g(x)\\) is a parabola. The parabola opens upward if \\(0 &lt; c\\). That’s the shape of a local minimum. The parabola opens downward if \\(c &lt; 0\\). That’s the shape of a local maximum Consider what happens if \\(c = 0\\). The function becomes simply \\(a + bx\\), the straight-line function. When \\(0 &lt; b\\) the line slopes upward. When \\(b &lt; 0\\) the line slopes downward. With the appropriate choice of parameters, the form \\(a + bx + cx^2\\) is capable of representing four of the eight simple shapes. What about the remaining four? This is where the idea of local becomes important. Those remaining four shapes are the sides of parabolas, as in Figure ??. 2830 ## Warning in validate_domain(domain, free_args): Missing domain names: x Figure 25.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 25.1. ## Warning in validate_domain(domain, free_args): Missing domain names: x Figure 25.3: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 25.1. 25.3 The low-order polynomial with two inputs For functions with two inputs, the low-order polynomial approximation looks like this: \\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\] In reading this form, note the system being used to name the polynomial’s coefficients. First, we’ve used \\(a\\) as the root name of all the coefficients. Sometimes we might want to compare two or more low-order polynomials, so it’s convenient to be able to use \\(a\\) for one, \\(b\\) for another, and so on. 2835 The subscripts on the coefficients describes exactly which term in the polynomial involves each coefficient. For instance, the \\(a_{yy}\\) coefficient applies to the \\(y^2\\) term, while \\(a_x\\) applies to the \\(x\\) term. 2840 Each of \\(a_0, a_x,\\) \\(a_y,\\) \\(a_{xy}, a_{yy}\\), and \\(a_{xx}\\) will, in the final model, be a constant quantity. Don’t be confused by the use of \\(x\\) or \\(y\\) in the name of the coefficients. Each coefficient is a constant and not a function of the inputs. Often, your prior knowledge of the system being modeled will tell you something about one or more of the coefficients, for example, whether it is positive or negative. Finding a precise value is often based on quantitative data about the system. 2845 It helps to have different names for the various terms. It’s not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronounciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term. 2850 \\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ + \\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\ \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\] ## Warning in validate_domain(domain, free_args): Using -5 to 5 in domain for ## missing domain names. ## Warning in validate_domain(domain, free_args): Missing domain names: x, y Figure 25.4: A saddle Figure 25.4: A saddle If you’re like many people, you find it harder to walk uphill than down, and find it takes more out of you to walk longer distances than shorter. Let’s build a model of this, using nothing more than your intuition and the method of low-order polynomial approximations. Let’s call the map distance walked \\(d\\). (“Map distance” is the horizontal change in position, disregarding vertical changes.) The steepness of the hill will be the “grade” \\(g\\), which is measured as the horizontal distance covered divided by the vertical climb. If you’re going downhill, the grade is negative. The key ingredient in the model: We’ll measure the “difficulty” or “exertion” to walking as the energy consumed during the walk: \\(E(d, g)\\). Some assumptions about walking and energy consumed: If you don’t walk, you consume zero energy walking. The energy consumed should be proportional to the length of the walk. This is an assumption, and is probably valid, only for walks of short to medium distances, as opposed to forced marches over tens of miles. We’ll start with the full 2nd-order polynomial in two variables, and then seek to eliminate terms that aren’t needed. \\[E_{big}(d, g) \\equiv a_0 + a_d\\, d + a_g\\, g + a_{dg}\\, d\\, g + a_{dd}\\,d^2 + a_{gg}\\,g^2\\] According to assumption (1), when \\(E(d=0, g) = 0\\). Of course, if you are walking zero distance, it doesn’t matter what the grade is; the energy consumed is still zero. Consequently, we know that all terms that don’t include a \\(d\\) should go away. This leaves us with \\[E_{medium}(d, g) \\equiv a_d\\, d + a_{dg}\\, d\\, g + a_{dd}\\,d^2 = d \\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\] Assumption (2) says that energy consumed is proportional to \\(d\\). The multiplier on \\(d\\) in \\(E_{medium}()\\) is \\(\\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\) which is itself a function of \\(d\\). A proportional relationship implies a multiplier that doesn’t depend on the quantity itself. This means that \\(a_{dd} = 0\\). This leaves us with a very simple model: \\[E(d, g) \\equiv \\left[\\strut a_1 + a_2\\, g\\right]\\, d\\] where we have simplified the labeling on the coefficients since there are only two in the model. Perhaps assumption (2) is mis-placed and that the energy consumed per unit distance in a walk increases with the length of the walk. If so, we would need to return to the question of \\(a_{dd}\\). This is typical of the modeling cycle. Trying to be economical with model terms highlights the question of which terms are so small they can be ignored. Example 25.1 In selecting cadets for pilot training, two criteria are the cadet’s demonstrated flying aptitude and the leadership potential of the cadet. Let’s assume that the overall merit \\(M\\) of a candidate is a function of flying aptitude \\(F\\) and leadership potential \\(L\\). Currently, the merit score is a simple function of the \\(F\\) and \\(L\\) scores: \\[M_{current}(F, L) \\equiv F + L\\] The general in charge of the training program is not satisfied with the current merit function. “I’m getting too many cadets who are great leaders but poor pilots, and too many pilot hot-shots who are not good leaders. I would rather have an good pilot who is a good leader than have a great pilot who is a poor leader or a poor pilot who is a great leader.” (You might reasonably agree or disagree with this point of view, but the general is in charge.) The general has tasked you to revise the formula to better match her views about the balance betwen flying ability and leadership potential. How should you go about constructing \\(M_{improved}(F, L)\\)? You recognize that \\(F + L\\) is a low-order polynomial: just the linear terms are present without a constant or interaction term or quadratic terms. Low-order polynomials are a good way to approximate any formula locally, so you have decided to follow that route. Quadratic terms are appropriate when a model needs to feature a locally optimal level of the of the inputs. But it will never be the case that a lower flying score will be more favored than a higher score, and the same thing for the leadership score. So your model doesn’t need quadratic terms. That leaves the interaction term as the way forward. The low-order polynomial model will be \\[M_{improved}(F, L) \\equiv d_0 + F + L + d_{FL} FL\\] Should \\(d_{FL}\\) be positive or negative? Imagine a cadet Drew with acceptable and equal F and L scores. Another cadet, Blake, has scores that are \\(F+\\epsilon\\) and \\(L-\\epsilon\\), where \\(\\epsilon\\) might be positive or negative. Under the original formula for merit, Drew and Blake have equal merit. Under the new criteria, Drew should have a higher merit than Blake. In other words: \\[M_{improved}(F, L) - M_{improved}(F+\\epsilon, L-\\epsilon) &gt; 0\\] Replace \\(M_{improved}(F, L)\\) with the low-order polynomial approximation given earlier. \\[\\underbrace{d_0 + F + L + d_{FL} FL}_{M_{improved}(F, L)} - \\underbrace{\\left[{\\large\\strut} d_0 + \\left[ F + \\epsilon\\right] + \\left[ L - \\epsilon\\right] + d_{FL} (FL -\\epsilon L + \\epsilon F - \\epsilon^2)\\right]}_{M_{improved}(F+\\epsilon, L-\\epsilon)} &gt; 0\\] Collecting and cancelling terms in the above gives \\[- d_{FL}(\\epsilon(F-L) + \\epsilon^2) &gt; 0\\] Since \\(F\\) and \\(L\\) were assumed equal, this results in \\[M_{improved}(F, L) - M_{improved}(F+\\epsilon, L-\\epsilon) = d_{FL}\\, \\epsilon^2 &gt; 0\\] Thus, \\(d_{FL}\\) will have to be positive. 25.4 Finding coefficients from data Low-order polynomials are often used for constructing functions from data. In this section, I’ll demonstrate briefly how this can be done. The full theory will be introduced in Block 5 of this text. The data I’ll use for the demonstration is a set of physical measurements of height, weight, abdominal circumference, etc. on 252 human subjects. These are contained in the Body_fat data frame, shown below. One of the variables records the body-fat percentage, that is, the fraction of the body’s mass that is fat. This is thought to be an indicator of fitness and health, but it is extremely hard to measure and involves weighing the person when they are fully submerged in water. This difficulty motivates the development of a method to approximation body-fat percentage from other, easier to make measurements such as height, weight, and so on. For the purpose of this demonstration, we’ll build a local polynomial model of body-fat percentage as a function of height (in inches) and weight (in pounds). The polynomial we choose will omit the quadratic terms. It will contain the constant, linear, and interaction terms only. That is \\[\\text{body.fat}(h, w) \\equiv c_0 + c_h h + c_w w + c_{hw} h w\\] The process of finding the best coefficients in the polynomial is called linear regression. Without going into the details, we’ll use linear regression to build the body-fat model and then display the model function as a contour plot. mod &lt;- lm(bodyfat ~ height + weight + height*weight, data = math141Z::Body_fat) body_fat_fun &lt;- makeFun(mod) contour_plot(body_fat_fun(height, weight) ~ height + weight, domain(weight=c(100, 250), height = c(60, 80))) %&gt;% gf_labs(title = &quot;Body fat percentage&quot;) Figure 25.5: A low order polynomial model of body fat percentage as a function of height (inches) and weight (lbs). That we can build such a model doesn’t mean that it’s useful for anything. In Block 5 of the text we’ll return to the question of how well a model constructed from data represents the real-world relationships that the model attempts to describe. 25.5 Exercises Exercise 25.02: ckslw Consider the model presented in Section 25.3 about energy expenditure walking distance \\(d\\) on a grade \\(g\\): \\[E(d,g) = (a_0 + a_1 g)d\\] where \\(d\\) is the (horizontal equivalent) of the distance walked and \\(g\\) is the grade of the slope (that is, rise over run). We want \\(E\\) to be measured in Joules which has dimension M L\\(^2\\) T\\(^{-2}\\). Of course, the dimension of \\(d\\) is L, that is \\([d] = \\text{L}\\). Question A What is the dimension of the parameter \\(a_0\\)?     dimensionless︎✘        \\(L/T^2\\)︎✘        \\(T/L^2\\)︎✘        \\(M/T^2\\)︎✘        \\(M L/T^2\\)\\(\\heartsuit\\ \\)       \\(M/L^2\\)︎✘        \\(M/(L^2 T^2)\\)︎✘        \\(M L^2 / T^2\\)︎✘ Question B What is the dimension of \\(g\\)? (Hint: \\(g\\) is the ratio of vertical to horizontal distance covered.)     dimensionless\\(\\heartsuit\\ \\)       \\(L/T^2\\)︎✘        \\(T/L^2\\)︎✘        \\(M/T^2\\)︎✘        \\(M L/T^2\\)︎✘        \\(M/L^2\\)︎✘        \\(M/(L^2 T^2)\\)︎✘        \\(M L^2 / T^2\\)︎✘ Question C What is the dimension of the parameter \\(a_1\\)?     dimensionless︎✘        \\(L/T^2\\)︎✘        \\(T/L^2\\)︎✘        \\(M/T^2\\)︎✘        \\(M L/T^2\\)\\(\\heartsuit\\ \\)       \\(M/L^2\\)︎✘        \\(M/(L^2 T^2)\\)︎✘        \\(M L^2 / T^2\\)︎✘ Exercise 25.04: ikdlx Suppose we describe the spread of an infection in terms of three variables: \\(N\\) infection rate with respect to time: the number of new infections per day \\(I\\) the current number of people who are infectious, that is, currently capable of spreading the infection \\(S\\) the number of people who are susceptible, that is, currently capable of becoming infectious if exposed to the infection. All three of these variables are functions of time. News reports in 2020 routinely such as the one below gave the graph of \\(N\\) versus time for Covid-19. On November 15, 2020, \\(N\\) was 135,187 people per day. (This is the number of positive tests. The true value of \\(N\\) was, based on later information, 5-10 times greater.) The news reports don’t usually report \\(S\\) on a day-by-day basis. But a basic strategy in modeling with calculus is to take a snapshot: Given \\(I\\) and \\(S\\) today, what is a model of \\(N\\) for today. (Next semester, we’ll study “differential equations,” which provide a way of assembling from the snapshot model what the time course of the pandemic will look like.) The low-order polynomial for \\(N(S, I)\\) is \\[N(S,I) = a_0 + a_1 S + a_2 I + a_{12} I S.\\] We don’t include quadratic terms because there is no local maximum in \\(N(S, I)\\)—common sense suggests that \\(\\partial_S N() \\geq 0\\) and \\(\\partial_I N() \\geq 0\\), whereas a local maximum requires at least one of these derivatives to be negative near the max. Your job is to figure out which, if any, terms can be safely deleted from the low-order polynomial. A good way to approach this is to figure out, using common sense, what \\(N\\) would be for either \\(S=0\\) or \\(I=0\\). (Note that the previous is not restricted to \\(S = I = 0\\). Only one of them needs to be zero to produce the relevant result.) Question A We know that if \\(I=0\\) there will be no new infections, regardless of how large \\(S\\) is. We also know that if \\(S=0\\), there will be no new infections no matter how many people are currently infective. Which of these low-order polynomials correctly represents these two facts? (Assume that all the coefficients in the various polynomials are non-zero.) \\(N(S,I) = a_0 + a_1 S + a_2 I + a_{12} I S\\)︎✘ We don’t need all four terms. Think about it! \\(N(S,I) = a_0 + a_1 S + a_2 I\\)︎✘ When both \\(S\\) and \\(I\\) are zero, \\(N\\) would be non-zero. \\(N(S,I) = a_1 S + a_2 I + a_{12} I S\\)︎✘ \\(N(S,I) = a_2 I + a_{12} I S\\)︎✘ \\(N(S,I) = a_1 S + a_{12} I S\\)︎✘ \\(N(S,I) = a_{12} I S\\)Nice!  \\(N(S,I) = a_1 S + a_2 I\\)︎✘ It’s much the same for downhill biking, but you have to keep in mind that a shallow downhill has a higher numerical slope than a steep downhill. That is, the derivative of the hill is near zero for a very shallow grade and far from zero (that is, more negative) for a steep downhill grade.↩︎ "],["polynomials.html", "Chapter 26 Polynomials 26.1 Basics of polynomials with one input 26.2 Multiple inputs? 26.3 High-order approximations 26.4 Indeterminate forms 26.5 Computing with indeterminate forms 26.6 Exercises", " Chapter 26 Polynomials A big part of the high-school algebra curriculum is about polynomials. In some ways, this is appropriate since polynomials played an outsized part in the historical development of mathematical theory. Indeed, the so-called “Fundamental theorem of algebra” is about polynomials.19 For modelers, polynomials are a mixed bag. They are very widely used in modeling. Sometimes this is entirely appropriate, for instance the low-order polynomials that are the subject of Chapter 25. The problems come when high-order polynomials are selected for modeling purposes. Building a reliable model with high-order polynomials requires a deep knowledge of mathematics, and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials, but newcomers often draw on their experience in high-school and give unwarranted credence to polynomials. This chapter attempts to guide you to the ways you are likely to see polynomials in your future work and to help you avoid them when better alternatives are available. 26.1 Basics of polynomials with one input A polynomial is a linear combination of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, …. The individual functions are called monomials, a word that echoes the construction of chemical polymers out of monomers; for instance, the material polyester is constructed by chaining together a basic chemical unit called an ester. In one variable, say \\(x\\), the monomials are \\(x^1, x^2, x^3\\), and so on. (There’s also \\(x^0\\), but that’s better thought of as the constant function.) An n-th order polynomial has monomials up to exponent \\(n\\). For example, the form of a third-order polynomial is \\[a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3\\] The domain of polynomials, like the power-law functions they are assembled from, is the real numbers, that is, the entire number line \\(-\\infty &lt; x &lt; \\infty\\). But for the purposes of understanding the shape of high-order polynomials, it’s helpful to divide the domain into three parts: a wriggly domain at the center and two tail domains to the right and left of the center. Figure 26.1: A \\(n\\)th-order polynomial can have up to \\(n-1\\) critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly. Figure 26.1 shows a 7th order polynomial—that is, the highest-order term is \\(x^7\\). In one of the tail domains the function value heads off to \\(\\infty\\), in the other to \\(-\\infty\\). This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7, … In contrast, for even-order polynomials (2, 4, 6, …) the function value in the two tail domains go in the same direction, either both to \\(\\infty\\) (Hands up!) or both to \\(-\\infty\\). In the wriggly domain in Figure 26.1, there are six argmins or argmaxes. An \\(n\\)th-order polynomial can have up to \\(n-1\\) extrema. Note that the local polynomial approximations in Chapter 25 are at most 2nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms (\\(x^2\\) or \\(y^2\\)) then there is no argmax for the function. 26.2 Multiple inputs? High-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, \\(x\\), and \\(y\\). \\[\\underbrace{b_0 + b_x x + b_y y}_\\text{first-order terms} + \\underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\\text{second-order terms} + \\underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\\text{third-order terms}\\] This has 10 coefficients. With so many coefficients it’s hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in terms of the modeling situation, that meaning is spread out and hard to quantify. 26.3 High-order approximations The potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data.&amp;[The mathematical background needed for those better tools won’t be available to us until Block 5, when we explore linear algebra.] Polynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: \\[g(x) \\equiv x - \\frac{1}{6} x^3\\] Since the highest-order term is \\(x^3\\) this is a third-order polynomial. (As you’ll see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for \\(g(x=1)\\) is \\(5/6\\). Similarly, \\(g(x=1/2) = 23/48\\) and \\(g(x=2) = 2/3\\). A person of today’s generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton’s time were accomplished human calculators. It would have been well within their capabilities to calculate, using paper and pencil, \\(g(\\pi/4) = 0.7046527\\).20 Our example polynomial, \\(g(x) \\equiv x - \\frac{1}{6}x^3\\), graphed in color in Figure 26.2, doesn’t look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid’s output is always in \\(-1 \\leq \\sin(x) \\leq 1\\), while the polynomial’s tails are heading off to \\(\\infty\\) and \\(-\\infty\\). But, for a small interval around \\(x=0\\), exactly aligns with the sinusoid. Figure 26.2: The polynomial \\(g(x) \\equiv x -x^3 / 6\\) is remarkably similar to \\(\\sin(x)\\) near \\(x=0\\). It’s clear from the graph that the approximation is excellent near \\(x=0\\) and gets worse as \\(x\\) gets larger. The approximation is poor for \\(x \\approx \\pm 2\\). We know enough about polynomials to say that the approximation will not get better for larger \\(x\\); the sine function has a range of \\(-1\\) to \\(1\\), while the left and right tails of the polynomial are heading off to \\(\\infty\\) and \\(-\\infty\\) respectively. One way to measure the quality of the approximation is the error \\({\\cal E}(x)\\) which gives, as a function of \\(x\\), the difference between the actual sinusoid and the approximation: \\[{\\cal E}(x) \\equiv |\\strut\\sin(x) - g(x)|\\] The absolute value used in defining the error reflects our interest in how far the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure 26.3 shows \\({\\cal E}(x)\\) as a function of \\(x\\). Since the error is the same on both sides of \\(x=0\\), only the positive \\(x\\) domain is shown. Figure 26.3: The error \\({\\cal E}(x)\\) of \\(x - x^3/6\\) as an approximation to \\(\\sin(x)\\). Top panel: linear scale. Bottom panel: on a log-log scale. Figure 26.3 shows that for \\(x &lt; 0.3\\), the error in the polynomial approximation to \\(\\sin(x)\\) is in the 5th decimal place. For instance, \\(\\sin(0.3) = 0.2955202\\) while \\(g(0.3) = 0.2955000\\). That the graph of \\({\\cal E}(x)\\) is a straight-line on log-log scales diagnoses \\({\\cal E}(x)\\) as a power law. That is: \\({\\cal E}(x) = A x^p\\). As always for power-law functions, we can estimate the exponent \\(p\\) from the slope of the graph. It’s easy to see that the slope is positive, so \\(p\\) must also be positive. The inevitable consequence of \\({\\cal E}(x)\\) being a power-law function with positive \\(p\\) is that \\(\\lim_{x\\rightarrow 0} {\\cal E}(x) = 0\\). That is, the polynomial approximation \\(x - \\frac{1}{6}x^3\\) is exact as \\(x \\rightarrow 0\\). Throughout this book, we’ve been using straight-line approximations to functions around an input \\(x_0\\). \\[g(x) = f(x_0) + \\partial_x f(x_0) [x-x_0]\\] One way to look at \\(g(x)\\) is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of \\(f(x)\\) at \\(x_0\\), we can arrange things so that the second-order polynomial will also match the curvature of the \\(f()\\). Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative will necessarily match the slope and curvature of \\(f()\\). This can be accomplished by setting the polynomial coefficients appropriately. Start with a general, second-order polynomial centered around \\(x_0\\): \\[g(x) \\equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\\] The first- and second-derivatives, evaluated at \\(x=x_0\\) are: \\[\\partial_x g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = a_1 + 2 a_2 [x - x_0] \\left.{\\Large\\strut}\\right|_{x=x_0} = a_1\\] \\[\\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = 2 a_2\\] Notice the 2 in the above expression. When we want to write the coefficient \\(a_2\\) in terms of the second derivative of \\(g()\\), we’ll end up with \\[a_2 = \\frac{1}{2} \\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] To make \\(g(x)\\) approximate \\(f(x)\\) at \\(x=x_0\\), we need merely set \\[a_1 = \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] and \\[a_2 = \\frac{1}{2} \\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of \\(f(x)\\) at \\(x_0\\), set \\[a_3 = \\frac{1}{6} \\partial_{xxx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. \\(a_1\\) involves only \\(\\partial_x f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_2\\) coefficient involves only \\(\\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_3\\) coefficient involves only \\(\\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\), and so on. Now we can explain where the polynomial that started this section, \\(x - \\frac{1}{6} x^3\\) came from and why those coefficients make the polynmomial approximate the sinusoid near \\(x=0\\). Order \\(\\sin(x)\\) derivative \\(x - \\frac{1}{6}x^3\\) derivative 0 \\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\) \\(\\left( 1 - \\frac{1}{6}x^3\\right)\\left.{\\Large\\strut}\\right|_{x=0} = 0\\) 1 \\(\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = 1\\) \\(\\left(1 - \\frac{3}{6} x^2\\right) \\left.{\\Large\\strut}\\right|_{x=0}= 1\\) 2 \\(-\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\) \\(\\left(- \\frac{6}{6} x\\right) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\) 3 \\(-\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = -1\\) \\(- 1\\left.{\\Large\\strut}\\right|_{x=0} = -1\\) 4 \\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\) \\(0\\left.{\\Large\\strut}\\right|_{x=0} = 0\\) The first four derivatives of \\(x - \\frac{1}{6} x^3\\) exactly match, at \\(x=0\\), the first four derivatives of \\(\\sin(x)\\). The polynomial constructed by matching successive derivatives of a function \\(f(x)\\) at some input \\(x_0\\) is called a Taylor polynomial. Let’s construct a 3rd-order Taylor polynomial approximation to \\(f(x) = e^x\\) around \\(x=0\\). We know it will be a 3rd order polynomial: \\[g_{\\exp}(x) \\equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\\] The exponential function is particularly nice for examples because the function value and all it’s derivatives are identical: \\(e^x\\). So \\[f(x= 0) = 1\\] \\[ \\partial_x f(x=0) = 1\\] \\[\\partial_{xx} f(x=0) = 1\\] \\[\\partial_{xxx} f(x=0) = 1\\] and so on. The function value and derivatives of \\(g_{\\exp}(x)\\) at \\(x=0\\) are: \\[g_{\\exp}(x=0) = a_0\\] \\[\\partial_{x}g_{\\exp}(x=0) = a_1\\] \\[\\partial_{xx}g_{\\exp}(x=0) = 2 a_2\\] \\[\\partial_{xxx}g_{\\exp}(x=0) = 2\\cdot3\\cdot a_3 = 6\\, a_3\\] Matching these to the exponential evaluated at \\(x=0\\), we get \\[a_0 = 1\\] \\[a_1 = 1\\] \\[a_2 = \\frac{1}{2}\\] \\[a_3 = \\frac{1}{2 \\cdot 3} = \\frac{1}{6}\\] Result: the 3rd-order Taylor polynomial approximation to the exponential at \\(x=0\\) is \\[g_{\\exp}(x) = 1 + x + \\frac{1}{2} x^2 + \\frac{1}{2\\cdot 3} x^3 +\\frac{1}{2\\cdot 3\\cdot 4} x^4\\] Figure 26.4 shows the exponential function \\(e^x\\) and its 3th-order Taylor polynomial approximation near \\(x=0\\): Figure 26.4: The 3th-order Taylor polynomial approximation to \\(e^x\\) arount \\(x=0\\) The polynomial is exact at \\(x=0\\). The error \\({\\cal E}(x)\\) grows with increasing distance from \\(x=0\\): Figure 26.5: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\). The plot of \\(\\log_{10} {\\cal E}(x)\\) versus \\(\\log_{10} | x |\\) in Figure 26.5 shows that the error grows from zero at \\(x=0\\) as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give \\({\\cal E}(x) = a |x-x_0|^5\\). This is typical of Taylor polynomials: for a polynomial of degree \\(n\\), the error will grow as a power-law with exponent \\(n+1\\). This means that the higher is \\(n\\), the faster \\(\\lim_{x\\rightarrow x_0}{\\cal E}(x) \\rightarrow 0\\). On the other hand, since \\({\\cal E}_x\\) is a power law function, as \\(x\\) gets further from \\(x_0\\) the error grows as \\(\\left(x-x_0\\right)^{n+1}\\). Brooke Taylor (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: “[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it ‘the main [theoretical] foundation of differential calculus’.”Source Figure 26.6: Brook Taylor Due to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data. Taylor’s work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss’s techniques are the foundation of an incredibly important statistical method that is ubiquitous today: least squares. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic fitModel() function for polishing parameter estimates is based on least squares. In Block 5, we’ll explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters. 26.4 Indeterminate forms Let’s return to an issue that has bedeviled calculus students since Newton’s time. The example we’ll use is the function \\[\\text{sinc}(x) \\equiv \\frac{\\sin(x)}{x}\\] The sinc() function (pronounced “sink”) is still important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound) into continuous signals. What is the value of \\(\\text{sinc}(0)\\)? One answer, favored by arithmetic teachers is that \\(\\text{sinc}(0)\\) is meaningless, because it involves division by zero. On the other hand, \\(\\sin(0) = 0\\) as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an indeterminant form. The logic is this: Suppose we assume that \\(0/0 = b\\) for some number \\(b\\). then \\(0 = 0 \\times b = 0\\). So any value of \\(b\\) would do; the value of \\(0/0\\) is “indeterminant.” Still another answer is suggested by plotting out sinc(\\(x\\)) near \\(x=0\\) and reading the value off the graph: sinc(0) = 1. slice_plot(sin(x) / x ~ x, domain(x=c(-10,10)), npts=500) Figure 26.7: To judge from this plot, sin(0) = 1. The graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to \\(x=0\\), the graph continues to look smooth. We call such functions well behaved. Compare the well-behaved sinc() to a very closely related function (which doesn’t seem to be so important in applied work): \\(\\frac{\\sin(x)}{x^3}\\). Both \\(\\sin(x)/x\\) and \\(\\sin(x) / x^3\\), evaluated at \\(x=0\\) involve a divide by zero. Both are indeterminate forms 0/0 at \\(x=0\\). But the graph of \\(\\sin(x) / x^3\\) (see Figure ??) is not we’ll behaved. \\(\\sin(x) / x^3\\) does not have any particular value at \\(x=0\\); instead, it has an asymptote. slice_plot(sin(x) / x ~ x, domain(x=c(-0.1, 0.1)), npts=500) %&gt;% gf_refine(scale_y_log10()) Figure 26.8: Zooming in around the division by zero. Left: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Right: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different. slice_plot(sin(x) / x^3 ~ x, domain(x=c(-0.1, 0.1)), npts=500) %&gt;% gf_refine(scale_y_log10()) Figure 26.9: Zooming in around the division by zero. Left: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Right: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different. Since both \\(\\sin(x)/x\\left.{\\Large\\strut}\\right|_{x=0}\\) and \\(\\sin(x)/x^3\\left. {\\Large\\strut}\\right|_{x=0}\\) involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it’s to be found near zero. For any non-zero value of \\(x\\), the arithmetic to evaluate the functions is straight-forward. Note that \\(\\sin(x) / x^3\\) starts its mis-behavior away from zero. The slope of \\(\\sin(x) / x^3\\) is very large near \\(x=0\\), while the slope of \\(\\sin(x) / x\\) smoothly approaches zero. Since we’re interested in behavior near \\(x=0\\), a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations. \\(\\sin(x) \\approx x - \\frac{1}{6} x^3\\) near \\(x=0\\) \\(x\\) is already a polynomial. \\(x^3\\) is already a polynomial. Remember, these approximations are exact as \\(x\\) goes to zero. So sufficiently close to zero, \\[\\frac{\\sin(x)}{x} = \\frac{x - \\frac{1}{6} x^3}{x} = 1 + \\frac{1}{6} x^2\\] Even at \\(x=0\\), there’s nothing indeterminant about \\(1 + x^2/6\\); it’s simply 1. Compare this to the polynomial approximation to \\(\\sin(x) / x^3\\): \\[\\frac{\\sin(x)}{x^3} = \\frac{x - \\frac{1}{6} x^3}{x^3} = \\frac{1}{x^2} - \\frac{1}{6}\\] Evaluating this at \\(x=0\\) involves division by zero. No wonder it’s badly behaved. The procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: The analysis into the infinitely small for the understanding of curved lines. In honor of the author, the Marquis de l’Hospital, the procedure is called l’Hopital’s rule.21 Conventionally, the relationship is written \\[\\lim_{x\\rightarrow x_0} \\frac{u(x)}{v(x)} = \\lim_{x\\rightarrow x_0} \\frac{\\partial_x u(x)}{\\partial_x v(x)}\\] Let’s try this out with our two example functions around \\(x=0\\): \\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 1} = \\frac{1}{1} = 1\\] \\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x^3} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 3x^2} = \\frac{1}{0} \\ \\ \\text{indeterminate}!\\] 26.5 Computing with indeterminate forms In the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient, since programmers did not always forsee division-by-zero situations and avoid them. As you’ve seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminant forms: Inf and NaN. Inf is the output for the simple case of dividing zero into a non-zero number, for instance: 17/0 ## [1] Inf NaN, standing for “not a number,” is the output for more challenging cases: dividing zero into zero, or multiplying Inf by zero, or dividing Inf by Inf. 0/0 ## [1] NaN 0 * Inf ## [1] NaN Inf / Inf ## [1] NaN The brilliance of the idea is that any calculation that involves NaN will return a value of NaN. This might seem to get us nowhere. But most programs are built out of other programs, usually written by other people interested in other applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it’s important to respond in some particularly way, you can always check the result for being NaN in your own programs. (Much the same is true for Inf, although dividing a non-Inf number by Inf will return 0.) Plotting software will often treat NaN values as “don’t plot this.” That’s why it’s possible to make a sensible plot of \\(\\sin(x)/x\\) even when the plotting domain includes zero. 26.6 Exercises Exercise 26.02: 3EdMBL The Taylor polynomial for \\(e^x\\) has an especially lovely formula: \\[p(x) = 1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots\\] Question A In the above formula, the center \\(x_0\\) does not appear. Why not? Having a center is not a requirement for a Taylor polynomial.︎✘ This is wrong. All Taylor polynomials are expansions around some fixed center value. There is a center, \\(x_0 = 1\\), but terms like \\(x_0 x^2\\) were simplified to \\(x^2\\).︎✘ Look again at the Taylor formula at the start of this section. The basis functions are \\(x-x_0, (x-x_0)^2, \\ldots\\), not \\(x_0 x, x_0 x^2, \\ldots\\) There is a center, \\(x_0 = 0\\), but the terms like \\((x-x_0)^2\\) were simplified to \\(x^2\\).Nice!  Exercise 26.04: o6dP4k Here is a Taylor polynomial: \\[p(x) = e + \\frac{e}{1!} (x-1) + \\frac{e}{2!} (x-1)^2 + \\cdots\\] Question A Where is the center \\(x_0\\) of this polynomial?     \\(x_0 = -2\\)︎✘        \\(x_0 = -1\\)︎✘        \\(x_0 = 0\\)︎✘        \\(x_0=1\\)\\(\\heartsuit\\ \\)       \\(x_0 = 2\\)︎✘ Question B Your roommate suggests that \\(p(x)\\) is a Taylor expansion of \\(e^x\\) around \\(x=1\\). Is he right? No, a polynomial doesn’t have functions like \\(e\\).︎✘ \\(e\\) is not a function, it’s just a number, 2.718282…. Yes. The center is \\(x_0 = 1\\).Good.  Not really. The formula suggests that the center is \\(x_0=1\\) but the coefficients are wrong.︎✘ The coefficients are right. Exercise 26.06: Co1Ekt Consider the function \\(f(x) \\equiv (x - 3)^2\\). Question A Using ordinary algebra, \\(f(x)\\) can be expanded as \\((x^2 - 6 x -9)\\). Is \\[p(x) = -9 - 6 x + x^2\\] a Taylor polynomial expansion of \\(f(x)\\)? Yes, with a center at \\(x_0 = 0\\)Right!  Yes, with a center at \\(x_0 = 3\\)︎✘ No, because there are no factorials involved︎✘ The factorials are already built-in to the coefficients. Exercise 26.08: llX7EF Here’s the Taylor polynomial expansion of \\(\\sin(x)\\) about a center \\(x_0\\): \\[p(x) = 1/2 - \\frac{\\sqrt{3}/{2}}{2!} (x - x_0)^2 + \\frac{1/2}{4!} (x - x_0)^4 + \\cdots\\] Question A Which of these is the numerical value of \\(x_0\\)? (Hint: Remember that the coefficients involve the function and its derivatives evaluated at \\(x_0\\) as described in Section 26.3.)     \\(\\pi/6\\)\\(\\heartsuit\\ \\)       \\(\\pi/3\\)︎✘        \\(\\pi/2\\)︎✘        \\(\\pi\\)︎✘ Exercise 26.10: ecdVKx With very high-order derivatives, it can be awkward to use a notation with repeated subscripts, like \\(\\partial_{xxxxx} f(x)\\). Another notation often encountered is \\(f^{(5)}\\), where the integer in the superscript tells the order of the derivative. We’ll use that notation in this problem. For a function \\(f(x)\\) and its derivatives \\(f^{(1)}(x)\\), \\(f^{(2)}(x)\\), … the Taylor polynomial \\(p(x)\\) centered on \\(x_0\\) is \\[p(x) \\equiv f(x_0) + \\frac{f^{(1)}(x_0)}{1!} (x - x_0)^1 + \\frac{f^{(2)}(x_0)}{2!} (x - x_0)^2 + \\cdots\\] A Taylor polynomial, like all polynomials, is a linear combination of basic functions. Question A Which of these are the basic functions being linearly combined in a Taylor polynomial? \\(f(x), f^{(1)}(x), f^{(2)}(x), \\ldots\\)︎✘ You won’t find any of these in the definition of \\(p(x)\\) given above. You’re confusing \\(x\\) with \\(x_0\\). \\(f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \\ldots\\)︎✘ These are not functions of \\(x\\). They are numbers computed by evaluating a function at the center point \\(x_0\\). \\(f(x_0), \\frac{f^{(1)}(x_0)}{1!}, \\frac{f^{(2)}(x_0)}{2!}, \\ldots\\)︎✘ These are not functions of \\(x\\). They are numbers computed by evaluating a function at the center point \\(x_0\\). \\((x-x_0), (x - x_0)^2, \\ldots\\)Correct. These are the only places where the variable \\(x\\) appears in the Taylor formula. Exercise 26.12: 682lsB Each of the three functions graphed below is a simple power-law function that can be written \\(\\left[x-x_0\\right]^n\\). The three functions have different values for \\(x_0\\) and for \\(n\\). Question A For the blue function, what is \\(x_0\\)?     0︎✘        1︎✘        2︎✘        3︎✘        4\\(\\heartsuit\\ \\)       5︎✘ Question B For the blue function, what is the order of the polynomial?     0︎✘        1\\(\\heartsuit\\ \\)       2︎✘        3︎✘        4︎✘        5︎✘ Question C For the orange function, what is \\(x_0\\)?     0︎✘        1︎✘        2\\(\\heartsuit\\ \\)       3︎✘        4︎✘        5︎✘ Question D For the orange function, what is the order of the polynomial?     0︎✘        1︎✘        2\\(\\heartsuit\\ \\)       3︎✘        4︎✘        5︎✘ Question E For the green function, what is \\(x_0\\)?     -2︎✘        -1\\(\\heartsuit\\ \\)       0︎✘        1︎✘        2︎✘        3︎✘ Question F For the green function, what is the order of the polynomial?     0︎✘        1︎✘        2︎✘        3\\(\\heartsuit\\ \\)       4︎✘        5︎✘ Exercise 26.14: cQT2v6 At \\(x=0\\), the value of \\(x \\ln(x)\\) is indeterminate, having the form \\(0 \\cdot \\infty\\). Using a sandbox, plot out \\(x \\ln(x)\\) over the domain \\(0 &lt; x &lt; \\frac{1}{2}\\). Question A From the graph, determine \\(\\lim_{x\\rightarrow 0} x \\ln(x()\\). Choose the correct answer.     -0.2︎✘        0\\(\\heartsuit\\ \\)       0.1︎✘        0.5︎✘        not well behaved︎✘ Use l’Hopital’s rule to confirm that \\(\\lim_{x\\rightarrow 0} x \\ln(x()\\) exists. Here, the rule has the form: \\[\\lim_{x\\rightarrow 0} x \\cdot \\ln(x) = [\\lim_{x\\rightarrow 0} \\partial_x x] \\cdot [\\lim_{x\\rightarrow 0} \\partial_x \\ln(x)]\\] Question B Does the application of l’Hopital’s rule confirm the result from graphing the function? YesRight!  No︎✘ Aren’t you getting \\(\\frac{1}{1/x} = x\\) when you carry out the differentiation involved? Exercise 26.16: jAOmME We would like to write a Taylor-series polynomial for \\(\\dnorm(x)\\). To do this, we will need the numbers \\[\\dnorm(x_0),\\ \\ \\partial_x \\dnorm(x_0),\\ \\ \\partial_{xx} \\dnorm(x_0),\\ \\ \\partial_{xxx} \\dnorm(x_0), ...\\] Here are the functions for the derivatives Order formula for derivative 0th \\(\\dnorm(x)\\) 1st \\(\\left[{\\large\\strut}-x\\right]\\dnorm(x)\\) 2nd \\(\\left[{\\large\\strut}x^2 - 1\\right]\\dnorm(x)\\) 3rd \\(\\left[{\\large\\strut}3x - x^3\\right]\\dnorm(x)\\) 4th \\(\\left[{\\large\\strut}3\\,x^2-x^4 \\right]\\dnorm(x)\\) 5th \\(\\left[{\\large\\strut}3-6\\,x^3 + x^5 \\right]\\dnorm(x)\\) 6th \\(\\left[{\\large\\strut}-15\\,x^2 + 10\\,x^4-x^6 \\right]\\dnorm(x)\\) 7th \\(\\left[{\\large\\strut}-15 + 45\\,x^3-15\\,x^5 + x^7 \\right]\\dnorm(x)\\) 8th \\(\\left[{\\large\\strut}105\\,x^2-105\\,x^4 + 21\\,x^6-x^8 \\right]\\dnorm(x)\\) Write a Taylor polynomial function for \\(\\dnorm(x)\\) around \\(x_0=0\\)). To do this, you will have to evaluate each of the 9 formulas above at \\(x=0\\), producing a number. For example, the first five formulas will give 0.3989, 0, -0.3989, 0, 1.1968. This would expand into the following Taylor function: T_at_0 = makeFun(0.3989 + 0*(x-x0) + - 0.3989*(x-x0)^2/factorial(2) + 0*(x-x0)^3/factorial(3) + 1.1968*(x-x0)^4/factorial(4) ~ x, x0=0) You’ll need to add in the 6th, 7th, and 8th terms. Plot T_at_0(x) ~ x over the domain \\(0 \\leq x \\leq 2\\), and pipe the result to a slice-plot of dnorm(x) ~ x so that you can compare the two plots. Define another function T_at_1() which will be much like T_at_zero() but with x0=1 and the coefficients changed to be the formulas evaluated at \\(x=1\\). Add a layer showing T_at_1() to your previous plot. Say over what domain T_at_0() is a good approximation to dnorm() and over what domain T_at_1() is a good approximation to dnorm(). Do the two domains overlap? Write a piecewise function of this form: T &lt;- makeFun(ifelse(abs(x) &lt; 0.5, T_at_0(abs(x)), T_at_1(abs(x))) ~ x) Plot this out on top of dnorm() to show whether T() is a good approximation to dnorm(). You could continue on to define T_at_2() and incorporate that into the piecewise T(), and so on, to construct an approximation to dnorm() that is accurate over a larger domain. The fundamental theorem says that an order-n polynomial has n roots (including multiplicities).↩︎ Unfortunately for these human calculators, pencils weren’t invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.↩︎ In many French words, the sequence “os” has been replaced by a single, accented letter, \\(\\hat{\\text{o}}\\).↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
