<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 30 Probability and evidence | MOSAIC-Calculus.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 30 Probability and evidence | MOSAIC-Calculus.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 30 Probability and evidence | MOSAIC-Calculus.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="opimization-and-constraint.html"/>
<link rel="next" href="mechanics.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<span class="math inline">
\(\newcommand{\line}{\text{line}}
\newcommand{\hump}{\text{hump}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\recip}{\text{recip}}
\newcommand{\diff}[1]{{\cal D}_#1}
\newcommand{\pnorm}{\text{pnorm}}
\newcommand{\dnorm}{\text{dnorm}}
\newcommand{\CC}[1]{\color{#648fff}{#1}}
\newcommand{\CE}[1]{\color{#785ef0}{#1}}
\newcommand{\CA}[1]{\color{#dc267f}{#1}}
\newcommand{\CB}[1]{\color{#fe6100}{#1}}
\newcommand{\CD}[1]{\color{#ffb000}{#1}}\)
</span>






<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="mosaic-calc-style-copy.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><span style="font-size: 150%;><strong><a href="../index.html">MOSAIC Calculus</a></strong></span></li>
<li><em><a>Calculus for the 21st century</a></em></li>
<li><a>Daniel Kaplan</a></li>
<hr>
<li><a href="../block-1/change.html">Block 1: Functions & quantities</a></li>
<li><a href="../block-2/change-relationships.html">Block 2: Differentiation</a></li>
<li><a href="../block-3/change-accumulation.html">Block 3: Accumulation</a></li>
<li><strong><a href="../block-4/block4-intro.html">Block 4: Manifestations</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="block4-intro.html"><a href="block4-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="27" data-path="operations.html"><a href="operations.html"><i class="fa fa-check"></i><b>27</b> Operations on functions</a>
<ul>
<li class="chapter" data-level="27.1" data-path="operations.html"><a href="operations.html#task-solve"><i class="fa fa-check"></i><b>27.1</b> Task: Solve</a></li>
<li class="chapter" data-level="27.2" data-path="operations.html"><a href="operations.html#task-argmax"><i class="fa fa-check"></i><b>27.2</b> Task: Argmax</a></li>
<li class="chapter" data-level="27.3" data-path="operations.html"><a href="operations.html#task-iterate"><i class="fa fa-check"></i><b>27.3</b> Task: Iterate</a></li>
<li class="chapter" data-level="27.4" data-path="operations.html"><a href="operations.html#software-for-the-tasks"><i class="fa fa-check"></i><b>27.4</b> Software for the tasks</a></li>
<li class="chapter" data-level="27.5" data-path="operations.html"><a href="operations.html#algorithmic-techniques"><i class="fa fa-check"></i><b>27.5</b> Algorithmic techniques</a></li>
<li class="chapter" data-level="27.6" data-path="operations.html"><a href="operations.html#exercises"><i class="fa fa-check"></i><b>27.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="data-driven-functions.html"><a href="data-driven-functions.html"><i class="fa fa-check"></i><b>28</b> Data-driven functions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="data-driven-functions.html"><a href="data-driven-functions.html#generating-smooth-motion"><i class="fa fa-check"></i><b>28.1</b> Generating smooth motion</a></li>
<li class="chapter" data-level="28.2" data-path="data-driven-functions.html"><a href="data-driven-functions.html#piecewise-but-smooth"><i class="fa fa-check"></i><b>28.2</b> Piecewise but smooth</a></li>
<li class="chapter" data-level="28.3" data-path="data-driven-functions.html"><a href="data-driven-functions.html#cubic-splines"><i class="fa fa-check"></i><b>28.3</b> C<sup>2</sup> smooth functions</a></li>
<li class="chapter" data-level="28.4" data-path="data-driven-functions.html"><a href="data-driven-functions.html#bézier-splines"><i class="fa fa-check"></i><b>28.4</b> Bézier splines</a></li>
<li class="chapter" data-level="28.5" data-path="data-driven-functions.html"><a href="data-driven-functions.html#exercises-1"><i class="fa fa-check"></i><b>28.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="opimization-and-constraint.html"><a href="opimization-and-constraint.html"><i class="fa fa-check"></i><b>29</b> Opimization and constraint</a>
<ul>
<li class="chapter" data-level="29.1" data-path="opimization-and-constraint.html"><a href="opimization-and-constraint.html#gradient-descent"><i class="fa fa-check"></i><b>29.1</b> Gradient descent</a></li>
<li class="chapter" data-level="29.2" data-path="opimization-and-constraint.html"><a href="opimization-and-constraint.html#objectives-and-constraints"><i class="fa fa-check"></i><b>29.2</b> Objectives and Constraints</a></li>
<li class="chapter" data-level="29.3" data-path="opimization-and-constraint.html"><a href="opimization-and-constraint.html#constraint-cost"><i class="fa fa-check"></i><b>29.3</b> Constraint cost</a></li>
<li class="chapter" data-level="29.4" data-path="opimization-and-constraint.html"><a href="opimization-and-constraint.html#exercises-2"><i class="fa fa-check"></i><b>29.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html"><i class="fa fa-check"></i><b>30</b> Probability and evidence</a>
<ul>
<li class="chapter" data-level="30.1" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#probability-density"><i class="fa fa-check"></i><b>30.1</b> Probability density</a></li>
<li class="chapter" data-level="30.2" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#three-density-functions"><i class="fa fa-check"></i><b>30.2</b> Three density functions</a></li>
<li class="chapter" data-level="30.3" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#mean-and-variance"><i class="fa fa-check"></i><b>30.3</b> Mean and variance</a></li>
<li class="chapter" data-level="30.4" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#risk-and-data"><i class="fa fa-check"></i><b>30.4</b> Risk and data</a></li>
<li class="chapter" data-level="30.5" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#exercises-3"><i class="fa fa-check"></i><b>30.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>31</b> Mechanics</a>
<ul>
<li class="chapter" data-level="31.1" data-path="mechanics.html"><a href="mechanics.html#work"><i class="fa fa-check"></i><b>31.1</b> Work</a></li>
<li class="chapter" data-level="31.2" data-path="mechanics.html"><a href="mechanics.html#energy"><i class="fa fa-check"></i><b>31.2</b> Energy</a></li>
<li class="chapter" data-level="31.3" data-path="mechanics.html"><a href="mechanics.html#momentum"><i class="fa fa-check"></i><b>31.3</b> Momentum</a></li>
<li class="chapter" data-level="31.4" data-path="mechanics.html"><a href="mechanics.html#center-of-mass"><i class="fa fa-check"></i><b>31.4</b> Center of mass</a></li>
<li class="chapter" data-level="31.5" data-path="mechanics.html"><a href="mechanics.html#angular-momentum-and-torque"><i class="fa fa-check"></i><b>31.5</b> Angular momentum and torque</a></li>
<li class="chapter" data-level="31.6" data-path="mechanics.html"><a href="mechanics.html#exercises-4"><i class="fa fa-check"></i><b>31.6</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="../block-5/vectors.html">Block 5: Linear combinations</a></li>
<li><a href="../block-6/dynamics.html">Block 6: Dynamics</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-and-evidence" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 30</span> Probability and evidence</h1>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
<p>This chapter is about the use of functions for the purpose of expressing what we do and do not know about uncertain situations. Such situations are common. For instance, a person may be at high risk for a disease, say, diabetes or lung cancer. The term “high risk” indicates that we know something about the situation: not whether or not the person has the disease but whether they are “likely” to have or to get it. Another example: a car might be said to be “unreliable.” We do not mean by this that the car cannot be used. Rather we’re thinking that from time to time the car might fail to start or run. A car where this happens once over a few year span is reliable, a car where this happens on a month-to-month basis is not reliable.</p>
<p>You may well have had some textbook exposure to <strong><em>probability</em></strong> as an intellectual field. Typical examples used to illustrate concepts and methods are coins being flipped, dice being tossed, and spinners spun. Colored balls are drawn from urns, slips of paper from hats, and so on. Each of these is a physical representation of an idealized mechanism where we feel sure we understand how likely each possible outcome is to happen.</p>
<p>In this chapter, we’ll use two basic imagined settings where uncertainty comes into play: the risk of disease before the disease is diagnosed and the safety of a self-driving car before it is in an accident. The word “imagined” signals that you should not draw conclusions about the facts of any particular disease or any particular self-driving car; we are merely using the imagined settings to lay out concepts and methods for the mathematical presentation and analysis of uncertainty and risk. Of particular importance will be the mathematical means by which we represent our knowledge or belief in these settings and the way we can properly update our knowledge/belief when new information becomes available.</p>
<div class="takenote">
<p>The calculus of probability and data introduces an additional convention for describing and naming functions. Throughout this book, the names have reflected the “shape” of the function—exponential, sinusoidal, sigmoidal, etc.—or the route by which the function was constructed, e.g. differentiation, anti-differentiation, inversion. Probability calculations often work out to be assembling an answer out of different kinds of component functions. An analogy is the assembly of an automobile out of different kinds of components: wheels, motors, body, and so on. You can’t put a wheel where the motor should go and produce a proper automobile. All motors play the same sort of role in the function of an automobile, but they can have different “shapes” such as gasoline, diesel, or electric.</p>
<p>To understand how cars are built, you have to be able easily to distinguish between the different kinds of components. This is second nature to you because you have so much experience with automobiles. Likewise, to understand the probability calculations, you’ll have to master the distinctions between the roles functions play in a calculation. In this chapter you’ll see <strong><em>probability density functions</em></strong> as well as <strong><em>likelihood functions</em></strong> and <strong><em>prior functions</em></strong> and <strong><em>posterior functions</em></strong> and some others. As you get started, you will confuse these roles for functions with one another, just as a newborn child can confuse “wheel” with “motor” until experience is gained.</p>
<p>Make sure to note the role-labels given to the functions you are about to encounter.</p>
</div>
<div id="probability-density" class="section level2" number="4.1">
<h2><span class="header-section-number">30.1</span> Probability density</h2>
<p>A probability, as you may know, is a dimensionless number between zero and one (inclusive). In this chapter, you’ll be dealing with functions relating to probabilities. The input to these functions will usually be a quantity that can have dimension, for instance, miles driven by a car. For some of the functions we will see in this chapter, the output will be a probability. For other functions in this chapter, the output will be a <strong><em>probability density</em></strong>.</p>
<p>Probability relates to the abstract notion of an <strong><em>event</em></strong>. An event is a process that produces an <strong><em>outcome</em></strong>. For instance:</p>
<ul>
<li>Flipping a coin is an event where the possible outcomes of H and T.</li>
<li>Taking a medical screening test is an event where the outcomes are “positive” or “negative.”</li>
<li>Throwing a dart at a bullseye is an event where the outcome is the distance of the landing point from the center of the bullseye.</li>
</ul>
<p>An event with a discrete outcome—coin flip, medical screening test—can be modeled by assigning a probability number to each of the possible outcomes. To be a valid probability model, each of those assigned numbers should be greater than or equal to zero. In addition, the sum of the assigned numbers across all the possible outcomes should be 1.</p>
<p>Events with a continuous outcome, such as the dart toss where the outcome—distance from the center—the probability model takes the form of a <strong><em>function</em></strong> whose domain is the possible outcomes. For the model to be a valid probability model, we require that the function output should never be less than zero. There’s another requirement as well: the integral of the function over the entire domain should be 1. For the dart-toss event, if we denote the distance from the bullseye as <span class="math inline">\(r\)</span> and the assigned number for the probability model as <span class="math inline">\(g(r)\)</span>, the integral requirement amounts to <span class="math display">\[\int_0^\infty g(r) dr = 1\ .\]</span></p>
<p>Note that the output <span class="math inline">\(g(r)\)</span> is <strong>not a probability</strong>, it is a <strong><em>probability density</em></strong>. To see why, let’s use the fundamental theorem of calculus to break up the integral into three segments: <span class="math inline">\(0 \leq r \leq a\)</span> and <span class="math inline">\(a &lt; r \leq b\)</span> and <span class="math inline">\(b &lt; r\)</span>. The total integral is <span class="math display">\[\int_0^\infty g(r) dr = 1\ = \int_0^a g(r) dr + \int_a^b g(r) dr + \int_b^\infty g(r) dr.\]</span>
The probability that the dart lands at a distance somewhere between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is <span class="math display">\[\int_a^b g(r) dr\ .\]</span>
The dimension <span class="math inline">\([r] =\ \)</span>L and, suppose the units are centimeters. We need <span class="math inline">\(\int g(r) dr\)</span> to be a dimensionless number. Since the dimension of the integral is <span class="math inline">\([r] \cdot [g(r)] = [1]\)</span>, it must be that <span class="math inline">\([g(r)] = [1/r] = \text{L}^{-1}\)</span>. Thus, <span class="math inline">\(g(r)\)</span> is not a probability simply because it is not dimensionless. Instead, in the dart example, it is a “probability-per-centimeter.” This kind of quantity—probability <em>per something</em>—is called a <strong><em>probability density</em></strong> and <span class="math inline">\(g(r)\)</span> itself is a <strong><em>probability density function</em></strong>.</p>
<p>To show the aptness of the word “density,” let’s switch to a graphic of a function that uses literal density of ink as the indicator of the function value. Figure <a href="probability-and-evidence.html#fig:dart-g">30.1</a> shows what the dart <span class="math inline">\(g(r)\)</span> might look like:</p>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dart-g"></span>
<img src="MOSAIC-Calculus_files/figure-html/dart-g-1.png" alt="Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink." width="90%" />
<p class="caption">
Figure 30.1: Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink.
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 30.1  </strong></span>Why a probability density function is often a good way to describe what you know about a situation. As an example, we’ll consider a simple competition of the sort you might encounter at a fund-raising fair. There is a jar on display, filled with coins that have been donated by one of the fair’s sponsors. You pay $1 (which goes to a good cause) to enter the contest. Your play is to describe how much money is in the jar, writing your description down along with your name on an entry form. At the end of the day, an official will open the jar, count the money, and announce who made the best estimate. The winner gets the money in the jar.</p>
<p><img src="www/jar-coins.png" width="25%" style="display: block; margin: auto;" /></p>
<p>In the usual way these contests are run, the contestants each write down a guess for the amount they think is in the jar, say $18.63. The winner is determined by seeing whose guess was closest to the actual value of the coins in the jar.</p>
<p>In reality, hardly anyone believes they can estimate the amount in the jar to the nearest penny. The person guessing $18.63 might prefer to be able to say, “between 18 and 19 dollars.” Or, maybe “$18 <span class="math inline">\(\pm 3\)</span>.” To communicate what you know about the situation, it’s best to express a range of possibilities that you think likely.</p>
<p>In our contest, we’ll ask the participants to describe their conclusions about the jar as a probability density function. For you, having worked through this book to this point, it would be sufficient for the instructions to say, “Hand in your probability density function for the amount of money in the jar.” But we would like even people who haven’t learned calculus to participate. So our instructions will state, “On the graph-paper axes below, sketch a continuous function expressing your best belief about how much money is in the jar. At all inputs, the function value must be zero or greater.”<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coin-graph-paper"></span>
<img src="MOSAIC-Calculus_files/figure-html/coin-graph-paper-1.png" alt="The entry form for the money-in-the-jar contest." width="90%" />
<p class="caption">
Figure 30.2: The entry form for the money-in-the-jar contest.
</p>
</div>
<p>Take a minute to look at the picture of the jar and draw your function on the axes shown above. Think about why we don’t need to scale the vertical axis on the entry form and don’t ask you to do it either.</p>
<p>Here are three contest entries.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contest-entries"></span>
<img src="MOSAIC-Calculus_files/figure-html/contest-entries-1.png" alt="Three contestants' contest entries." width="90%" />
<p class="caption">
Figure 30.3: Three contestants’ contest entries.
</p>
</div>
</div>
<p>Later in this chapter, we will work with methods that construct <strong><em>relative density</em></strong> functions. The “relative” means that the function clearly indicates where the probability is more or less dense, but the function has not yet been scaled to be a probability density function. Suppose <span class="math inline">\(h(x)\)</span> is a relative density function such that
<span class="math display">\[\int_{-\infty}^\infty h(x)\, dx = A \neq 1\ .\]</span> Although <span class="math inline">\(h(x)\)</span> is not a probability density function, the very closely related function <span class="math inline">\(\frac{1}{A} h(x)\)</span> will be a probability density function. We’ll use the term <strong><em>normalizing</em></strong> to refer to the simple process of turning a relative density function into a probability density function.</p>
<p>A relative density function is entirely adequate for describing the distribution of probability. However, when comparing two or more probability distributions, it’s important that they all be on the same scale. Normalizing the relative density functions to probability density functions accomplishes this. Figure <a href="probability-and-evidence.html#fig:contest-compare">30.4</a> compares the three relative probability functions in Figure <a href="probability-and-evidence.html#fig:contest-entries">30.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contest-compare"></span>
<img src="MOSAIC-Calculus_files/figure-html/contest-compare-1.png" alt="Comparing the contest entries by normalizing each of them to a probability density function." width="90%" />
<p class="caption">
Figure 30.4: Comparing the contest entries by normalizing each of them to a probability density function.
</p>
</div>
</div>
<div id="three-density-functions" class="section level2" number="4.2">
<h2><span class="header-section-number">30.2</span> Three density functions</h2>
<p>Three commonly used families of probability density functions are:</p>
<ul>
<li>the gaussian density function</li>
<li>the exponential density function</li>
<li>the uniform density function.</li>
</ul>
<p>The <strong><em>uniform density function</em></strong>, <span class="math inline">\(u(x, a, b)\)</span> is more or less the equivalent of the constant function. The family has two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with the function defined as:
<span class="math display" id="eq:uniform-dist">\[\begin{equation}\text{unif}(x, a, b) \equiv \left\{\begin{array}{cl}\frac{1}{b-a} &amp; \text{for}\ a \leq x \leq b\\0&amp; \text{otherwise} \end{array}\right.
\tag{30.1}
\end{equation}\]</span>
This function is used to express the idea of “equally likely to be any value in the range <span class="math inline">\([a, b]\)</span>.” For instance, to describe a probability that a January event is equally likely to occur at any point in the month, you can use <span class="math inline">\(u(x, 1, 31)\)</span> where <span class="math inline">\(x\)</span> and the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> have dimension T and are in units of days. Notice that the density itself has dimension T<sup>-1</sup> and units “per day.”</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uniform-density"></span>
<img src="MOSAIC-Calculus_files/figure-html/uniform-density-1.png" alt="The uniform density function." width="90%" />
<p class="caption">
Figure 30.5: The uniform density function.
</p>
</div>
<p>The <strong><em>gaussian density function</em></strong>, $(x, , ) is familiar to you from previous blocks in this book: the bell-shaped function. It’s known also as the <strong><em>normal distribution</em></strong> because it is so frequently encountered in practice. It is a way of expressing, “The outcome of the event will likely be close to this particular value.” The parameter named <strong>mean</strong> specifies “this particular value.” The parameter <strong>sd</strong> specifies what’s mean by “close.” The gaussian density function is smooth. It is never zero, but <span class="math inline">\(\lim_{x \rightarrow \pm \infty} dnorm(x, \text{mean}, \text{sd}) = 0\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gaussian-density"></span>
<img src="MOSAIC-Calculus_files/figure-html/gaussian-density-1.png" alt="Gaussian probability density. The labels show the probability mass in each division." width="90%" />
<p class="caption">
Figure 30.6: Gaussian probability density. The labels show the probability mass in each division.
</p>
</div>
<p>To use an analogy between physical density (e.g., kg per cubic-meter), where density times size gives mass, we can say that the total mass of a probability density function is always 1. For the gaussian density, 68% of of the total mass is within <span class="math inline">\(\pm 1\)</span>sd of the mean, 95% is within <span class="math inline">\(\pm 2\)</span>sd of the mean, 99.7% within <span class="math inline">\(\pm 3\)</span>sd, and 99.99% within <span class="math inline">\(\pm 4\)</span>sd.</p>
<p>The <strong><em>exponential probability density</em></strong> is shaped just like an exponential function <span class="math inline">\(e^{-kx}\)</span>. It’s used to describe events that are equally likely to happen in any <em>interval</em> of the input variable, and describes the relative probability that the first event to occur will be at <span class="math inline">\(x\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:exponential-density"></span>
<img src="MOSAIC-Calculus_files/figure-html/exponential-density-1.png" alt="Exponential density" width="90%" />
<p class="caption">
Figure 30.7: Exponential density
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 30.2  </strong></span>Do I want to do the beta distribution as an example? Not strictly needed unless I do some exercises with the driverless-car example.</p>
</div>
</div>
<div id="mean-and-variance" class="section level2" number="4.3">
<h2><span class="header-section-number">30.3</span> Mean and variance</h2>
<p>As you know, the gaussian function has two parameters: mean and sd. The mean describes the center of the distribution, the sd describes the width. You may also be familiar with the <em>statistic</em> called the <strong><em>arithmetic mean</em></strong>. You calculate the statistic by adding up a set of <span class="math inline">\(n\)</span> numbers and dividing by <span class="math inline">\(n\)</span>, that is,
<span class="math display">\[\text{mean} \equiv \frac{1}{n} \sum_{i=1}^n x_i\ ,\]</span>
where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>th row of a data frame with and <span class="math inline">\(x\)</span> is one of the columns. To illustrate, consider the <code>Zcalc::Home_utilities</code> data frame which has data on the monthly use of electricity and natural gas by a Minnesota household.</p>
<div class="rmosaic">
<p>The R/mosaic function for the calculation of the arithmetic mean is <code>df_stats()</code>. (The name stands for “data frame stats.”) Here’s a calculation of the mean outdoor temperature from <code>Home_utilities</code>:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="probability-and-evidence.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">df_stats</span>(<span class="sc">~</span> temp, <span class="at">data =</span> Zcalc<span class="sc">::</span>Home_utilities, mean)</span></code></pre></div>
<pre><code>##   response     mean
## 1     temp 48.08155</code></pre>
<p>The result has units of degrees Fahrenheit.</p>
<p><code>df_stats()</code> can calculate many other statistics on a column of data, such as <code>median</code>, <code>sd</code>, <code>var</code>, <code>quantile()</code>s (such as <code>quantile(0.33)</code>), confidence intervals (<code>ci.mean()</code>, <code>ci.median()</code>, <code>ci.sd()</code>), and so on. In this book, we’ll use <code>mean</code>, <code>sd</code>, and <code>var</code>.</p>
<p>Another technique we will use is to generate random events from a probability distribution, particularly from the gaussian, exponential, and uniform distributions. For example, the following command will construct a data frame (that’s what <code>tibble()</code> is for) that has a column named <code>y</code> that contains 100 random draws from a gaussian distribution with mean of 3 and sd of 0.5. Then the arithmetic mean and <strong><em>standard deviation</em></strong> are computed from the numbers in the simulation.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="probability-and-evidence.html#cb29-1" aria-hidden="true" tabindex="-1"></a>Simulation <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb29-2"><a href="probability-and-evidence.html#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">mean=</span><span class="dv">3</span>, <span class="at">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb29-3"><a href="probability-and-evidence.html#cb29-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-4"><a href="probability-and-evidence.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Simulation)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 1
##       x
##   &lt;dbl&gt;
## 1  2.84
## 2  3.28
## 3  2.66
## 4  3.11
## 5  3.16
## 6  3.59</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="probability-and-evidence.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">df_stats</span>( <span class="sc">~</span> x, <span class="at">data=</span>Simulation, mean, sd)</span></code></pre></div>
<pre><code>##   response     mean        sd
## 1        x 2.981404 0.4670355</code></pre>
<p>The calculated mean and sd only approximate the values used in the simulation. That’s because they are based on random draws.</p>
</div>
<p>Generating random draws from a probability density and calculating arithmetic statistics on the result is one way of computing the mean and standard deviation of the probability density. It can also be done by integration. Although it’s fair to describe the integral as a calculation of the <em>average value</em>, the term used professionally is <strong><em>expectation value</em></strong>. The expectation value of a quantity <span class="math inline">\(f(x)\)</span> over a probability distribution is traditionally written in a distinct notation and is calculated as an integral of the quantity times the relevant probability density function.
<span class="math display">\[{\cal E}\left[\strut f(x)\right] \equiv \int_{-\infty}^{\infty} f(x)\, \text{pdf}(x)\, dx\ ,\]</span>
The mean is the expectation value of <span class="math inline">\(x\)</span>, so the mean of a quantity with a gaussian distribution is
<span class="math display">\[{\cal E}\left[\strut x\right] = \int_{-\infty}^{\infty} x \dnorm(x, \text{mean}, \text{sd})\, dx\ .\]</span>
This integral is exactly the same as the integral for the <strong><em>center of mass</em></strong> that was presented in Section <a href="mechanics.html#center-of-mass">31.4</a>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 30.3  </strong></span>To illustrate the definition of the mean in terms of integrals, here’s the mean of a gaussian distribution.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="probability-and-evidence.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="dv">3</span>, <span class="at">sd=</span><span class="fl">0.5</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="sc">-</span><span class="cn">Inf</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>You might have anticipated the result, since the particular gaussian over which we are taking the mean was defined to have a mean of 3.</p>
</div>
<p>Another important quantity to describe data or probability distributions is the <strong><em>variance</em></strong>, which is the average of the square distance from the mean. In math notation, this looks like
<span class="math display">\[{\cal E}\left[\strut (x - \text{mean})^2\right] = \int_{-\infty}^{\infty} \left(\strut x - \text{mean}\right)^2\, \dnorm(x, \text{mean}, \text{sd})\, dx\ .\]</span>
This integral is exactly the same as the integral for the <strong><em>moment of inertial</em></strong> that was presented in Section <a href="mechanics.html#center-of-mass">31.4</a>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 30.4  </strong></span>Compute the variance of a gaussian probability density.</p>
<p>To do this, we must first know the mean, then we can carry out the integration:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="probability-and-evidence.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="dv">-3</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="dv">3</span>, <span class="at">sd=</span><span class="fl">0.5</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="sc">-</span><span class="cn">Inf</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>Again, you might have anticipated this result, since the variance is the square of the standard deviation (<code>sd</code>) and we were using a particular gaussian distribution with sd equaling 0.5.</p>
</div>
<p>The examples above are intended to demonstrate how the mean and variance can be calculated from a probability density function. It’s often useful to try out a calculation in a setting where you already know the answer!</p>
<p>To illustrate the calculations in another setting, we will use an exponential probability function. Just as the R function <code>dnorm()</code> gives the density of the “normal”/gaussian distribution, the R function <code>dexp()</code> outputs the density of the exponential distribution. We used <span class="math inline">\(k\)</span> as the parameter in the exponential distribution. In R, the parameter is framed in terms of the <code>rate</code> at which events happen, that is, the expected number of events per unit time. For instance, the following integrals compute the mean and standard deviation of an exponential process where events happen on average twice per time unit.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="probability-and-evidence.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">2</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p>The result shouldn’t surprise you. If events are occurring on average twice per unit time, the average time between events should be 0.5 time units.</p>
<p>Here’s the variance of the same distribution</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="probability-and-evidence.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="fl">-0.5</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">2</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>It works out that for an exponential distribution with parameter <span class="math inline">\(k\)</span>, the mean is <span class="math inline">\(1/k\)</span> and the standard deviation (square root of the variance) is also <span class="math inline">\(1/k\)</span>.</p>
<p>Finally, let’s look at the mean and variance of a uniform distribution with, say, <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=10\)</span>. We can do this symbolically or numerically. For the mean:
<span class="math display">\[\int_{-\infty}^\infty x\  \text{unif}(x, 0, 10)\, dx = \int_0^{10} \frac{x}{10-0}\, dx = \left.{\Large\strut} \frac{x^2}{20}\right|_{x=0}^{10} \\= \frac{100}{20} - \frac{0}{20} = 5\]</span>
For the variance,
<span class="math display">\[\int_{-\infty}^\infty (x-5)^2\  \text{unif}(x, 0, 10)\, dx  
= \int_0^{10} \frac{(x-5)^2}{10-0}\, dx = 
\left.{\Large\strut}\frac{(x-5)^3}{30}\right|_{x=0}^{10}\\
=\frac{5^3}{30} - \frac{(-5)^3}{30} = \frac{125}{30} - \frac{-125}{30} = 8 \tiny{\frac{1}{3}}\]</span></p>
<p>Or, numerically<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="probability-and-evidence.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dunif</span>(x, <span class="dv">0</span>, <span class="dv">10</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 5.000001</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="probability-and-evidence.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="dv">-5</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dunif</span>(x, <span class="dv">0</span>, <span class="dv">10</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="sc">-</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 8.333336</code></pre>
</div>
<div id="risk-and-data" class="section level2" number="4.4">
<h2><span class="header-section-number">30.4</span> Risk and data</h2>
<p>In this section, we’ll examine the accepted technique for combining data with probability density functions in order to combine previous knowledge with new observations. The technique, called <strong><em>Bayesian inference</em></strong>, is used throughout science and engineering. (Surprisingly, satisfactory computer algorithms to do the calculations were invented only in the last 40 years; this is a modern topic.)</p>
<p>Up to this point we’ve worked with different kinds of mathematical entities involving probability.</p>
<ul>
<li>Relative density functions: The domain is a possible outcome from a random event, the output is a non-negative quantity. Examples: The jar contest entries.</li>
<li>Probability density functions. Much like relative density functions but with the added restriction that the definite integral over the domain produces the dimensionless number 1. Example: gaussian function.</li>
<li>Probability. A dimensionless number between 0 and 1.</li>
</ul>
<p>Another kind of mathematical entity that plays a central role in probability calculations—particularly those that involve data—is the <strong><em>likelihood function</em></strong>. In everyday speech, the word “likelihood” is a synonym for “probability.” But the technical word “likelihood,” in the nomenclature of the calculus of probability, means something utterly different from “relative density function” or “probability density function.”</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 30.5  </strong></span>It’s time to calculate who won the jar-of-coins contest!</p>
<p>When last we encountered the contest, we had translated the contest entries from their initial, unnormalized form as relative density functions into their properly normalized probability-density-function format as shown in Figure <a href="probability-and-evidence.html#fig:contest-compare">30.4</a>.</p>
<p>Now we are going to build a likelihood function that will tell us the winner.</p>
<p><strong>The starting point is data.</strong> The officials have opened the jar and carefully counted the money. There was $32.14 in the jar.</p>
<p>The domain of this likelihood function is the t.hree contestants, Johnny, Louisa, and Geoff. (Later, we’ll work with likelihood functions where the domain is a continuous quantity.) The output of the likelihood function is the probability density assigned to the observed $32.14. For ease of reference, Figure <a href="probability-and-evidence.html#fig:contest-compare">30.4</a> has been annotated with a faint vertical line marking the position of $32.14 in the domain. You can read off the value of the likelihood from that graph.</p>
<p>The convention in denoting a likelihood function with an input <span class="math inline">\(x\)</span> is <span class="math inline">\({\cal L}(x | D)\)</span>, where <span class="math inline">\(D\)</span> is the data. The vertical bar in <span class="math inline">\(x | D\)</span> is a subtle reminder to the experienced that the <span class="math inline">\(D\)</span> is not an input to the function but a <strong><em>given</em></strong> quantity: the observed data. In fact, the word “data” comes from the Latin for “given.” Once we have observed and recorded data, it is fixed: given. For this example, the observed, given amount is $32.14</p>
<p>The domain <span class="math inline">\(x\)</span> in the likelihood function for the contest is the set {Johnny, Louisa, Geoff}. We can present the likelihood in tabular form:</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\({\cal L}(\,x\ |\ \$32.14)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Johnny</td>
<td align="left">0.000 per dollar</td>
</tr>
<tr class="even">
<td align="left">Louisa</td>
<td align="left">0.010 per dollar</td>
</tr>
<tr class="odd">
<td align="left">Geoff</td>
<td align="left">0.066 per dollar</td>
</tr>
</tbody>
</table>
<p>The units, “per dollar,” are needed because the output of <span class="math inline">\({\cal L}(x | \$32.14)\)</span> is a probability density function. But the likelihood function itself is <strong>not</strong> a probability density function, it merely reports the level of the three different probability density functions entered by the three contestants. One way to see that <span class="math inline">\({\cal L}(x | \$32.14)\)</span> is not itself a probability density function is to sum the outputs over the entire domain: <span class="math inline">\(0.000 + 0.010 + 0.066 = 0.076\)</span> per dollar which, obviously, is not equal to dimensionless 1.</p>
<p>In statistics, likelihood functions are used to describe how to estimate a quantity given some data about the quantity. The techique is called <strong><em>maximum likelihood estimation</em></strong>: the estimate is the argmax of the likelihood function. For the coins-in-jar contest, the argmax is Geoff. Therefore, Geoff wins!</p>
<p>In the spirit of “Monday morning quarterbacking,” let’s look carefully at Johnny’s entry. If his bar-shaped probability density function were shifted just a little to the right, he would have won. This illustrates a flaw in Johnny’s logic in constructing his probability density function. The function indicates that he thought the probability of the amount being $23 was the same as being 30 dollars. In other words, he was uncertain to a considerable extent. But given this uncertainty, why would he insist that $30.01 is impossible (that is, has probability density 0 per dollar). Wouldn’t it make more sense to admit nonzero density for $30.01, and similarly for $30.02 and upward, with the density gradually decreasing with the amount of money. This is why, absent very specific knowledge about the circumstances, probability densities are so often framed as Gaussian distributions, as in Geoff’s entry.</p>
</div>
<p>We’ll illustrate the Bayesian inference process with an example about the <strong><em>risk of disease</em></strong>.</p>
<p>The risk of a disease is presented to the public in one of two ways. An <strong>absolute risk</strong> is a number like 2% per year, that is, 1 in 50 people will get the disease each year. A <strong>relative risk</strong> is a ratio: the risk of one population of interest (e.g. smokers) divided by the risk of another group (e.g. non-smokers). Since a relative risk is the ratio of two absolute risks, it is a dimensionless non-negative number, for instance 5.4.</p>
<p>For professionals, the risk of a disease is presented not as a single number but as a statistical <strong>range</strong> of numbers. For instance, an absolute risk might be stated as 1-3%. People who haven’t studied statistics have a hard time distinguishing between “1-3%” and “2%.” It amounts to much the same thing. To the professionals, “1-3%” conveys two different aspects of knowledge: i) the level of absolute risk—about 2% here—and 2) how much uncertainty there is in the knowledge of (i).</p>
<p>Saying “1-3%” seems a lot like Johnny’s entry in the coins-in-jar contest. The statement has edges that are too sharp. Professionals know how to interpret “1-3%” along the lines of Geoff’s entry: a summary of a smoother distribution. There is a more complete way to state our knowledge of the risk that combines both the level of risk and the uncertainty.</p>
<p>The representation we choose is a probability density function. To illustrate, Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a> shows three different examples of what such probability density functions might look like for risk. Each is called a <strong><em>prior on the risk</em></strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:beta-shapes"></span>
<img src="MOSAIC-Calculus_files/figure-html/beta-shapes-1.png" alt="Three different opinions about the risk of a disease." width="90%" />
<p class="caption">
Figure 30.8: Three different opinions about the risk of a disease.
</p>
</div>
<p>Panel (A) in Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a> is a strong statement that the risk of disease is small. Panel (B) says, “I have no idea!” Panel (C) expresses the belief that the risk is around 10%, but there is a great deal of uncertainty.</p>
<p>How to construct such statements when you have some evidence for the risk of the disease. For instance, evidence might be the results of screening tests on <span class="math inline">\(n = 10\)</span> randomly selected people.</p>
<p>Each screening test produces a positive result (we’ll write “+”) or a negative result (“-”). People seem to interpret such results as indicating that they have the disease (+) or don’t have the disease (-). But this is a wrong interpretation.</p>
<p>In order to make sense of a screening test, you need to know two probabilities. These are:</p>
<ol style="list-style-type: decimal">
<li>The probability of a + test in a group of people <em>with</em> the disease.</li>
<li>The probability of a - test in a group of people <em>without</em> the disease.</li>
</ol>
<p>We’ll call these two probabilities <span class="math inline">\(p_d(+)\)</span> and <span class="math inline">\(p_h(-)\)</span> where the subscript indicates whether we are referring to the probability in the disease group (<span class="math inline">\(p_d\)</span>) or in the no-disease (“healthy”) group (<span class="math inline">\(p_h\)</span>). For definiteness, we’ll imagine a test where <span class="math inline">\(p_d(+) = 80\%\)</span> and <span class="math inline">\(p_h(-) = 90\%\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>In a perfect test, <span class="math inline">\(p_d(+)\)</span> and <span class="math inline">\(p_h(-)\)</span> would both be 100%. Our imagined test, like real-world screening tests, is imperfect.</p>
<p>Given this information about the performance of the test, we can construct two likelihood functions. Each will be a function of the risk level <span class="math inline">\(R\)</span>. The two functions will differ in that one describes the likelihood of <span class="math inline">\(R\)</span> <em>given</em> a positive test; the other describes the likelihood of <span class="math inline">\(R\)</span> <em>given</em> a negative test.</p>
<p>Keep in mind that <span class="math inline">\(0 \leq R \leq 100\%\)</span> is the probability that the person being tested has the disease.</p>
<p><span class="math inline">\({\cal L}(R | +)\)</span> is the likelihood of a + test for any given level of <span class="math inline">\(R\)</span>. There are two cases to consider:</p>
<ol style="list-style-type: lower-alpha">
<li>You have the disease, in which case the probability of <span class="math inline">\(+\)</span> is <span class="math inline">\(p_d(+) = 0.8\)</span> for our example.</li>
<li>You do not have the disease, in which case the probability of <span class="math inline">\(+\)</span> is <span class="math inline">\(1-p_h(-) = 0.1\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></li>
</ol>
<p>Since you might or might not have the disease, we have to put (a) and (b) together based on the probability <span class="math inline">\(R\)</span> that you have the disease. Consequently, <span class="math display">\[{\cal L}(R | +) = p_d(+)\ R + (1-p_h(-))\ (1-R) = 0.8 R + 0.1 (1-R)\]</span> where <span class="math inline">\(1-R\)</span> is the probability that you don’t have the disease.</p>
<p>A similar calculation gives the likelihood of <span class="math inline">\(R\)</span> given a <span class="math inline">\(-\)</span> test:
<span class="math display">\[{\cal L}(R | -) = (1-p_d(+))\ R + p_h(-)\, (1-R)\ .\]</span>
In Bayesian inference, we will start with a probability density function expressing what we know (or, more precisely, “believe”) about the risk of disease. This is called our <strong><em>prior</em></strong> belief and it’s in the form of a probability density function. Then, as data comes in, we update our beliefs on the basis of the data. The prior can also be in the form of a relative density function, since we can normalize a relative density to become a probability density function.</p>
<p>After we have updated our <em>prior</em>, our state of knowledge is called a <strong><em>posterior</em></strong> belief. The posterior also has the form of a probability density function, but of course a relative density function will do just as well.</p>
<p>The formula for updating is called <strong><em>Bayes Rule</em></strong>: posterior is likelihood times prior.
<span class="math display">\[\text{posterior}(R) = {\cal L}(R | \text{data}) \times \text{prior}(R)\ .\]</span></p>
<p>Imagine that we have just heard about a newly discovered disease and have a screening test for it. Any of the three priors shown in Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a> could be a reasonable starting point. The prior shown in panel (B) is a state of complete ignorance. But it can be argued that “newly discovered” is a big hint about the risk and so a reasonable person might prefer to start with the priors in panels (A) or (C) instead. The calculations are simple, so we’ll trace them through for all three different priors, (A), (B), and (C).</p>
<p>Suppose our first observations are the results of screening tests on ten randomly selected individuals.</p>
<table>
<thead>
<tr class="header">
<th align="left">Subject ID</th>
<th align="left">Test outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">4349A</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="even">
<td align="left">7386A</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">6263E</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="even">
<td align="left">5912C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">7361C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">9384C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">6312A</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">3017C</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="odd">
<td align="left">1347B</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">9611D</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
</tbody>
</table>
<p>To summarize: Three <span class="math inline">\(+\)</span> tests out of ten.</p>
<p>After the first test outcome is available we can calculate the posterior: <span class="math display">\[\text{posterior}_1 (R | +) = {\cal L}(R | -) \times \text{prior(R)}\ .\]</span> After the second test outcome, the new posterior is <span class="math display">\[\text{posterior}_2 (R) = {\cal L}(R | -) \times {\cal L}(R | +) \times \text{prior(R)}\ .\]</span> And after the third (a <span class="math inline">\(+\)</span> result!) it will be <span class="math display">\[\text{posterior}_3 (R) = {\cal L}(R | +) \times {\cal L}(R | -) \times {\cal L}(R | +) \times \text{prior(R)}\ .\]</span>
We continue on in this way through all ten rows of the data to get the posterior distribution.</p>
<p>Figure <a href="probability-and-evidence.html#fig:after-10">30.10</a> shows the posterior after the 10 rows of data have been considered for each of the three priors from Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-1"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-1-1.png" alt="Posteriors (blue) after the first screening test, which was $+$, for each of the priors in Figure \@ref(fig:beta-shapes). The prior itself is drawn in gray." width="90%" />
<p class="caption">
Figure 30.9: Posteriors (blue) after the first screening test, which was <span class="math inline">\(+\)</span>, for each of the priors in Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a>. The prior itself is drawn in gray.
</p>
</div>
<p>With just one row of data considered, the priors depend very much on the particular posterior selected.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-10"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-10-1.png" alt="Posteriors (blue) after the first ten rows of data, for each of the priors in Figure \@ref(fig:beta-shapes). The prior itself is drawn in gray." width="90%" />
<p class="caption">
Figure 30.10: Posteriors (blue) after the first ten rows of data, for each of the priors in Figure <a href="probability-and-evidence.html#fig:beta-shapes">30.8</a>. The prior itself is drawn in gray.
</p>
</div>
<p>After the first 10 rows of data have been considered, the priors are similar despite the different priors.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-100"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-100-1.png" alt="Posteriors (blue) after 100 subjects have been screen, with 30 $+$ results." width="90%" />
<p class="caption">
Figure 30.11: Posteriors (blue) after 100 subjects have been screen, with 30 <span class="math inline">\(+\)</span> results.
</p>
</div>
<p>As data accumulates, the priors become irrelevant; the knowledge about the risk of disease is being driven almost entirely by the data.</p>
</div>
<div id="exercises-3" class="section level2" number="4.5">
<h2><span class="header-section-number">30.5</span> Exercises</h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>OK, so we’re assuming that the contestants know what a function is, what an input is, and what a graph is. So maybe it does take some calculus background to participate.<a href="probability-and-evidence.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Numerical integrals from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> of functions that are zero almost everywhere are challenging. The computer has to figure out where, out of the whole number line, the function has non-zero output. We’ve given the computer a head start by using 0 in the limits of integration. This would not be a problem for the exponential or gaussian distribution, which are non-zero everywhere (for the gaussian) or for half the number line (for the exponential).<a href="probability-and-evidence.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>In news reports, you may often hear about the <strong><em>accuracy</em></strong> of a screening test. Here, the accuracy would be between 80 and 90%. But “accuracy” is not a complete way to describe a test and can be very misleading. You need two numbers to describe the performance of a screening test.<a href="probability-and-evidence.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Make sure you’re comfortable with this statement. Here’s the justification: <span class="math inline">\(p_h(-)\)</span> is the probability that a person <strong>without</strong> the disease has a <span class="math inline">\(-\)</span> test. Since in our example the only alternative to a <span class="math inline">\(-\)</span> test outcome is a <span class="math inline">\(+\)</span> test outcome, the probability that a person without the disease has a <span class="math inline">\(+\)</span> test is <span class="math inline">\(1 - p_h(-)\)</span>.<a href="probability-and-evidence.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="opimization-and-constraint.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mechanics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/MOSAIC-Calculus/shared/B4-probability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/dtkaplan/MOSAIC-Calculus/shared/B4-probability.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
