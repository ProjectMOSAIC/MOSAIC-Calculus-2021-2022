<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 35 Probability and evidence | MOSAIC-Calculus.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 35 Probability and evidence | MOSAIC-Calculus.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 35 Probability and evidence | MOSAIC-Calculus.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="optimization-and-constraint.html"/>
<link rel="next" href="future-value.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<span class="math inline">
\(\newcommand{\line}{\text{line}}
\newcommand{\hump}{\text{hump}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\recip}{\text{recip}}
\newcommand{\diff}[1]{{\cal D}_#1}
\newcommand{\pnorm}{\text{pnorm}}
\newcommand{\dnorm}{\text{dnorm}}
\newcommand{\CC}[1]{\color{#648fff}{#1}}
\newcommand{\CE}[1]{\color{#785ef0}{#1}}
\newcommand{\CA}[1]{\color{#dc267f}{#1}}
\newcommand{\CB}[1]{\color{#fe6100}{#1}}
\newcommand{\CD}[1]{\color{#ffb000}{#1}}\)
</span>






<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="mosaic-calc-style-copy.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><span style="font-size: 150%;><strong><a href="../index.html">MOSAIC Calculus</a></strong></span></li>
<li><em><a>Calculus for the 21st century</a></em></li>
<li><a>Daniel Kaplan</a></li>
<hr>
<li><a href="../block-1/change.html">Block 1: Functions & quantities</a></li>
<li><a href="../block-2/change-relationships.html">Block 2: Differentiation</a></li>
<li><a href="../block-3/change-accumulation.html">Block 3: Accumulation</a></li>
<li><strong><a href="../block-4/block4-intro.html">Block 4: Manifestations</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="block4-intro.html"><a href="block4-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="32" data-path="operations.html"><a href="operations.html"><i class="fa fa-check"></i><b>32</b> Operations on functions</a>
<ul>
<li class="chapter" data-level="32.1" data-path="operations.html"><a href="operations.html#task-solve"><i class="fa fa-check"></i><b>32.1</b> Task: Solve</a></li>
<li class="chapter" data-level="32.2" data-path="operations.html"><a href="operations.html#task-argmax"><i class="fa fa-check"></i><b>32.2</b> Task: Argmax</a></li>
<li class="chapter" data-level="32.3" data-path="operations.html"><a href="operations.html#task-iterate"><i class="fa fa-check"></i><b>32.3</b> Task: Iterate</a></li>
<li class="chapter" data-level="32.4" data-path="operations.html"><a href="operations.html#software-for-the-tasks"><i class="fa fa-check"></i><b>32.4</b> Software for the tasks</a></li>
<li class="chapter" data-level="32.5" data-path="operations.html"><a href="operations.html#exercises"><i class="fa fa-check"></i><b>32.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>33</b> Data-driven functions</a>
<ul>
<li class="chapter" data-level="33.1" data-path="splines.html"><a href="splines.html#generating-smooth-motion"><i class="fa fa-check"></i><b>33.1</b> Generating smooth motion</a></li>
<li class="chapter" data-level="33.2" data-path="splines.html"><a href="splines.html#piecewise-but-smooth"><i class="fa fa-check"></i><b>33.2</b> Piecewise but smooth</a></li>
<li class="chapter" data-level="33.3" data-path="splines.html"><a href="splines.html#cubic-splines"><i class="fa fa-check"></i><b>33.3</b> C<sup>2</sup> smooth functions</a></li>
<li class="chapter" data-level="33.4" data-path="splines.html"><a href="splines.html#bézier-splines"><i class="fa fa-check"></i><b>33.4</b> Bézier splines</a></li>
<li class="chapter" data-level="33.5" data-path="splines.html"><a href="splines.html#exercises-1"><i class="fa fa-check"></i><b>33.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html"><i class="fa fa-check"></i><b>34</b> Optimization and constraint</a>
<ul>
<li class="chapter" data-level="34.1" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html#gradient-descent"><i class="fa fa-check"></i><b>34.1</b> Gradient descent</a></li>
<li class="chapter" data-level="34.2" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html#objectives-and-constraints"><i class="fa fa-check"></i><b>34.2</b> Objectives and Constraints</a></li>
<li class="chapter" data-level="34.3" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html#constraint-cost"><i class="fa fa-check"></i><b>34.3</b> Constraint cost</a></li>
<li class="chapter" data-level="34.4" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html#note-other-optimization-algorithms"><i class="fa fa-check"></i><b>34.4</b> Note: Other optimization algorithms</a></li>
<li class="chapter" data-level="34.5" data-path="optimization-and-constraint.html"><a href="optimization-and-constraint.html#exercises-2"><i class="fa fa-check"></i><b>34.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html"><i class="fa fa-check"></i><b>35</b> Probability and evidence</a>
<ul>
<li class="chapter" data-level="35.1" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#probability-density"><i class="fa fa-check"></i><b>35.1</b> Probability density</a></li>
<li class="chapter" data-level="35.2" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#three-density-functions"><i class="fa fa-check"></i><b>35.2</b> Three density functions</a></li>
<li class="chapter" data-level="35.3" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#expectation-value-mean-and-variance"><i class="fa fa-check"></i><b>35.3</b> Expectation value, mean and variance</a></li>
<li class="chapter" data-level="35.4" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#likelihood-and-data"><i class="fa fa-check"></i><b>35.4</b> Likelihood and data</a></li>
<li class="chapter" data-level="35.5" data-path="probability-and-evidence.html"><a href="probability-and-evidence.html#exercises-3"><i class="fa fa-check"></i><b>35.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="future-value.html"><a href="future-value.html"><i class="fa fa-check"></i><b>36</b> Present and future value</a>
<ul>
<li class="chapter" data-level="36.1" data-path="future-value.html"><a href="future-value.html#present-value-time"><i class="fa fa-check"></i><b>36.1</b> Present value (time)</a></li>
<li class="chapter" data-level="36.2" data-path="future-value.html"><a href="future-value.html#exercises-4"><i class="fa fa-check"></i><b>36.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>37</b> Mechanics</a>
<ul>
<li class="chapter" data-level="37.1" data-path="mechanics.html"><a href="mechanics.html#work"><i class="fa fa-check"></i><b>37.1</b> Work</a></li>
<li class="chapter" data-level="37.2" data-path="mechanics.html"><a href="mechanics.html#energy"><i class="fa fa-check"></i><b>37.2</b> Energy</a></li>
<li class="chapter" data-level="37.3" data-path="mechanics.html"><a href="mechanics.html#momentum"><i class="fa fa-check"></i><b>37.3</b> Momentum</a></li>
<li class="chapter" data-level="37.4" data-path="mechanics.html"><a href="mechanics.html#center-of-mass"><i class="fa fa-check"></i><b>37.4</b> Center of mass</a></li>
<li class="chapter" data-level="37.5" data-path="mechanics.html"><a href="mechanics.html#angular-momentum-and-torque"><i class="fa fa-check"></i><b>37.5</b> Angular momentum and torque</a></li>
<li class="chapter" data-level="37.6" data-path="mechanics.html"><a href="mechanics.html#exercises-5"><i class="fa fa-check"></i><b>37.6</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="../block-5/vectors.html">Block 5: Linear combinations</a></li>
<li><a href="../block-6/dynamics.html">Block 6: Dynamics</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-and-evidence" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 35</span> Probability and evidence</h1>
<p>We often deal with situations of <strong><em>uncertainty</em></strong>, situations where only partial predictions are possible. For instance, we can say whether a person may be at high risk for a disease, say, diabetes or lung cancer. But this doesn’t let us predict with certainty whether the person will get the disease. Instead, the term “high risk” indicates that we know something but not everything about the situation: not whether or not the person will get the disease but whether they are “likely” to have or to get it. Another example: a car might be said to be “unreliable.” We do not mean by this that the car cannot be used. Rather we’re thinking that from time to time the car might fail to start or run. A car where this happens once over a few year span is reliable, a car where this happens on a month-to-month basis is not reliable.</p>
<p>You may well have had some textbook exposure to <strong><em>probability</em></strong> as an intellectual field. Typical examples used to illustrate concepts and methods are coins being flipped, dice being tossed, and spinners spun. Colored balls are drawn from urns, slips of paper from hats, and so on. Each of these is a physical representation of an idealized mechanism where we feel sure we understand how likely each possible outcome is to happen.</p>
<p>In this chapter, we’ll use two basic <em>imagined</em> settings where uncertainty comes into play: the risk of disease before the disease is diagnosed and the safety of a self-driving car as it comes out of the factory. The word “imagined” signals that you should not draw conclusions about the facts of any particular disease or any particular self-driving car; we are merely using the imagined settings to lay out concepts and methods for the mathematical presentation and analysis of uncertainty and risk. Of particular importance will be the mathematical means by which we represent our knowledge or belief in these settings and the way we can properly update our knowledge/belief as new information becomes available.</p>
<div class="takenote">
<p>The calculus of probability and data introduces an additional convention for describing and naming functions. Throughout this book, the names have reflected the “shape” of the function—exponential, sinusoidal, sigmoidal, etc.—or the route by which the function was constructed, e.g. differentiation, anti-differentiation, inversion. Probability calculations involve not only the shapes of functions but also the properties mandated by the role each function plays in the calculation. An analogy is the assembly of an automobile out of different kinds of components: wheels, motors, body, and so on. You can’t put a wheel where the motor should go and produce a proper automobile. All motors play the same sort of role in the function of an automobile, but they can have different “shapes” such as gasoline, diesel, or electric.</p>
<p>To understand how cars are built, you have to be able easily to distinguish between the different kinds of components. This is second nature to you because you have so much experience with automobiles. Likewise, to understand the probability calculations, you’ll have to master the distinctions between the roles functions play in a calculation. In this chapter you’ll see <strong><em>probability density functions</em></strong> as well as <strong><em>likelihood functions</em></strong> and <strong><em>prior functions</em></strong> and <strong><em>posterior functions</em></strong> and some others. As you get started, you will confuse these roles for functions with one another, just as a newborn child can confuse “wheel” with “motor” until experience is gained.</p>
<p>Make sure to note the role-labels given to the functions you are about to encounter. We’ll start with probability density functions.</p>
</div>
<div id="probability-density" class="section level2" number="4.1">
<h2><span class="header-section-number">35.1</span> Probability density</h2>
<p>A probability, as you may know, is a dimensionless number between zero and one (inclusive). In this chapter, you’ll be dealing with functions relating to probabilities. The input to these functions will usually be a quantity that can have dimension, for instance, miles driven by a car. For some of the functions we will see in this chapter, the output will be a probability. For other functions in this chapter, the output will be a <strong><em>probability density</em></strong>.</p>
<p>Probability relates to the abstract notion of an <strong><em>event</em></strong>. An event is a process that produces an <strong><em>outcome</em></strong>. For instance:</p>
<ul>
<li>Flipping a coin is an event where the possible outcomes of H and T.</li>
<li>Taking a medical screening test is an event where the outcomes are “positive” or “negative.”</li>
<li>Throwing a dart at a bullseye is an event where the outcome is the distance of the impact point from the center of the bullseye.</li>
</ul>
<p>An event with a discrete outcome—coin flip, medical screening test—can be modeled by assigning a probability number to each of the possible outcomes. To be a valid probability model, each of those assigned numbers should be greater than or equal to zero. In addition, the sum of the assigned numbers across all the possible outcomes should be 1.</p>
<p>For events with a continuous outcome, such as the dart toss where the outcome is distance from the center, the probability model takes the form of a <strong><em>function</em></strong> whose domain is the possible outcomes. For the model to be a valid probability model, we require that the function output should never be less than zero. There’s another requirement as well: the integral of the function over the entire domain should be 1. For the dart-toss event, if we denote the distance from the bullseye as <span class="math inline">\(r\)</span> and the assigned number for the probability model as <span class="math inline">\(g(r)\)</span>, the integral requirement amounts to <span class="math display">\[\int_0^\infty g(r) dr = 1\ .\]</span></p>
<p>Note that the output <span class="math inline">\(g(r)\)</span> is <strong>not a probability</strong>, it is a <strong><em>probability density</em></strong>. To see why, let’s use the fundamental theorem of calculus to break up the integral into three segments:</p>
<ul>
<li>close to the bullseye: <span class="math inline">\(0 \leq r \leq a\)</span></li>
<li>far from the bullseye: <span class="math inline">\(b &lt; r\)</span></li>
<li>not close but not far: <span class="math inline">\(a &lt; r \leq b\)</span></li>
</ul>
<p>The total integral is <span class="math display">\[\int_0^\infty g(r) dr = 1\ = \int_0^a g(r) dr + \int_a^b g(r) dr + \int_b^\infty g(r) dr.\]</span>
The probability that the dart lands at a distance somewhere between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is <span class="math display">\[\int_a^b g(r) dr\ .\]</span>
Since <span class="math inline">\(r\)</span> is a distance, the dimension <span class="math inline">\([r] =\ \)</span>L. Suppose the units of <span class="math inline">\(r\)</span> are centimeters. We need <span class="math inline">\(\int g(r) dr\)</span> to be a dimensionless number. Since the dimension of the integral is <span class="math inline">\([r] \cdot [g(r)] = [1]\)</span>, it must be that <span class="math inline">\([g(r)] = [1/r] = \text{L}^{-1}\)</span>. Thus, <span class="math inline">\(g(r)\)</span> is not a probability simply because it is not dimensionless. Instead, in the dart example, it is a “probability-per-centimeter.” This kind of quantity—probability <em>per something</em>—is called a <strong><em>probability density</em></strong> and <span class="math inline">\(g(r)\)</span> itself is a <strong><em>probability density function</em></strong>.</p>
To show the aptness of the word “density,” let’s switch to a graphic of a function that uses literal density of ink as the indicator of the function value. Figure <a href="probability-and-evidence.html#fig:dart-g">35.1</a> shows what the dart toss’s <span class="math inline">\(g(r)\)</span> probability density function might look like:
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dart-g"></span>
<img src="MOSAIC-Calculus_files/figure-html/dart-g-1.png" alt="Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink." width="90%" />
<p class="caption">
Figure 35.1: Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink.
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 35.1  </strong></span>Consider a simple competition of the sort you might encounter at a fund-raising fair. There is a jar on display, filled with coins that have been donated by one of the fair’s sponsors. You pay $1 (which goes to a good cause) to enter the contest. Your play is to describe how much money is in the jar, writing your description down along with your name on an entry form. At the end of the day, an official will open the jar, count the money, and announce who made the best estimate. The winner gets the money in the jar.</p>
<p><img src="www/jar-coins.png" width="25%" style="display: block; margin: auto;" /></p>
<p>In the usual way these contests are run, the contestants each write down a guess for the amount they think is in the jar, say $18.63. The winner is determined by seeing whose guess was closest to the actual value of the coins in the jar.</p>
<p>In reality, hardly anyone believes they can estimate the amount in the jar to the nearest penny. The person guessing $18.63 might prefer to be able to say, “between 18 and 19 dollars.” Or, maybe “$18 <span class="math inline">\(\pm\)</span> 3.” To communicate what you know about the situation, it’s best to express a range of possibilities that you think likely.</p>
<p>In our more mathematical contest, we ask the participants to specify a function that describes their beliefs about the money in the jar. The instructions state, “On the graph-paper axes below, sketch a continuous function expressing your best belief about how much money is in the jar. The only requirement is that the function value must be zero or greater for all inputs.”</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coin-graph-paper"></span>
<img src="MOSAIC-Calculus_files/figure-html/coin-graph-paper-1.png" alt="The entry form for the money-in-the-jar contest." width="90%" />
<p class="caption">
Figure 35.2: The entry form for the money-in-the-jar contest.
</p>
</div>
<p>Take a minute to look at the picture of the jar and draw your function on the axes shown above. Think about why the contest form appropriately doesn’t ask you to scale the vertical axis.</p>
<p>Here are contest entries from three competitors.</p>
<pre><code>## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will
## replace the existing scale.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contest-entries"></span>
<img src="MOSAIC-Calculus_files/figure-html/contest-entries-1.png" alt="Three contestants' contest entries." width="90%" />
<p class="caption">
Figure 35.3: Three contestants’ contest entries.
</p>
</div>
</div>
<p>The functions called for by the contest instructions are <strong><em>relative density</em></strong> functions. The “relative” means that the function clearly indicates where the probability is more or less dense, but the function has not yet been scaled to be a probability density function. Suppose <span class="math inline">\(h(x)\)</span> is a relative density function such that
<span class="math display">\[\int_{-\infty}^\infty h(x)\, dx = A \neq 1\ .\]</span> Although <span class="math inline">\(h(x)\)</span> is not a probability density function, the very closely related function <span class="math inline">\(\frac{1}{A} h(x)\)</span> will be a probability density function. We’ll use the term <strong><em>normalizing</em></strong> to refer to the simple process of turning a relative density function into a probability density function.</p>
<p>A relative density function is entirely adequate for describing the distribution of probability. However, when comparing two or more probability distributions, it’s important that they all be on the same scale. Normalizing the relative density functions to probability density functions accomplishes this. Figure <a href="probability-and-evidence.html#fig:contest-compare">35.4</a> compares the three relative probability functions in Figure <a href="probability-and-evidence.html#fig:contest-entries">35.3</a>. Johnny makes the density large over a narrow domain and zero elsewhere, while Louisa specifies a small density over a large domain. All three competitors’ functions have an area-under-the-curve of dimensionless 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contest-compare"></span>
<img src="MOSAIC-Calculus_files/figure-html/contest-compare-1.png" alt="Comparing the contest entries by normalizing each of them to a probability density function." width="90%" />
<p class="caption">
Figure 35.4: Comparing the contest entries by normalizing each of them to a probability density function.
</p>
</div>
</div>
<div id="three-density-functions" class="section level2" number="4.2">
<h2><span class="header-section-number">35.2</span> Three density functions</h2>
<p>Three commonly used families of probability density functions are:</p>
<ul>
<li>the gaussian density function</li>
<li>the exponential density function</li>
<li>the uniform density function.</li>
</ul>
<p>Figure <a href="probability-and-evidence.html#fig:three-density-funs">35.5</a> shows their shapes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:three-density-funs"></span>
<img src="MOSAIC-Calculus_files/figure-html/three-density-funs-1.png" alt="Three probability density functions that are often used in applied work." width="60%" /><img src="MOSAIC-Calculus_files/figure-html/three-density-funs-2.png" alt="Three probability density functions that are often used in applied work." width="60%" /><img src="MOSAIC-Calculus_files/figure-html/three-density-funs-3.png" alt="Three probability density functions that are often used in applied work." width="60%" />
<p class="caption">
Figure 35.5: Three probability density functions that are often used in applied work.
</p>
</div>
<p>The <strong><em>uniform density function</em></strong>, <span class="math inline">\(u(x, a, b)\)</span> is more or less the equivalent of the constant function. The family has two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with the function defined as:
<span class="math display">\[\text{unif}(x, a, b) \equiv \left\{\begin{array}{cl}\frac{1}{b-a} &amp; \text{for}\ a \leq x \leq b\\0&amp; \text{otherwise} \end{array}\right.\]</span>
This function is used to express the idea of “equally likely to be any value in the range <span class="math inline">\([a, b]\)</span>.” For instance, to describe a probability that a January event is equally likely to occur at any point in the month, you can use <span class="math inline">\(u(x, 0, 31)\)</span> where <span class="math inline">\(x\)</span> and the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> have dimension T and are in units of days. Notice that the density itself has dimension T<sup>-1</sup> and units “per day.”</p>
<p>The <strong><em>gaussian density function</em></strong>, <span class="math inline">\(\dnorm(x, \text{mean}, \text{sd})\)</span> is familiar to you from previous blocks in this book: the bell-shaped function. It’s known also as the <strong><em>normal distribution</em></strong> because it is so frequently encountered in practice. It is a way of expressing, “The outcome of the event will likely be close to this particular value.” The parameter named <strong>mean</strong> specifies “this particular value.” The parameter <strong>sd</strong> specifies what’s mean by “close.” The gaussian density function is smooth. It is never zero, but <span class="math inline">\(\lim_{x \rightarrow \pm \infty} \dnorm(x, \text{mean}, \text{sd}) = 0\)</span>.</p>
<p>To use an analogy between physical density (e.g., kg per cubic-meter), where density times size gives mass, we can say that the total mass of a probability density function is always 1. For the gaussian density, 68% of of the total mass is within <span class="math inline">\(\pm 1\)</span>sd of the mean, 95% is within <span class="math inline">\(\pm 2\)</span>sd of the mean, 99.7% within <span class="math inline">\(\pm 3\)</span>sd, and 99.99% within <span class="math inline">\(\pm 4\)</span>sd.</p>
<p>The <strong><em>exponential probability density</em></strong> is shaped just like an exponential function <span class="math inline">\(e^{-kx}\)</span>. It’s used to describe events that are equally likely to happen in any <em>interval</em> of the input variable, and describes the relative probability that the first event to occur will be at <span class="math inline">\(x\)</span>.</p>
</div>
<div id="expectation-value-mean-and-variance" class="section level2" number="4.3">
<h2><span class="header-section-number">35.3</span> Expectation value, mean and variance</h2>
<p>Probability theory was originally motivated by problems in gambling, specifically, figuring out what casino games are worth betting on. A feature of casino games—roulette, slot machines, blackjack, Texas hold’em, etc.—is that they are played over and over again. In any one round of play, you might win or you might lose, that is, your “earnings” might be positive or they might be negative. Over many plays, however, the wins and loses tend to cancel out. One way to summarize the game itself, as opposed to the outcome of any single play, is by the average earnings per play. This is called the <strong><em>expected value</em></strong> of the game.</p>
<p>This logic is often applied to summarizing a probability density function. If <span class="math inline">\(x\)</span> is the outcome of the random event described by a probability density <span class="math inline">\(f(x)\)</span>, the <strong><em>expected value</em></strong> of the probability density is defined as
<span class="math display">\[\mathbb{E}\!\left[{\strut} x\right] \equiv \int_{-\infty}^\infty x\, f(x) \, dx\ .\]</span>
In Section <a href="mechanics.html#center-of-mass">37.4</a>, we’ll see this same form of integral for computing the <strong><em>center of mass</em></strong> of an object.</p>
<div class="why">
<p>Why are you using square braces <span class="math inline">\(\left[\strut\ \ \right]\)</span> rather than parentheses <span class="math inline">\(\left(\strut \ \  \right)\)</span>.</p>
<p>We always used parentheses to indicate that the enclosed quantity is the input to a function. But <span class="math inline">\(\mathbb{E}\!\left[{\strut} x\right]\)</span> is not a function, let alone a function of <span class="math inline">\(x\)</span>. Instead, <span class="math inline">\(\mathbb{E}\!\left[{\strut} x\right]\)</span> is a numerical summary of a probability density function <span class="math inline">\(f(x)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 35.2  </strong></span>Find the expected value of the gaussian probability density <span class="math inline">\(\dnorm(x, \text{mean}=6.3, \text{sd}= 17.5)\)</span>. Using the R/mosaic <code>Integrate()</code> function, we have</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="probability-and-evidence.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="fl">6.3</span>, <span class="fl">17.5</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="sc">-</span><span class="cn">Inf</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 6.3</code></pre>
<p>The expected value of a gaussian is exactly the same as the parameter called mean which describes the argmax of the gaussian.</p>
</div>
<p>Another important quantity to describe data or probability distributions is the <strong><em>variance</em></strong>, which is the average of the square distance from the mean. In math notation, this looks like
<span class="math display">\[\mathbb{E}\!\left[{\large\strut} (x - \mathbb{E}[x])^2\right] = \int_{-\infty}^{\infty} \left(\strut x - \text{mean}\right)^2\, \dnorm(x, \text{mean}, \text{sd})\, dx\ .\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 35.3  </strong></span>Compute the variance of a gaussian probability density <span class="math inline">\(\dnorm(x, \text{mean}=6.3, \text{sd}= 17.5)\)</span>.</p>
<p>To do this, we must first know the mean, then we can carry out the integration.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="probability-and-evidence.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="fl">-6.3</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dnorm</span>(x, <span class="at">mean=</span><span class="fl">6.3</span>, <span class="at">sd=</span><span class="fl">17.5</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="sc">-</span><span class="cn">Inf</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 306.25</code></pre>
<p>Again, you might have anticipated this result, since the variance is the square of the standard deviation (<code>sd</code>) and we were using a particular gaussian distribution with sd equaling 17.5. Of course, <span class="math inline">\(17.5^2 = 306.25\)</span>.</p>
</div>
<p>To illustrate the calculations in another setting, we will use an exponential probability function. Just as the R function <code>dnorm()</code> gives the density of the “normal”/gaussian distribution, the R function <code>dexp()</code> outputs the density of the exponential distribution. We used <span class="math inline">\(k\)</span> as the parameter in the exponential distribution. In R, the parameter is framed in terms of the <code>rate</code> at which events happen, that is, the expected number of events per unit time. For instance, the following integrals compute the mean and standard deviation of an exponential process where events happen on average twice per time unit.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="probability-and-evidence.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">2</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<p>The result shouldn’t surprise you. If events are occurring on average twice per unit time, the average time between events should be 0.5 time units.</p>
<p>Here’s the variance of the same distribution</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="probability-and-evidence.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="fl">-0.5</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dexp</span>(x, <span class="at">rate=</span><span class="dv">2</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>It works out that for an exponential distribution with parameter <span class="math inline">\(k\)</span>, the mean is <span class="math inline">\(1/k\)</span> and the standard deviation (square root of the variance) is also <span class="math inline">\(1/k\)</span>.</p>
<p>Finally, let’s look at the mean and variance of a uniform distribution with, say, <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=10\)</span>. We can do this symbolically or numerically. For the mean:
<span class="math display">\[\int_{-\infty}^\infty x\  \text{unif}(x, 0, 10)\, dx = \int_0^{10} \frac{x}{10-0}\, dx = \left.{\Large\strut} \frac{x^2}{20}\right|_{x=0}^{10} \\= \frac{100}{20} - \frac{0}{20} = 5\]</span>
For the variance,
<span class="math display">\[\int_{-\infty}^\infty (x-5)^2\  \text{unif}(x, 0, 10)\, dx  
= \int_0^{10} \frac{(x-5)^2}{10-0}\, dx = 
\left.{\Large\strut}\frac{(x-5)^3}{30}\right|_{x=0}^{10}\\
=\frac{5^3}{30} - \frac{(-5)^3}{30} = \frac{125}{30} - \frac{-125}{30} = 8 \tiny{\frac{1}{3}}\]</span></p>
<p>Or, numerically<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="probability-and-evidence.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>(x <span class="sc">*</span> <span class="fu">dunif</span>(x, <span class="dv">0</span>, <span class="dv">10</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 5.000001</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="probability-and-evidence.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Integrate</span>((x<span class="dv">-5</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">dunif</span>(x, <span class="dv">0</span>, <span class="dv">10</span>) <span class="sc">~</span> x, <span class="fu">domain</span>(<span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="cn">Inf</span>))</span></code></pre></div>
<pre><code>## [1] 8.333336</code></pre>
</div>
<div id="likelihood-and-data" class="section level2" number="4.4">
<h2><span class="header-section-number">35.4</span> Likelihood and data</h2>
<p>In this section, we’ll examine the accepted technique for combining data with probability density functions in order to combine previous knowledge with new observations. The technique, called <strong><em>Bayesian inference</em></strong>, is used throughout science and engineering.</p>
<p>Recall that a <strong><em>relative density function</em></strong> is a format to describe the relatively likeliness of possible outcomes from a random event. The domain for a relative density function is the complete set of possible outcomes from the event. An example: The distance of a dart’s impact from the bullseye.</p>
<p>The output of a relative density function is a non-negative number. For an expert dart thrower, the relative density will be high for small distances and low for large distances. This is just a way of quantifying that the expert’s is likely to hit close to the bullseye.</p>
<p>In comparing two relative density functions, for instance the function for an expert dart thrower versus that for an amateur, it’s helpful to <strong><em>normalize</em></strong> them so that the integral of the relative density over the entire domain is dimensionless 1. The normalized version of a relative density function is called a <strong><em>probability density functions</em></strong>. Note that the probability density function contains the same information as the relative density function.</p>
<p>In this section, we introduce a new type of function that’s important in probability calculations involving data. This new type of function is, perhaps confusingly, called a <strong><em>likelihood function</em></strong>.</p>
<p>Likelihood functions always involve <strong><em>hypothetical reasoning</em></strong>. The idea is to construct a model world whose characteristics are exactly known. In that world, we can imagine constructing a function that gives the probability or probability density of any possible value of a measurement.</p>
<p>For instance, Johnny, Louisa, and Geoff each created hypothetical worlds that describe the amount of money in the jar. For each contestant, their personal hypothesis states a probability density over all the theoretically possible amounts of money in the jar.</p>
<p>The domain of a likelihood function is all the competing hypotheses. Take a moment to digest that. The domain of money-in-jar likelihood function is <strong>not</strong> the amount of money in the jar, it is instead the three hypotheses: Johnny’s, Louisa’s, and Geoff’s.</p>
<p>It’s conventional to denote name a likelihood function <span class="math inline">\({\cal L}()\)</span>. For the competition, a likelihood function will be <span class="math inline">\({\cal L}(\text{contestant})\)</span>, where <span class="math inline">\(\text{contestant}\)</span> will be one of “Johnny” or “Louisa” or “Geoff” in our example.</p>
<p>There are many likelihood functions that might be relevant to the money-in-jar situation. There is one likelihood function for each possible amount of money in the jar. For instance, <span class="math inline">\({\cal L}_{\$10}(\text{contestant})\)</span> is relevant if there were ten dollars in the jar. Another likelihood function <span class="math inline">\({\cal L}_{\$11.50}(\text{contestant})\)</span> would be relevant if there were eleven dollars and fifty cents in the jar.</p>
<p>This notation of naming functions using a subscript can get awkward when there are a huge number of functions. For instance, for the money-in-jar contest there will be a likelihood function for $0.01, $0.02, $0.03, and all other possibilities such as $21.83 or <span class="math inline">\(47.06\)</span>. If we want to be able to refer to the whole set of likelihood functions, better to replace the dollar amount in the subscript with a symbol, say <span class="math inline">\(m\)</span> for money. Then the whole set of likelihood functions potentially relevant to the contest would be written <span class="math inline">\({\cal L}_m(\text{contestant})\)</span>.</p>
<div class="takenote">
<p>There is another style for notation that you may encounter in your future work. In the alternative style, for example, instead of <span class="math inline">\({\cal L}_m(\text{contestant})\)</span> the likelihood function would be written <span class="math inline">\({\cal L}(\text{contestant}\, {\mathbf |} m )\)</span>. The vertical bar is pronounced “given” and is part of a notational system often used in probability calculations.</p>
</div>
<p>Since the output of any likelihood function is a probability or a probability density depending on context, we know that the output will be a non-negative quantity.</p>
<p>Likelihood functions provide the link between data and hypotheses. The idea is that when data become available, it’s possible to choose the relevant likelihood function.</p>
<p>To illustrate, let’s return to the jar-of-money contest and the three competitors’ entries as shown in Figure <a href="probability-and-evidence.html#fig:contest-compare">35.4</a>. For convenience, that Figure is reproduced here:</p>
<div class="figure" style="text-align: center">
<img src="MOSAIC-Calculus_files/figure-html/jar_functions2-1.png" alt="The contest entries shown in Figure \@ref(fig:contest-compare)." width="90%" />
<p class="caption">
(#fig:jar_functions2)The contest entries shown in Figure <a href="probability-and-evidence.html#fig:contest-compare">35.4</a>.
</p>
</div>
<p>The functions shown in the Figure are <strong>not</strong> likelihood functions. But we can use them to construct whatever likelihood function turns out to be relevant in the money-in-jar contest.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 35.4  </strong></span>It’s time to calculate who won the jar-of-coins contest! That is, we’ll calculate whose entry is best. The word “best” should remind you of optimization and indeed the winner of the contest will be the argmax of the relevant likelihood function. At this point, remember that the likelihood functions are <span class="math inline">\({\cal L}_m(\text{contestant})\)</span>, so the argmax will be one of the contestants!</p>
<p>First, we need to pick the relevant likelihood function. Common sense tells us that you can only pick a winner when the jar has been opened and the money counted. That is, we need some data.</p>
<p>Here’s the data: The officials have opened the jar and carefully counted the money. There was $32.14 in the jar. This tells us that the relevant likelihood function is <span class="math inline">\({\cal L}_{\$32.14}(\text{contestant})\)</span>.</p>
<p>The output of <span class="math inline">\({\cal L}_{\$32.14}(\text{contestant})\)</span> is the probability density assigned by the contestant to the observed value $32.14. You can read this from Figure <a href="#fig:jar-functions2"><strong>??</strong></a>. For your convenience, the observation <span class="math inline">\(32.14\)</span> has been annotated with a faint brown vertical line.</p>
<p>Here’s a tabular version of <span class="math inline">\({\cal L}_{\$32.14}(\text{contestant})\)</span>.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\text{contestant}\)</span></th>
<th align="left"><span class="math inline">\({\cal L}_{\$32.14}(\text{contestant})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Johnny</td>
<td align="left">0.000 per dollar</td>
</tr>
<tr class="even">
<td align="left">Louisa</td>
<td align="left">0.010 per dollar</td>
</tr>
<tr class="odd">
<td align="left">Geoff</td>
<td align="left">0.066 per dollar</td>
</tr>
</tbody>
</table>
<p>In statistics, likelihood functions are used to describe how to estimate a quantity given some data about the quantity. The techique is called <strong><em>maximum likelihood estimation</em></strong>: the estimate is the argmax of the likelihood function. For the coins-in-jar contest, the argmax is Geoff. Therefore, Geoff wins!</p>
<p>In the spirit of “Monday morning quarterbacking,” let’s look carefully at Johnny’s entry. If his bar-shaped probability density function were shifted just a little to the right, he would have won. This illustrates a weakness in Johnny’s logic in constructing his probability density function. The function indicates that he thought the probability of the amount being $23 was the same as being 30 dollars. In other words, he was uncertain to a considerable extent. But given this uncertainty, why would he insist that $30.01 is impossible (that is, has probability density 0 per dollar). Wouldn’t it make more sense to admit nonzero density for $30.01, and similarly for $30.02 and upward, with the density gradually decreasing with the amount of money. This is why, absent very specific knowledge about the circumstances, probability densities are so often framed as Gaussian distributions, as in Geoff’s entry.</p>
</div>
<p>The previous example is intended to give you an idea about what a likelihood function is. In that example, we use the calculus operator <strong>argmax</strong> to find the contest winner.</p>
<p>Let’s turn now to another important use of likelihood functions: their role in the Bayesian inference process. The example concerns figuring out the <strong><em>risk of disease transmission</em></strong>.</p>
<div class="intheworld">
<p>Consider the situation in November 2019 at the start of the COVID-19 pandemic. At that time, there was almost no information about the illness or how it spreads. In the US and many other countries, most people assumed that the spread of illness outside its origin in Wuhan, China, would be prevented by standard public health measures such as testing, contact tracing, quarantine, and restrictions on international travel. Intuitively, most people translated this assumption into a sense that the personal risk of illness was small.</p>
<p>In communicating with the public about risk, it’s common to present risk as a number: a probability. This is an adequate presentation only when we have a solid idea of the risk. To form a solid idea, we need evidence.</p>
<p>Before there’s enough evidence responsibly to form a solid idea, it’s best to present risk not as a probability but as a probability density function. To illustrate, Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a> shows three different examples of what such probability density functions might look like for vague, preliminary ideas of risk.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:beta-shapes"></span>
<img src="MOSAIC-Calculus_files/figure-html/beta-shapes-1.png" alt="Three different opinions about the risk of a disease." width="90%" />
<p class="caption">
Figure 35.6: Three different opinions about the risk of a disease.
</p>
</div>
<p>Panel (A) in Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a> is a strong statement that the risk is believed to be small. Even so, the density function is non-zero even for values of the risk near 100%. This is an honest admission that, as with COVID-19, something that we don’t know might be going on. In the case of COVID-19, what most people didn’t realize is 1) that the reported numbers were completely unrepresentative of the extent of spread, since most cases are asymptomatic and 2) that the illness can spread even by those who are asymptomatic. Epidemiologists and other public health workers knew enough from previous experience to be aware of their lack of knowledge about (1) and (2), but the rest of us, including many policy makers, didn’t even know what they didn’t know. The word “unk-unk” is sometimes used by engineers to refer to such an “unknown unknown”.</p>
<p>Panel (B) says, “I have no idea!” This can often be an honest, useful appraisal of the situation. But experts who are honest in this way are often regarded by the public and policy makers as lacking credibility.</p>
<p>Panel (C) expresses the belief that the risk might well be small but also might be large.</p>
<p>Any of the three probability density functions would be reasonable statements about what we knew and didn’t know about COVID-19 at the very beginning of the pandemic, before there was much data. Such statements are called <strong><em>priors</em></strong>; summaries of what we know up to the present.</p>
<p>In Bayesian inference, as data become available we can revise or <strong><em>update</em></strong> the priors, giving a better informed description of the risk.</p>
<p>For COVID-19, data eventually came in many different forms: estimates of incubation periods, testing to determine what fraction of cases are asymptomatic, and so on.</p>
<p>For our presentation of Bayesian reasoning, we’ll consider a simplified situation where data come in only one form: screening tests for the illness. Imagine that you are conducting a contact-tracing study. Whenever a patient presents with COVID-19 symptoms and has a positive PCR test, that patient’s close contacts are given a screening test for COVID. The objective is to estimate how transmissible the virus is by figuring out what proportion of close contacts become infected.</p>
<p>We can’t know which of the three priors in Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a> is most appropriate. After all, until rich enough data become available, each prior is just an opinion. So we’ll repeat the update-with-data analysis for each of the three priors. If, in the end, the results from the three priors substantially agree, then we can conclude that the data is shaping the results, rather than the prior.</p>
<p>The unknown here is the risk <span class="math inline">\(R\)</span> of transmission. We’ll denote the three priors as <span class="math inline">\(\text{prior}_A (R)\)</span>, <span class="math inline">\(\text{prior}_B (R)\)</span>, and <span class="math inline">\(\text{prior}_C (R)\)</span>. But, in general, we’ll write <span class="math inline">\(\text{prior}(R)\)</span> to stand for any of those three specific priors.</p>
</div>
<p>In Bayesian inference, the <strong><em>prior</em></strong> represents the starting point for what we know (or, more precisely, “believe”) about the risk of transmission. It has the form of a relative density function. As data come in, we update our prior beliefs on the basis of the data.</p>
<p>After we have updated our <em>prior</em>, our state of knowledge is called a <strong><em>posterior</em></strong> belief. Think of the prior as “pre-data” belief and the posterior as “post-data” belief. The posterior also has the form of a relative density function.</p>
<p>The formula for updating is called <strong><em>Bayes Rule</em></strong>: posterior is likelihood times prior.
<span class="math display">\[\text{posterior}(R) = {\cal L}_\text{data}(R) \times \text{prior}(R)\ .\]</span>
Recall that the output of a likelihood function is a non-negative quantity. Since the prior is a relative density function, it too is non-negative for all <span class="math inline">\(R\)</span>. Therefore the posterior will have a non-negative output and be a valid relative density function.</p>
<div class="takenote">
<p>Most texts prefer to define priors and posteriors as probability density functions rather than relative density functions. The only difference, of course, is the normalization. But that can be performed at any time, so to streamline the updating process, we’ll let posteriors and priors be relative density functions.</p>
</div>
<p>Notice that the posterior has just one input, the parameter <span class="math inline">\(R\)</span>. That’s because the <span class="math inline">\(\text{data}\)</span> is fixed by our observations: the posterior only makes sense once we have the data available in order to choose the relevant likelihood function.</p>
<p>Our task now is to construct the appropriate likelihood function that reflects how the screening test works. To outline the process, let’s consider a group of 1000 people who are taking the screening test. If we knew the parameter <span class="math inline">\(R\)</span>, we could split those 1000 people into two groups: one group with the illness and one group without.</p>
<ul>
<li>Whole group of 1000, made up of
<ul>
<li>1000 <span class="math inline">\(R\)</span> with the illness</li>
<li>1000 <span class="math inline">\((1-R)\)</span> without the illness</li>
</ul></li>
</ul>
<p>For instance, if <span class="math inline">\(R=0.2\)</span>, then out of the whole group of 1000 people, 200 would have the illness and 800 would not.</p>
<p>After taking the screening test, each person will have either a positive test result (we’ll write this “+”) or a negative test result (we’ll write “-”).</p>
<p>In order to make sense of a screening test, you need to know two probabilities. These are:</p>
<ol style="list-style-type: decimal">
<li>The probability of a + test in a group of people <em>with</em> the disease. We’ll call this <span class="math inline">\(p_d(+)\)</span>.</li>
<li>The probability of a - test in a group of people <em>without</em> the disease. We’ll call this <span class="math inline">\(p_h(-)\)</span>.</li>
</ol>
<p>Note that the subscript indicates whether we are referring to the probability in the has-the-illness group (<span class="math inline">\(p_d\)</span>) or in the no-illness (“healthy”) group (<span class="math inline">\(p_h\)</span>).</p>
<p>You may know that the result of a screening test is <strong>not definitive</strong>. That is, a person with a <span class="math inline">\(+\)</span> result may not actually have the illness. Likewise, a <span class="math inline">\(-\)</span> result is no guarantee that the person does not have the illness. The word “screening” is meant to emphasize the imperfections of such tests. But often the imperfect test is the best we have available.</p>
<p>After the screening test has been taken by the 1000 people in our example group, we can divide them further</p>
<ul>
<li>Whole group of 1000, made up of
<ul>
<li>1000 <span class="math inline">\(R\)</span> with the illness, made up of
<ul>
<li>1000 <span class="math inline">\(R\ p_d(+)\)</span> who had a correct positive test result</li>
<li>1000 <span class="math inline">\(R\ (1-p_d(+))\)</span> who had a negative result despite having the illness</li>
</ul></li>
<li>1000 <span class="math inline">\((1-R)\)</span> without the illness, made up of
<ul>
<li>1000 <span class="math inline">\((1-R)\ (1-p_h(-))\)</span> who had a positive test result despite being healthy</li>
<li>1000 <span class="math inline">\((1-R)\ p_h(-)\)</span> who had a correct negative result.</li>
</ul></li>
</ul></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 35.5  </strong></span>Suppose that <span class="math inline">\(R=0.2\)</span>, <span class="math inline">\(p_d(+) = 80\%\)</span>, and <span class="math inline">\(p_h(-) = 70\%\)</span>. Then the division would be:</p>
<ul>
<li>Whole group of 1000, made up of
<ul>
<li>200 with the illness, of whom
<ul>
<li>160 who got a correct <span class="math inline">\(+\)</span> result</li>
<li>40 who got a <span class="math inline">\(-\)</span> result, despite having the illness</li>
</ul></li>
<li>800 without the illness, of whom
<ul>
<li>240 who got a <span class="math inline">\(+\)</span> result, despite not having the illness</li>
<li>560 who got a correct <span class="math inline">\(-\)</span> result.</li>
</ul></li>
</ul></li>
</ul>
<p>There are two likelihood functions reflecting the two different possible test results: <span class="math inline">\({\cal L}_+ (R)\)</span> and <span class="math inline">\({\cal L}_- (R)\)</span>. We will need to construct both of these functions since the test result is going to be <span class="math inline">\(+\)</span> for some people and <span class="math inline">\(-\)</span> for others.</p>
<p>Recall now that each likelihood function is based on a <strong><em>hypothesis</em></strong> about the world. In this case, the hypothesis is a particular value for <span class="math inline">\(R\)</span>. Let’s look at the situation for the hypothesis that <span class="math inline">\(R=0.2\)</span>. We can figure out the values of both <span class="math inline">\({\cal L}_+ (R=0.2)\)</span> and <span class="math inline">\({\cal L}_- (R=0.2)\)</span> from the breakdown given in the above example.</p>
<p><span class="math inline">\({\cal L}_+ (R=0.2)\)</span> is the probability of observing the given data (<span class="math inline">\(+\)</span>) in the hypothetical world where <span class="math inline">\(R=0.2\)</span>. Out of the 1000 people, 160 will get a correct test result <span class="math inline">\(+\)</span> and 240 people will get a <span class="math inline">\(+\)</span> despite not having the illness. Therefore, <span class="math display">\[{\cal L}_+ (R=0.2) = \frac{160+240}{1000} = 40\%\ .\]</span></p>
<p>Similarly, <span class="math inline">\({\cal L}_- (R=0.2)\)</span> is the probability of observing the given data (<span class="math inline">\(-\)</span>) in the hypothetical world where <span class="math inline">\(R=0.2\)</span>. In the above breakdown, altogether 600 people received a <span class="math inline">\(-\)</span> result: 560 of these were indeed healthy and 40 had the illness but nonetheless got a <span class="math inline">\(-\)</span> result. So,
<span class="math display">\[{\cal L}_- (R=0.2) = \frac{40+560}{1000} = 60\%\ .\]</span></p>
</div>
<p>The above example calculated the output of the likelihood function for both <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span> results when <span class="math inline">\(R=0.2\)</span>. We can repeat the calculation for any other value of <span class="math inline">\(R\)</span>. The results, as you can confirm yourself, are</p>
<p><span class="math display">\[{\cal L}_+(R) = p_d(+)\ R + (1-p_h(-))\ (1-R) \]</span>
and</p>
<p><span class="math display">\[{\cal L}_-(R) = (1-p_d(+))\ R + p_h(-)\, (1-R)\ .\]</span>
In a real-world situation, we would have to do some experiments to measure <span class="math inline">\(p_h(-)\)</span> and <span class="math inline">\(p_d(+)\)</span>. For our example we’ll set <span class="math inline">\(p_d(+) = 0.8\)</span> and <span class="math inline">\(p_h(-) = 0.7\)</span>.</p>
<p>Now that we have constructed the likelihood functions for the two possible observations <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span>, we can use them to update the priors.</p>
<p>Suppose our first observations are the results of screening tests on ten randomly selected individuals.</p>
<table>
<thead>
<tr class="header">
<th align="left">Subject ID</th>
<th align="left">Test outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">4349A</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="even">
<td align="left">7386A</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">6263E</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="even">
<td align="left">5912C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">7361C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">9384C</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="left">6312A</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">3017C</td>
<td align="left"><span class="math inline">\(+\)</span></td>
</tr>
<tr class="odd">
<td align="left">1347B</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="left">9611D</td>
<td align="left"><span class="math inline">\(-\)</span></td>
</tr>
</tbody>
</table>
<p>To summarize: Three <span class="math inline">\(+\)</span> tests out of ten.</p>
<p>After the first test outcome is available we can calculate the posterior: <span class="math display">\[\text{posterior}_1 (R) = {\cal L}_+(R) \times \text{prior(R)}\ .\]</span> After the second test outcome, the new posterior is <span class="math display">\[\text{posterior}_2 (R) = {\cal L}_-(R) \times {\cal L}_+(R) \times \text{prior(R)}\ .\]</span> And after the third (a <span class="math inline">\(+\)</span> result!) it will be <span class="math display">\[\text{posterior}_3 (R) = {\cal L}_+(R) \times {\cal L}_-(R) \times {\cal L}_+(R) \times \text{prior(R)}\ .\]</span>
We continue on in this way through all ten rows of the data to get the posterior distribution after all 10 test results have been incorporated.</p>
<p>Figure <a href="probability-and-evidence.html#fig:after-10">35.8</a> shows the posterior after the 10 rows of data have been considered for each of the three priors from Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-1"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-1-1.png" alt="Posteriors (blue) after the first screening test, which was $+$, for each of the priors in Figure \@ref(fig:beta-shapes). The prior itself is drawn in gray." width="90%" />
<p class="caption">
Figure 35.7: Posteriors (blue) after the first screening test, which was <span class="math inline">\(+\)</span>, for each of the priors in Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a>. The prior itself is drawn in gray.
</p>
</div>
<p>With just one row of data considered, the posteriors depend very much on the particular prior selected. This shouldn’t be a surprise; one test result from an imperfect screening test is not going to tell us much. Let’s process all of the</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-10"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-10-1.png" alt="Posteriors (blue) after the first ten rows of data, for each of the priors in Figure \@ref(fig:beta-shapes). The prior itself is drawn in gray." width="90%" />
<p class="caption">
Figure 35.8: Posteriors (blue) after the first ten rows of data, for each of the priors in Figure <a href="probability-and-evidence.html#fig:beta-shapes">35.6</a>. The prior itself is drawn in gray.
</p>
</div>
<p>After the first 10 rows of data have been considered, the posteriors are similar despite the different priors.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:after-100"></span>
<img src="MOSAIC-Calculus_files/figure-html/after-100-1.png" alt="Posteriors (blue) after 100 subjects have been screen, with 30 $+$ results." width="90%" />
<p class="caption">
Figure 35.9: Posteriors (blue) after 100 subjects have been screen, with 30 <span class="math inline">\(+\)</span> results.
</p>
</div>
<p>As data accumulates, the priors become irrelevant; the knowledge about the risk of disease is being driven almost entirely by the data.</p>
<p>Remarkably, even though 30% of the tests were positive, all the posteriors place almost all the probability density on transmission risks less than 20%. This is because the likelihood functions correctly take into account the imperfections of the screening test.</p>
<!--
## Monte Carlo (optional)

::: {.underconstruction}
**Under Construction**

Content subject to revision.
:::


Monte Carlo is a neighborhood inside the Principality of Monaco, located on the Mediterranean Sea between France and Italy. It's famous for the *Place du Casino*, where a wealthy clientele gambles fortunes at roulette and other games of chance.

"Monte Carlo" is the name cleverly coined at the start of the modern computer era for a type of computation that builds on random number generation. As a very simple example, consider the problem of finding the definite integral
$$\int_{-\infty}^\infty x^2 \dnorm(x, 5, 3)\, dx\ .$$


The mean describes the center of the distribution, the sd describes the width. You may also be familiar with the *statistic* called the ***arithmetic mean***. You calculate the statistic by adding up a set of $n$ numbers and dividing by $n$, that is,
$$\text{mean} \equiv \frac{1}{n} \sum_{i=1}^n x_i\ ,$$
where $x_i$ is the $i$th row of a data frame with and $x$ is one of the columns. To illustrate, consider the `Zcalc::Home_utilities` data frame which has data on the monthly use of electricity and natural gas by a Minnesota household.

::: {.rmosaic data-latex=""}
The R/mosaic function for the calculation of the arithmetic mean is `df_stats()`. (The name stands for "data frame stats.") Here's a calculation of the mean outdoor temperature from `Home_utilities`:

```r
df_stats(~ temp, data = Zcalc::Home_utilities, mean)
```

```
##   response     mean
## 1     temp 48.29454
```
The result has units of degrees Fahrenheit.

`df_stats()` can calculate many other statistics on a column of data, such as `median`, `sd`, `var`, `quantile()`s (such as `quantile(0.33)`), confidence intervals (`ci.mean()`, `ci.median()`, `ci.sd()`), and so on. In this book, we'll use `mean`, `sd`, and `var`.

Another technique we will use is to generate random events from a probability distribution, particularly from the gaussian, exponential, and uniform distributions. For example, the following command will construct a data frame (that's what `tibble()` is for) that has a column named `y` that contains 100 random draws from a gaussian distribution with mean of 3 and sd of 0.5. Then the arithmetic mean and ***standard deviation*** are computed from the numbers in the simulation. 

```r
Simulation <- tibble(
  x = rnorm(100, mean=3, sd=0.5)
)
head(Simulation)
```

```
## # A tibble: 6 × 1
##       x
##   <dbl>
## 1  2.84
## 2  3.28
## 3  2.66
## 4  3.11
## 5  3.16
## 6  3.59
```

```r
df_stats( ~ x, data=Simulation, mean, sd)
```

```
##   response     mean        sd
## 1        x 2.981404 0.4670355
```
The calculated mean and sd only approximate the values used in the simulation. That's because they are based on random draws.
:::

-->
</div>
<div id="exercises-3" class="section level2" number="4.5">
<h2><span class="header-section-number">35.5</span> Exercises</h2>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/kitten-pay-sheet.Rmd" href="#aoxjLV"><img src="www/icons8-signpost.png" title="Location: Exercises/kitten-pay-sheet.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">aoxjLV</red></span>
</summary>
<p>In Figure <a href="probability-and-evidence.html#fig:contest-compare">35.4</a> estimate the area under each of the three curves. Which of these is true? Keep in mind that your estimates will at best be an approximation, so remember to use your theoretical knowledge of probability density functions to refine your estimates.</p>
<ul>
<li>area(Johnny) &gt; area(Louisa)</li>
<li>area(Louisa) &gt; area(Geoff)</li>
<li>area(Geoff) &gt; area(Johnny)</li>
<li>none of the above
</details>
<!-- densities all have area 1 --></li>
</ul>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/lamb-hear-roof.Rmd" href="#nHCIMc"><img src="www/icons8-signpost.png" title="Location: Exercises/lamb-hear-roof.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">nHCIMc</red></span>
</summary>
In Equation <a href="#eq:unif-mean">(<strong>??</strong>)</a>, explain why the integral from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> is the same as the integral from 0 to 10.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/horse-cost-spoon.Rmd" href="#ku4QGW"><img src="www/icons8-signpost.png" title="Location: Exercises/horse-cost-spoon.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">ku4QGW</red></span>
</summary>
<p>Exponential distributions are <strong><em>self similar</em></strong>. Looking at Figure <a href="#fig:exponential-density"><strong>??</strong></a> and assume that <span class="math inline">\(1/k = 100\)</span> days. According to the density function, the probability of an event happening in the first 100 days is 63.2%. Of course that means there is 36.8% chance that the event will happen after the 100 day mark. If the event does not happen in the first 100 days, there is a 63.2% chance that it will happen in interval 100-200 days. Similarly, if the event does not happen in the first 200 days, there is a 63.2% chance that it will happen in interval 200-300 days. Use these facts to calculate the probability mass in each of these intervals:</p>
<ul>
<li>0-100 days</li>
<li>100-200 days</li>
<li>200-300 days</li>
<li>300-400 days</li>
</ul>
Hint: Make sure that the sum of these probability masses does not exceed 1.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/kitten-ring-plant.Rmd" href="#uKHwI4"><img src="www/icons8-signpost.png" title="Location: Exercises/kitten-ring-plant.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">uKHwI4</red></span>
</summary>
<p>The functions <code>dunif()</code>, <code>dnorm()</code>, and <code>dexp()</code>, respectively, implement the uniform, gaussian, and exponential families of distributions. The word “family” is used because each each family has it’s own parameters:</p>
<ul>
<li>Uniform: min and max</li>
<li>Gaussian: mean and sd</li>
<li>Exponential: rate (with the exponential function parameterized as <span class="math inline">\(\exp\left(-\frac{t}{\text{rate}}\right)\)</span>.)</li>
</ul>
<p>Pick 3 very different sets of parameters for each family. (They should be meaningful, for instance sd <span class="math inline">\(&gt;0\)</span> and rate <span class="math inline">\(&gt;0\)</span>.)</p>
<ol style="list-style-type: decimal">
<li><p>Numerically integrate each of the 9 distributions to confirm that the total probability is 1.</p></li>
<li><p>Compute the expectation value of each of the 9 distributions.</p></li>
<li><p>Compute the variance of each of the 9 distributions.</p></li>
</ol>
Hand in your commands for each of the above tasks and the corresponding output.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/snake-hurt-candy.Rmd" href="#HNJn9i"><img src="www/icons8-signpost.png" title="Location: Exercises/snake-hurt-candy.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">HNJn9i</red></span>
</summary>
<p>Plot out <code>dnorm()</code> on semi-log axis. You can choose your own mean and sd, and your graphics domain should cover at least mean <span class="math inline">\(\pm 3\)</span>sd.</p>
<ol style="list-style-type: decimal">
<li>Describe the shape of the function graph on semi-log axes.</li>
<li>Explain what about the graphic indicates that <code>dnorm(x)</code> will never be zero for any finite <code>x</code>.
</details></li>
</ol>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/maple-come-rug.Rmd" href="#3eLqIt"><img src="www/icons8-signpost.png" title="Location: Exercises/maple-come-rug.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">3eLqIt</red></span>
</summary>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
Generate random numbers from distributions to illustrate density: none of the generated numbers will match a pre-specified value.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/seahorse-catch-cotton.Rmd" href="#PEAKVD"><img src="www/icons8-signpost.png" title="Location: Exercises/seahorse-catch-cotton.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">PEAKVD</red></span>
</summary>
<p><span class="math inline">\(e^{-k x}\)</span> can be thought of as a relative density function. Why?</p>
<!-- It's never negative. -->
<p>The exponential probability function <code>dexp()</code> is a scaled version of the relative density function. Find, symbolically, the scalar by which <span class="math inline">\(e^{-k x}\)</span> must be multiplied to turn it into a probability density function.</p>
<!-- 1/k  -->
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/oak-hurt-ring.Rmd" href="#xa6qsb"><img src="www/icons8-signpost.png" title="Location: Exercises/oak-hurt-ring.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">xa6qsb</red></span>
</summary>
Count the grid squares under the probability density function in Figure <a href="#fig:exponential-density"><strong>??</strong></a> and calculate the area a single grid square. What’s the total area under the curve? Make sure to give units, if any.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/goat-sit-knob.Rmd" href="#1dG7W8"><img src="www/icons8-signpost.png" title="Location: Exercises/goat-sit-knob.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">1dG7W8</red></span>
</summary>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
<p>Describe constructing the cumulative and using it to calculate quantiles.</p>
<p>The <strong>cumulative</strong> distribution translates the probability density into an actual probability (a number between zero and one). Formally, the cumulative distribution is <span class="math display">\[P(t) \equiv \int_{-\infty}^t p(t) dt\]</span></p>
<p>Evaluating <span class="math inline">\(P(t)\)</span> at given value of <span class="math inline">\(t\)</span> gives a probability. For instance, <span class="math inline">\(P(10) \approx 0.095\)</span>, roughly 10%. In terms of storms, this means that according to the standard model of these things, the time between consequtive 100-year storms has a 10% chance of being 10 years or less!</p>
<p>A graph of the <strong>cumulative</strong> distribution shows what you might have anticipated: the gaussian function <span class="math inline">\(p(t)\)</span> has an integral that is a sigmoid function.</p>
<p><img src="MOSAIC-Calculus_files/figure-html/unnamed-chunk-69-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p><strong>Question A</strong> Imagine that a 100-year storm has just happened at your location. What is the probability that the next 100-year storm will happen within 50 years?</p>
<ol style="list-style-type: lower-roman">
<li><span class="Zchoice">11%<span class="mcanswer">︎✘ What’s the value of <span class="math inline">\(P(t=50)\)</span></span></span><br />
</li>
<li><span class="Zchoice">27%<span class="mcanswer">︎✘ What’s the value of <span class="math inline">\(P(t=50)\)</span></span></span><br />
</li>
<li><span class="Zchoice">39%<span class="mcanswer">Right! </span></span><br />
</li>
<li><span class="Zchoice">51%<span class="mcanswer">︎✘ What’s the value of <span class="math inline">\(P(t=50)\)</span></span></span></li>
</ol>
<p><strong>Question B</strong> The <em>median</em> time between 100-year storms is the value where there is a 50% probability that consecutive storms will happen closer in time than this value and 50% that consecutive storms will happen further apart than this value. What is the <em>median</em> time between 100-year storms, according to the standard model? (Hint: You can read this off the graph.)</p>
    <span class="Zchoice">about 30 years<span class="mcanswer">︎✘ </span></span>       <span class="Zchoice">50 years<span class="mcanswer">︎✘ </span></span>       <span class="Zchoice">about 70 years<span class="mcanswer"><span class="math inline">\(\heartsuit\ \)</span></span></span>       <span class="Zchoice">100 years<span class="mcanswer">︎✘ </span></span>       <span class="Zchoice">about 130 years<span class="mcanswer">︎✘ </span></span>
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/birch-burn-piano.Rmd" href="#gOhIfR"><img src="www/icons8-signpost.png" title="Location: Exercises/birch-burn-piano.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">gOhIfR</red></span>
</summary>
<p>Calculate symbolically the expectation value and variance of the uniform distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[\text{unif}(x, a, b) \equiv \left\{{\Large\strut}\begin{array}{cl}\frac{1}{b-a}&amp; \text{for}\ a \leq x \leq b\\0&amp; \text{otherwise} \end{array}\right.\]</span></p>
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/ape-hurt-fridge.Rmd" href="#Muhyz8"><img src="www/icons8-signpost.png" title="Location: Exercises/ape-hurt-fridge.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">Muhyz8</red></span>
</summary>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
<p>INTRODUCE THE CUMULATIVE.</p>
<p>Variety of exercises on probabilities, e.g. prob of further out than 1 sd in gaussian, prob of further out than 1 sd in uniform, and so on.</p>
<p>For every probability density function, such as the one displayed in Figure <a href="probability-and-evidence.html#fig:dart-g">35.1</a>, there is another way of displaying the model called a <strong><em>cumulative distribution function</em></strong>. For the probability density function <span class="math inline">\(g(x)\)</span>, the cumulative distribution function <span class="math inline">\(G(x)\)</span> is <span class="math display">\[G(x) \equiv \int_{-\infty}^infty g(x) dx\]</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dart-g-cumulative"></span>
<img src="MOSAIC-Calculus_files/figure-html/dart-g-cumulative-1.png" alt="The cumulative distribution function corresponding to $g(x)$ in Figure \@ref(fig:dart-g)." width="90%" />
<p class="caption">
Figure 35.10: The cumulative distribution function corresponding to <span class="math inline">\(g(x)\)</span> in Figure <a href="probability-and-evidence.html#fig:dart-g">35.1</a>.
</p>
</div>
<p>The output of the cumulative distribution function is a probability: a number between zero and one. But the probability of what? The cumulative distribution function tells the probability of the outcome of the event being <strong><em>less-than</em></strong> the value of the input. For instance, in Figure <a href="probability-and-evidence.html#fig:dart-g-cumulative">35.10</a>, the probability of the dart being closer to the bullseye than 5 cm is about 0.7.</p>
<p>Cumulative distribution functions are the means with which many common probability calculations are done. For instance, the probability that the dart will fall 2 to 5 cm from the bullseye is <span class="math inline">\(G(5) - G(2)\)</span>. Such calculations are simply in the style of definite integrals: <span class="math display">\[G(5) - G(2) = \int_2^5 g(x)\,dx\ .\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 35.6  </strong></span>What is the probability that the dart will fall farther than 5 cm from the bullseye?</p>
<p>This is the same as asking for the probability that the dart will fall somewhere in the interval 5cm to <span class="math inline">\(\infty\)</span>, that is:
<span class="math display">\[\int_5^\infty g(x)\, dx = G(\infty) - G(5) = 1 - 0.7127 = 0.2873\]</span>
To understand why <span class="math inline">\(G(\infty) = 1\)</span>, ask yourself what is the probability that a thrown dot will land closer to the bullseye than <span class="math inline">\(\infty\)</span>. Of course it will! That corresponds to a probability value of 1.</p>
</div>
<p>EXERCISE: CALCULATIONS on the normal, F, and exponential distributions.</p>
EXERCISE: Calculate the 95% credible intervals on the posterior on the disease.
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/seal-tug-mattress.Rmd" href="#F7klxV"><img src="www/icons8-signpost.png" title="Location: Exercises/seal-tug-mattress.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">F7klxV</red></span>
</summary>
<p>In the Social Security life-table <code>M2014F</code>, one column is <code>nliving</code>. The <code>nliving</code> variable is computed by tracking the age-specific mortality rate as it plays out in a hypothetical population of 100,000 newborns. The age-specific mortality rate at age 0 is applied the the 100,000 to calculate the number of deaths in the first year: 531. Therefore 99,469 survive to age 1. Then the age-specific mortality rate at age 1 is applied to the 99,469 survivors to calculate the number of deaths of one-year olds: 34. This leaves 99,434 surviving two-year olds. (There’s round-off error, involved, which is why the number is not 99,435.) The process is continued up through age 120, at which point there are no survivors.</p>
<p>The following R code constructs from <code>M2014F</code> a function <code>died_before(age)</code> giving the fraction of the cohort of 100,000 who died at or before the given <code>age</code>.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="probability-and-evidence.html#cb36-1" aria-hidden="true" tabindex="-1"></a>died_before <span class="ot">&lt;-</span> M2014F <span class="sc">%&gt;%</span></span>
<span id="cb36-2"><a href="probability-and-evidence.html#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(age, nliving) <span class="sc">%&gt;%</span></span>
<span id="cb36-3"><a href="probability-and-evidence.html#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> nliving<span class="sc">/</span><span class="dv">100000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb36-4"><a href="probability-and-evidence.html#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spliner</span>(<span class="dv">1</span><span class="sc">-</span>prob <span class="sc">~</span> age, <span class="at">data =</span> .)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Plot out <code>died_before(age)</code> vs <code>age</code>. Explain what you see in the graph that tells you that this is a <strong>cumulative</strong> probability function.</p></li>
<li><p>To calculate life-expectancy, we need to convert <code>died_before(age)</code> into <code>died_at(age)</code>, the probability <strong>density</strong> of death at any given age. Use R/mosaic to construct <code>died_at(age)</code>, which will be a basic calculus transformation of <code>died_before()</code>.</p></li>
<li><p>What are the units of the output of the <code>died_at(age)</code> function?</p></li>
</ol>
<!-- per year -->
<ol start="4" style="list-style-type: decimal">
<li>Find the expectation value of <code>age</code> under the probability density <code>died_at(age)</code>. This is called the <strong><em>life-expectancy at birth</em></strong>: the average number of years of life of the people in the imaginary cohort of 100,000.</li>
</ol>
</details>
<p><!-- cumulative probability density and life-expectancy at birth. --></p>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/seaweed-dig-cotton.Rmd" href="#uZLhZ0"><img src="www/icons8-signpost.png" title="Location: Exercises/seaweed-dig-cotton.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">uZLhZ0</red></span>
</summary>
<p>The R code below will construct a function, <code>prob_death60(age)</code> that gives the probability that a person reaching her 60th birthday will die at any given age. (The function is constructed from US Social Security administration data for females in 2014.)</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="probability-and-evidence.html#cb37-1" aria-hidden="true" tabindex="-1"></a>prob_death60 <span class="ot">&lt;-</span> M2014F <span class="sc">%&gt;%</span></span>
<span id="cb37-2"><a href="probability-and-evidence.html#cb37-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(age <span class="sc">&gt;=</span> <span class="dv">60</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-3"><a href="probability-and-evidence.html#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(age, died) <span class="sc">%&gt;%</span></span>
<span id="cb37-4"><a href="probability-and-evidence.html#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> died<span class="sc">/</span><span class="dv">91420</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-5"><a href="probability-and-evidence.html#cb37-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spliner</span>(prob <span class="sc">~</span> age, <span class="at">data =</span> .)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>The “life expectancy at age 60” is the expectation value for the number of years of additional life for person who reaches age 60. (The number of years of additional life is <code>age - 60</code>.) Compute the life-expectancy at age 60 based on the <code>prob_death(age)</code> function.</p></li>
<li><p>A more technically descriptive name for life-expectancy would be “expectation value of additional life-duration.” Calculate the standard deviation of “additional life-duration.”</p></li>
<li><p>Construct the cumulative probability function for age at death for those reaching age 60. (Hint: Since the value of the cumulative at age 60 should be 0, set the argument <code>lower.bound=60</code> in <code>antiD()</code> so that the value will be zero at age 60.) From the cumulative, find the <strong><em>median</em></strong> age of death for those reaching age 60. (Hint: <code>Zeros()</code>.)</p></li>
<li><p>In a previous exercise, we found from these same data that the life expectancy at birth is about 81 years. Many people mis-understand “life expectancy at birth” to mean that people will die mainly around 81 years of age. That’s not quite so. People who are approaching 81 should keep in mind that they likely have additional years of life. A good way to quantify this is with the life-expectancy at age 81. We can calculate life-expectancy at 81 based on the <code>prob_death60()</code>. You can do this by scaling <code>prob_death60()</code> by <span class="math inline">\(A\)</span> such that <span class="math display">\[\frac{1}{A}  = \int_{81}^{120} \text{prob_death60}(\text{age})\, d\text{age}\ .\]</span></p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Calculate <span class="math inline">\(A\)</span> for age 81.</p></li>
<li><p>Using the <span class="math inline">\(A\)</span> you just calculated, find the life-expectancy at age 81, that is, the expectation value of additional years of life at age 81. Also calculate the standard deviation.</p></li>
</ol>
</details>
<p><!-- life expectancy and standard deviation --></p>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/titmouse-read-chair.Rmd" href="#CeN4GV"><img src="www/icons8-signpost.png" title="Location: Exercises/titmouse-read-chair.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">CeN4GV</red></span>
</summary>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
<ol style="list-style-type: decimal">
<li><p>Introduce “information” as the integral of <span class="math inline">\(-\log_2(R)\)</span> over the distribution.</p></li>
<li><p>Kulbach-Liebler entropy of measure of the gain in information from the disease.</p></li>
</ol>
</details>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/aspen-bet-roof.Rmd" href="#ixpCg0"><img src="www/icons8-signpost.png" title="Location: Exercises/aspen-bet-roof.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">ixpCg0</red></span>
</summary>
<div class="underconstruction">
<p><strong>Under Construction</strong></p>
<p>Content subject to revision.</p>
</div>
<p>One of the goals for self-driving cars is to reduce road accidents, especially fatal accidents. People are understandably skeptical that an automated system can cope with all the varying conditions of traffic, visibility, road damage, etc. without the benefit of human judgment or experience. For this reason, society will need to accumulate substantial evidence for enhanced safety in self-driving cars before accepting any claims to that effect.</p>
<p>This exercise is about how to accumulate such evidence.</p>
<p>Based on experience with tens of millions of regular cars driving hundreds of billions of total miles, suppose we decide the accident probability is approximately 10% per 20,000 miles. Note that this is not stating that an accident is certain to occur in the first 200,000 miles of driving. The probability that, for a representative car, an accident occurs at <span class="math inline">\(m\)</span> miles will have an <strong><em>exponential shape</em></strong> <span class="math inline">\(g(m, k)\)</span> where <span class="math display">\[g(m, D) \equiv \frac{1}{D} e^{-m/D}\, .\]</span> Consequently, an accident might happen in the first few miles or at 300,000 miles or not at all within the lifetime of the car.</p>
<p>Take note that we have parameterized the exponential with <span class="math inline">\(D\)</span>, which will have units of miles. Thus, bigger <span class="math inline">\(D\)</span> means a safer car. You can think about <span class="math inline">\(D\)</span> as indicating the distance traveled by a typical car before it has an accident.</p>
<p><strong>Part A</strong>. Estimate the <span class="math inline">\(D\)</span> to be consistent with the idea that there is a 10% chance of an accident at or before the first <span class="math inline">\(m=20,000\)</span> miles of driving.</p>
<ol style="list-style-type: lower-roman">
<li><p>Recognize that “at or before” corresponds to a <strong><em>cumulative probability function</em></strong> which in this case will be <span class="math display">\[\int_0^m \frac{1}{D} e^{- x/D}\, dx\ .\]</span></p></li>
<li><p>Construct the cumulative probability density in symbolic form. It will be a function of both <span class="math inline">\(D\)</span> and <span class="math inline">\(m\)</span>, let’s call it <span class="math inline">\(G(m, D)\)</span>. <!-- It will be $1- e^{- m/D}$ --> (Hint: <span class="math inline">\(G(0, D)\)</span> must be zero. <span class="math inline">\(G(m, D)\)</span> will have a roughly sigmoidal increase with <span class="math inline">\(m\)</span>. <span class="math inline">\(\lim_{m \rightarrow \infty} G(m, D) = 1\)</span>.)</p></li>
<li><p>Set <span class="math inline">\(G(m=20000, D_0) = 0.10\)</span>, per our assumption for ordinary cars of a 10% risk in <span class="math inline">\(m=20,000\)</span> miles of driving. Solve this to find <span class="math inline">\(D_0\)</span>. <!-- It will be $k \approx 190,000$ miles --></p></li>
</ol>
<p>Show your work.</p>
<p>To start the calculations, we will need a <strong><em>prior</em></strong> relative density function for <span class="math inline">\(k\)</span>. We’re hopeful but skeptical about self-driving cars. The best case scenario, let’s say, is that the self-driving cars will be 10 times safer than regular cars. The worst case is that they will be 10 times worse. Using <span class="math inline">\(k\)</span> as our measure of safety, we’ll set our prior to have the form <span class="math inline">\(1/D\)</span>.</p>
<p><strong>Part B</strong>: Implement the function <span class="math inline">\(\text{prior}(D) = 1/D\)</span>.</p>
<ol style="list-style-type: lower-roman">
<li>Graph out <span class="math inline">\(\text{prior}(D)\)</span> on the domain 2,000 to 2,000,000, that is, roughly <span class="math inline">\(D_0/10\)</span> to <span class="math inline">\(10 D_0\)</span>.</li>
<li>Referring to your graph, write down your intuition about whether this prior seems to favor <span class="math inline">\(D &lt; D_0\)</span> or not.</li>
<li>Now remember that <span class="math inline">\(\text{prior}(D)\)</span> shows relative <strong>density</strong>. So compare the total probability that <span class="math inline">\(D_0/10 \leq D \leq D_0\)</span> to the probability of <span class="math inline">\(D_0 \leq D \leq 10 D_0\)</span>
<span class="math display">\[\int_{20,000}^{200,000} \text{prior}(D)\, dD\ \ \ \ \ \text{to}\ \ \ \ \ \int_{200,000}^{2,000,000} \text{prior}(D)\, dD\ .\]</span> Does this indicate that the <span class="math inline">\(1/D\)</span> prior is biased toward the assumption that self-driving cars will be no safer than ordinary cars? Explain your reasoning.</li>
</ol>
<p>Now imagine that you work for a safety organization collecting accident information. To figure out the safety of self-driving cars, you are monitoring a fleet of 100 self-driving cars. Each year you get a report giving the odometer reading of the car, or, if the car has been in an accident that year, the odometer reading at the time of the accident. The data might look like the following table. (The table is entirely fictitious and shouldn’t be misinterpreted as representing real-world self-driving cars.):</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>Status</th>
<th align="right">Mileage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>on road</td>
<td align="right">85,300</td>
</tr>
<tr class="even">
<td>2</td>
<td>on road</td>
<td align="right">65,200</td>
</tr>
<tr class="odd">
<td>3</td>
<td>accident</td>
<td align="right">13,495</td>
</tr>
<tr class="even">
<td>4</td>
<td>on road</td>
<td align="right">131,200</td>
</tr>
<tr class="odd">
<td>5</td>
<td>on road</td>
<td align="right">96,000</td>
</tr>
<tr class="even">
<td>6</td>
<td>accident</td>
<td align="right">54,682</td>
</tr>
<tr class="odd">
<td>7</td>
<td>accident</td>
<td align="right">105,200</td>
</tr>
<tr class="even">
<td>8</td>
<td>on road</td>
<td align="right">53,900</td>
</tr>
<tr class="odd">
<td>9</td>
<td>accident</td>
<td align="right">86,000</td>
</tr>
<tr class="even">
<td>10</td>
<td>on road</td>
<td align="right">94,300</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td>100</td>
<td>on road</td>
<td align="right">107,200</td>
</tr>
</tbody>
</table>
<p>Thr first two cars in the fleet have accumulated 85,300 and 65,200 accident-free miles respectively. The third car was in an accident at 13,495 miles and is no longer on the road.</p>
<p>In response to the data, you issue a yearly report in the form of a <strong>posterior</strong> distribution on <span class="math inline">\(D\)</span>, our measure of safety.</p>
<p>To update the original prior into a posterior, you need to construct the likelihood functions. There are two functions, because there are two different kinds of observations on each car:</p>
<ol style="list-style-type: lower-alpha">
<li>If the car was in an accident, then you want the likelihood of <span class="math inline">\(D\)</span> <strong>given</strong> the mileage at which the accident happened. Since the probability model is <span class="math inline">\(\frac{1}{D}\, e^{-D\, m}\)</span>, the likelihood function for a car that had an accident at <span class="math inline">\(m_\text{accident}\)</span> miles will be <span class="math inline">\(\frac{1}{D}\, e^{-D\, m_\text{accident}}\)</span>.</li>
<li>If the car has not been in an accident, then you want the likelihood of <span class="math inline">\(D\)</span> <strong>given</strong> the number of miles traveled.</li>
</ol>
<p>The likelihood function (b) is based on the probability model that the car has <strong>not</strong> had an accident in <span class="math inline">\(m_\text{driven}\)</span> miles of driving. Recall that the cumulative probability, <span class="math inline">\(G(m, D)\)</span> in part A of this exercise, is the probability that the car <strong>did</strong> have an accident at or before <span class="math inline">\(m\)</span> miles of driving. So the probability that the car did not have an accident in that amount of driving is <span class="math inline">\(1 - G(m, D)\)</span>. The likelihood function will therefore be <span class="math inline">\(1 - G(m_\text{driven}, D)\)</span>.</p>
<p><strong>Part C</strong>. Implement the two likelihood functions in R as <code>Laccident(m, D)</code> and <code>Lno(m D)</code>.</p>
<p>Plot out the two functions and explain why their shape makes sense. Show the code that implements the two functions.</p>
<p>Now you are in a position to update the prior to create the posterior probability density on <span class="math inline">\(D\)</span> given the data at hand. As always with Bayesian inference, the posterior will be: <span class="math display">\[\text{posterior}(D) = \frac{1}{A} \prod_\text{cars}\text{likelihood}_i(D | \text{miles}_i) \text{prior}(D)\]</span> where <span class="math inline">\(\text{likelihood}_i()\)</span> refers to whichever one of the two likelihood functions in Part C is relevant to car <span class="math inline">\(i\)</span> and <span class="math inline">\(\text{miles}_i\)</span> is the observed mileage for that car. The constant <span class="math inline">\(A\)</span> is selected to normalize the posterior probability density.</p>
<p><strong>Part D</strong>. To keep things simple, we’ll use just the first 10 cars in the fleet report. The unnormalized posterior function will be:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="probability-and-evidence.html#cb38-1" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="cf">function</span>(D) {</span>
<span id="cb38-2"><a href="probability-and-evidence.html#cb38-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">Lno</span>(<span class="dv">85300</span>, D) <span class="sc">*</span> <span class="fu">Lno</span>(<span class="dv">65200</span>, D) <span class="sc">*</span> </span>
<span id="cb38-3"><a href="probability-and-evidence.html#cb38-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">Laccident</span>(<span class="dv">13495</span>, D) <span class="sc">*</span> <span class="fu">Lno</span>(<span class="dv">131200</span>, D) <span class="sc">*</span>  </span>
<span id="cb38-4"><a href="probability-and-evidence.html#cb38-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#similarly for each of the remaining cars ...</span></span>
<span id="cb38-5"><a href="probability-and-evidence.html#cb38-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prior</span>(D)</span>
<span id="cb38-6"><a href="probability-and-evidence.html#cb38-6" aria-hidden="true" tabindex="-1"></a>}    </span></code></pre></div>
<p>Based on the data from the first 10 cars, do you think that the self-driving cars are safer than the ordinary cars? Recall that <span class="math inline">\(D_0\)</span>, calculated above, represents the safety of ordinary cars.</p>
</details>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Numerical integrals from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> of functions that are zero almost everywhere are challenging. The computer has to figure out where, out of the whole number line, the function has non-zero output. We’ve given the computer a head start by using 0 in the limits of integration. This would not be a problem for the exponential or gaussian distribution, which are non-zero everywhere (for the gaussian) or for half the number line (for the exponential).<a href="probability-and-evidence.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimization-and-constraint.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future-value.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/MOSAIC-Calculus/shared/B4-probability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/dtkaplan/MOSAIC-Calculus/shared/B4-probability.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
