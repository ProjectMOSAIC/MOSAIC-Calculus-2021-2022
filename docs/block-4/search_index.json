[["block4-intro-draft.html", "Introduction", " Introduction In Blocks 1 through 3, you learned how to formulate calculus questions in terms of functions and the two main operations of calculus, differentiation and anti-differentiation, which transform one function into another. You also learned the primary use of anti-differentiation, to accumulate the output of a function over an interval of the input variable. There is more mathematics yet to come. In Block 5 you’ll meet a major technique for modeling used in essentially all quantitative disciplines and with particularly important applications to data science. In Block 6 we’ll start to work with multi-component systems as they evolve in time. Here, in Block 4, we’re going to step momentarily away from the introduction of new mathematical concepts. Functions, differentiation, and anti-differentiation are impressive and useful in their own right so now we’re going to look at how they are used, how they manifest themselves in diverse ways in the world of science and technology. Our emphasis will not be on field-specific applications, although there are many! Instead, we’ll explore ways the concepts of calculus appear and are used broadly in quantitative work. "],["operations.html", "Chapter 27 Operations on functions 27.1 Task: Solve 27.2 Task: Argmax 27.3 Task: Iterate 27.4 Software for the tasks 27.5 Algorithmic techniques 27.6 Newton’s method 27.7 Exercises", " Chapter 27 Operations on functions In Block 1, we introduced the idea of mathematical modeling, creating a representation of some aspect of the world out of mathematical “stuff.” As you know, for us the relevant “stuff” includes the concept of a function with its inputs and output, units and dimensions of quantities, frameworks such as the basic modeling functions and ways of combining functions via linear combination, composition, and multiplication. When we construct functions, the symbol \\(\\equiv\\) is appropriately used to mean “is defined to be.” We have used the tradition equal sign (\\(=\\)) but always to mean “is” or “amounts to” or “happens to equal.” For instance, we write \\(\\sin(\\pi/2) = 1\\) not as a definition of the sinusoid but to identify the output corresponding to a single specific input.. In high-school algebra, much more emphasis was placed on equations. For instance, you might well see an equation like \\[{\\mathbf{\\text{equation:}}}\\ \\ \\ x^2 - 6 x - 3 = 0\\] in a beginning algebra textbook. It pays to think a little about what such an equation means and what information it is intended to convey. In particular, how is it different from a function like \\[{\\mathbf{\\text{function:}}}\\ \\ \\ p(x) \\equiv x^2 - 6 x - 3 \\ .\\] A simple equation like \\(3 + 4 = 7\\) is a statement of fact: three plus four is indeed exactly the same as seven. But \\(x^2 - 6 x - 3 = 0\\) is not a fact. The equality might be true or false, depending on what \\(x\\) happens to be. In an algebra course, is really meant to be an instruction to a person: \\[x^2 - 6 x - 3 = 0, \\ \\ {\\mathbf{\\text{Instruction}}}\\text{: Find x.}\\] What’s meant by “find \\(x\\)” is to determine which numerical values (if any) when substituted for \\(x\\) in the equation will produce a true statement. This is also called solving for \\(\\mathbf x\\). Algebra courses offer a variety of solving techniques that are effective for different types of problem statements. “Solving for \\(x\\)” is an example of a mathematical task. We undertake such tasks in order to extract useful information from a mathematical object. For instance, in high-school textbook “word problems,” you translate a verbal description of a situation—typically involving canoes paddling across a flowing river—into a matching mathematical form and then, having constructed the mathematical form, you apply some mathematical task to the form in order to reveal the answer you seek. In this chapter, we’re going to look at the mathematical tasks that are commonly performed on functions, that is, operations on functions. We’ll introduce you to algorithms that enable you to construct the task result without having to apply much mental work. Importantly, we’ll give a name to each task. That way, confronted with a mathematical problem, you will be able to look down the short menu of common tasks to decide which one is applicable to your circumstance. Even better, once the task has a name, you can tell a computer to do it for you. Here are some common mathematical tasks that you’ve already learned about: Given a function and specific values for the inputs, apply the function to the inputs to produce an output. Another name for this is to evaluate a function on inputs. Given a function and the name of a with-respect-to input, construct a new function that is the derivative of the given function. The name for this task is to differentiate the function. Like (2), given a function and the name of a with respect to input, anti-differentiate the function. Given a function and an interval of the domain of that function, accumulate the function on that interval. This is named to integrate the function on the interval. (You may recognize that you can perform this task by breaking it down into task (3) and then applying task (1) to the result. That is, \\(\\int_a^b f(t) dt = F(b) - F(a)\\).) We’ll focus on these new operations on functions that you may not yet have mastered. Given a function and an output value from the function, find values for an input (or inputs) which will generate that output value. This is the solving task. A closely related task is zero-finding, which is to find an input that will cause the function to produce the output zero. Given a function and an interval of the domain, find an input value that will produce an output value that’s higher than would be produced for nearby inputs. As you might recognize, this is called finding an argmax. The problem of finding an argmin is exactly the same kind of problem, and can be solved by finding the argmax of the negative of the function. Given a function and an input value, iterate the function to produce a new input that will be better than the original for some purpose. These seven tasks allow you to perform a huge variety of the important mathematical work of extracting useful information from a model you’ve constructed of a situation of interest. Human judgement and creativity is needed to construct the model. And judgement and experience is needed to figure out which tasks to perform and in what order. But carrying out the tasks does not require judgement, experience, or creativity. Performing the tasks requires only an algorithm and the tools to step through the algorithm. Computers are excellent for this; you just have to give them the function and whatever additional input is required (e.g. the name of a with-respect-to-variable), and then tell the computer which task it is to perform. 27.1 Task: Solve Starting materials: a function \\(f(x)\\), a known output value \\(v\\), and a candidate for a suitable input value \\(\\color{brown}{x_0}.\\) Ideal result from the algorithm: A new candidate \\(\\color{magenta}{x^\\star}\\) such that \\(f(\\color{magenta}{x^\\star}) = v\\) or, equivalently, that \\[\\left|\\strut f(\\color{magenta}{x^\\star}) - v \\right| = 0\\ .\\] Realistic result from the algorithm: The new candidate \\(\\color{magenta}{x^\\star}\\) will be better than \\(x_0\\), that is, \\[ \\left|\\strut f(\\color{magenta}{x^\\star}) - v\\right|\\ \\ {\\mathbf &lt;}\\ \\ \\left|\\strut f(\\color{brown}{x_0}) - v\\right|\\] Easy case: When \\(f(x)\\) is a straight-line function, that is \\(f(x) \\equiv \\line(x) = a x + b\\), the solution can be found by simple arithmetic: \\[a x^\\star + b - v = 0 \\ \\ \\implies \\ \\ \\ x^\\star = \\frac{b-v}{a}\\] Approximation strategy: If \\(f(x)\\) is not a straight-line function, approximate \\(f()\\) by a straight-line function. The approximate function will be \\(\\hat{f}(x) = f(x_0) + f&#39;(x_0) \\left[\\strut x - x_0 \\right]\\).1 Then find the easy-case result for \\(\\hat{f}\\). The easy case is … well, easy. We’ll plug in the desired answer, \\(x^\\star\\) (which we don’t yet know), and solve. \\[f(x_0) + f&#39;(x_0) \\left[\\strut x^\\star - x_0 \\right] = v\\] implying \\[\\left[\\strut x^\\star - x_0 \\right] = \\frac{v - f(x_0)}{f&#39;(x_0)}\\] That is, \\[x^\\star = x_0 + \\frac{v-f(x_0)}{f&#39;(x_0)}\\] Figure 27.1: Calculation of \\(x^\\star\\) seen graphically. The brown function is approximated as a straight-line function at the initial point \\(x_0\\). The resulting \\(x^\\star\\) is where that straight line crosses the value \\(v\\) on the output scale. Here, \\(x^\\star\\) is a little to the left of the actual place where \\(f()\\) crosses \\(v\\). Example 27.1 For the function \\[f(x) \\equiv x^2 - x\\] find \\(x^\\star\\) such that \\(f(x^\\star) = 4\\), that is, \\(v=4\\). We’ll start with a guess: \\(x_0 = 3\\). Note that the guess is not the answer: \\(f(3) = 9 - 3 = 6 \\neq 4\\). We can compute \\(f&#39;(x)\\) in the standard way: \\[f&#39;(x) \\equiv \\partial_x f(x) = 2 x - 1\\], giving \\(f&#39;(x_0) = f&#39;(3) = 6 - 1 = 5\\). Now plug the components \\(x_0 = 3\\) (our guess), \\(f(x_0) = 6\\) (the value of \\(f()\\) at our guess), \\(v=4\\), and \\(f&#39;(x_0) = 5\\) into the formula for \\(x^\\star\\). \\[x^\\star = 3 + (4-6)/5 = 3 - 2/5 = 2.6\\ .\\] We have our answer! We can confirm that \\(x^\\star\\) is the answer by plugging it in to \\(f()\\): \\[f(2.6) = 2.6^2 - 2.6 = 4.16\\ .\\] Wait a minute! The idea was to find \\(x^\\star\\) such that \\(f(x^\\star) = 4\\). We didn’t quite do that. Our proposed candidate, \\(x^\\star = 2.6\\) is almost right, but not exactly right. The traditional math instructor would say, “\\(x^\\star = 2.6\\) is a wrong answer.” In response, the student might look hard for what he or she has done wrong. But we will take a gentler point of view. \\(x^\\star = 2.6\\) is a good effort, a step forward. Even though \\(f(x^star) = 4.16\\), which is not exactly what the problem asked for, \\(x^\\star = 4.16\\) is much better than our initial guess \\(x_0 = 3\\) which produced \\(f(x_0) = 6\\). That is, \\[\\underbrace{\\left|\\strut \\overbrace{f(x^\\star)}^{4.16} - v \\right|}_{\\left|4.16 - 4\\right| = 0.16}\\ \\ &lt;\\ \\ \\underbrace{\\left|\\strut \\overbrace{f(x_0)}^6 - v \\right|}_{\\left|6 - 4\\right|= 2}\\ .\\] Although we haven’t completed the task, we’ve gotten closer. In much the same way, the task of digging a well needs to start with the first shovelful of dirt removed from ground: It’s a start! One excellent advantage of this approach to \\(x^\\star\\) is the ease with which it can be translated to instructions for a computer. We’ll write such a function here. We don’t expect you to understand every detail, but hopefully you can see an outline of what’s going on: solve_step &lt;- function(tilde, v, x0) { f &lt;- makeFun(tilde) df &lt;- D(tilde) xstar &lt;- x0 + (v-f(x0))/df(x0) return(xstar) } Now that the computer knows the algorithm, we can easily test it out: f &lt;- makeFun(x^2 - x ~ x) xstar &lt;- solve_step(f(x) ~ x, v=4, x0=3) xstar       2.6 Confirming that xstar is the answer … f(xstar) # Should be 4       4.16 Not exactly the right answer, but a good effort. By the way, now that we have solve_step(), which makes a good effort We can try making another good effort by plugging in xstar in place of our initial guess x0. That is, xstar &lt;- solve_step(f(x) ~ x, v=4, x0=xstar) xstar       2.5619 Do we have the answer now? f(xstar) # should be 4       4.0015 Not quite. But it looks like we’re close. Example 27.2 Calculate \\(\\sqrt[3]{6}\\) using the solve_step() method. \\(\\sqrt[3]{6}\\) is the solution to the problem traditionally framed as “Solve for \\(x\\) in \\(x^3 = 6\\).” In the function-oriented approach solving described in this section, we have \\[f(x) \\equiv x^3 \\ \\ \\text{and} \\ \\ \\ v=6\\] We’ll start with a guess: \\(x_0 = 2\\). (This is motivated by noting \\(2^3 = 8\\), which is pretty close to the desired \\(v=6\\).) f &lt;- makeFun(x^3 ~ x) xstar &lt;- solve_step(f(x) ~ x, v=6, x=2) xstar       1.8333 f(xstar)       6.162 Taking another shovel of dirt out of the well that we’ve started to dig … xstar &lt;- solve_step(f(x) ~ x, v=6, x=xstar) xstar       1.8173 f(xstar)       6.0014 Let’s briefly consider the process of finding \\(x^\\star\\) from a slightly different perspective. \\[x^\\star = x_0 + \\underbrace{\\ \\ \\frac{v-f(x_0)}{f&#39;(x_0)}\\ \\ }_{\\text{&quot;Newton step&quot;}}\\] We can think of \\(x^\\star\\) as being the result of taking a step from the starting point \\(x_0\\) that leads to the destination. This is often called a Newton step since Newton was an early author of the method. Overall, the process of taking successive Newton steps to get closer and closer to the goal is called the Newton-Raphson method. Example 27.3 The process we’ve presented here is not guaranteed to work. By exploring cases where it fails, computational mathematicians2 have developed strategies for increasing the range of situations for which it works. As an example of a difficult situation, consider \\(x_0\\) accidentally picked close to an argmax, that is \\(f&#39;(x_0) \\approx 0\\). (The situation is illustrated in Figure 27.2.) The length of the Newton step is proportional to \\(1/f&#39;(x_0)\\), which is large when \\(f&#39;(x_0) \\approx 0\\). Such a Newton step can produce \\(x^\\star\\) further from the actual solution rather than closer to it. Figure 27.2: An unlucky choice (magenta) of \\(x_0\\) near a local maximum has resulted in a Newton step that is too long. The Newton step creates \\(x^\\star \\approx -3\\), far to the left of the actual zero crossing which is near \\(x\\approx 0\\). This understanding can be integrated into the solve_step() algorithm in several ways A simple modification is to take not a full Newton step, but a fraction of one. Such modifications, including more sophisticated ones that monitor progress toward the goal, can involve elaborate computer programming. Since this book is not about programming per se, we won’t head down this path. But we do point out that better, more reliable, professional-level computer functions for finding \\(x^\\star\\) are available, and you should use them. The R/mosaic function Zeros() incorporates several good practices, including attempting to make a good guess for \\(x_0\\) automatically. To use findZero(), you construct a function \\(g(x) \\equiv f(x) - v\\), and findZero() will produce a good \\(x^\\star\\) or, even better, a set of them. Illustrating with a previous example: g &lt;- makeFun(x^3 - 6 ~ x) xstar &lt;- findZeros(g(x) ~ x, domain(x=c(1,6))) xstar       1.8171 How well did this work? with(xstar, g(x)) # should be zero       -2e-04 Solving a function with two or more inputs can be done in an analogous manner. Pick an initial point \\((x_0, y_0, ...)\\) and write a first-order polynomial approximation, set it equal to \\(v\\), and find \\(x^\\star, y^\\star, ...\\). \\[g(x, y, ...) \\approx g(x_0, y_0) + \\partial_x g(x_0, y_0)\\, \\left[x^\\star - x_0\\right] + \\partial_y g(x_0, y_0)\\, \\left[y^\\star - y_0\\right] + \\cdots = v\\] This is a somewhat daunting jumble of symbols, so the “find \\(x^\\star, y^\\star, ...\\)” may seem easier said than done. And many students, with an experience solving systems of multiple equations, may start out thinking that no solution can be found with just one equation. But, in reality, there is an infinite number of solutions. For instance, you could set all but one of the starred quantities to zero, leaving you with one linear equation in one variable: easy! In Block 5, Section ?? we’ll return to this problem of picking suitable values for \\((x^\\star, y^\\star, ...)\\) in a framework that dramatically simplifies the notation and points to a solution where the starred values can be as small as possible. 27.2 Task: Argmax The task of finding the input value that corresponds to a local maximum is called argmax finding. We don’t need to know the value of the local maximum to solve this problem. Instead, we designate a locale by specifying an initial guess \\(x_0\\) for the argmax. For argmax finding of an objective function \\(f(x)\\), we seek a \\(x^\\star\\) such that \\(f(x^\\star) &gt; f(x_0)\\). To accomplish this, we’ll approximate \\(f(x)\\) with a low-order polynomial, as we so often do. We’ll call the approximation \\(\\widehat{f(x)}\\). In the solving task, the approximation was with a first-order polynomial. But first-order polynomials—that is, straight-line functions—don’t have a local argmax. We need to use a second-order polynomial. Easy enough: construct the second-order Taylor polynomial around \\(x_0\\): \\[\\widehat{f(x)} \\equiv f(x_0) + f&#39;(x_0) \\left[x - x_0\\right] + \\frac{1}{2} f&#39;&#39;(x_0) \\left[x-x_0\\right]^2\\] Remember that \\(f(x_0)\\), \\(f&#39;(x_0)\\) and \\(f&#39;&#39;(x_0)\\) are all fixed quantities; they don’t depend on \\(x\\). To find the argmax of \\(\\widehat{f(x)}\\), differentiate it with respect to \\(x\\) and find the zero of the derivative: \\[\\partial_x \\widehat{f(x)} = f&#39;(x_0) \\underbrace{\\partial_x\\left[x - x_0\\right]}_{1} + \\frac{1}{2} f&#39;&#39;(x_0) \\underbrace{\\partial_x\\left[x-x_0\\right]^2}_{2 \\left[x - x_0\\right]} = 0 \\] Giving \\[f&#39;(x_0) + f&#39;&#39;(x_0) \\left[x - x_0\\right] = 0\\ .\\] As with all solving-task problems, we seek \\(x=x^\\star\\) that will make the above statement true. This is \\[x^\\star = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}\\ .\\] In other words, our new guess \\(x^\\star\\) will be a step away from the old guess \\(x_0\\), with the step being \\(-f&#39;(x_0) / f&#39;&#39;(x_0)\\). Use the R/mosaic argM() function to find argmaxes and argmins. Like other R/mosaic calculus functions, the first argument is a tilde expression defining the objective function. The second argument is the domain to search. To illustrate, the following code creates a randomly shaped function (displayed in Figure 27.3) and calls argM() to generate the argmaxes and argmins. f &lt;- rfun(~ x, 3215) Xstar &lt;- argM(f(x) ~ x, domain(x = -5:5)) Xstar x .output. concavity 1.028013 -26.52652 1 -2.950761 3.79984 -1 slice_plot(f(x) ~ x, domain(x=-5:5)) %&gt;% gf_point(.output. ~ x, data = Xstar, color=&quot;magenta&quot;, size=4, alpha=0.5) Figure 27.3: Illustrating the use of simple_argM() to find argmaxes and argmins. Notice that simple_argM() identified both a local maximum and a local minimum, that is, one argmax and one argmin. Visually, it’s easy to tell which one is which. In terms of the data frame returned by simple_argM(), the sign of the concavity does the identification for you: positive concavity points to an argmin, negative concavity to an argmax. The name argM(), of course, refers to this versatility of finding both argmins and argmaxes. 27.3 Task: Iterate In everyday language, to iterate means simply to repeat: to do something over and over again. In mathematics and in computing, “iterate” has a more specific meaning: to repeatedly perform an operation, each time taking the output from the previous round as the input to the current round. For our purposes, it suffices to define iteration in terms of the use of a function \\(g(x)\\). The function must be such that the output of the function can be used as an input to the function; the output must be the same kind of thing as the input. The iteration starts with a specific value for the input. We’ll call this value \\(x_0\\). Iteration then means simply to compose the function with itself starting with \\(x_0\\) as the initial input. Here, for instance, is a four-step iteration: \\[g(g(g(g(x_0))))\\] Or, you might choose to iterate for ten steps: \\[g(g(g(g(g(g(g(g(g(g(x_0))))))))))\\] However many iteration steps you take, the output from the final step is what you work with. Iteration is useful when you have constructed \\(g()\\) to correspond to a mathematical task you need to perform, especially the solve-task or the argmax-task. \\(g()\\) should be made so that the output is a better answer than the input. To illustrate, consider the solve-task, when we are trying to find an input value to a given function that will produce an output of some desired level, which we’ll call \\(v\\). Earlier, we developed the idea of a Newton step for solving a function \\(f()\\): \\[g(x) \\equiv \\underbrace{x}_{\\text{starting value}} + \\underbrace{\\frac{v - f(x)}{f&#39;(x)}}_{\\text{Newton step}}\\] The output of \\(g()\\) is designed to be better than the input. The idea here is to use the derivative of \\(f()\\) in order to tweak an initial guess for the solution to be a little better. A Newton step is actually a rather aggressive proposal for improving the starting value. Like many forms of aggression, it can sometimes get you exactly the opposite of what you sought. So in practice, it may be better to take only part of a Newton step. Here is a function that creates another function that gives the Newton step for a specified \\(f()\\) and \\(v\\): NewtonStepFun &lt;- function(tilde, v) { f &lt;- makeFun(tilde) fprime &lt;- D(tilde) # create a new function that calculates the Newton step for # this f() and this v, at a specified input x. function(x) { (v - f(x))/fprime(x) } } To illustrate, let’s make an example function \\(f(x)\\equiv \\left[x^2 - 3x\\right] \\left[1.3 + \\pnorm(x)\\right]\\) (Figure @ref(fig:nstep-example1}). Figure 27.4: An example function \\(f(x)\\) which we seek to solve for an output of 5. We seek to solve \\(f()\\) for an output of 5, that is, we are looking for an input \\(x^\\star\\) where \\(f(x^\\star) = 5\\). You can see the two solutions in Figure 27.4: for an input near \\(x=-1\\) the output is close to 5, as it is for an input of \\(x=3.5\\). Supposed you were given an initial guess \\(x_0 = -2\\). You can see from the graph that a step of about length 1 to the right will bring you close to where the output of the function will be 5. Likewise, for an initial guess of \\(x_0 = 4\\), you would want to take a step to the left of length about 0.5. That’s what a Newton step is supposed to do, bring you closer to the correct answer. Let’s construct a function that gives us the Newton step for any starting point \\(x\\), and test it out step_for_f &lt;- NewtonStepFun(f(x) ~ x, v = 5) step_for_f(-2) # should be about + 1       0.9436 step_for_f(4) # should be about - 0.5       -0.3652 This is a good match to the two crossings of \\(v=5\\) in Figure 27.4 Now that we have an expression for the step, we package it up into the function \\(g()\\) to be used for the iteration and then iterate \\(g()\\) from a starting guess. g &lt;- function(x) x + step_for_f(x) g(-2)       -1.0564 g(g(-2))       -0.8702 g(g(g(-2)))       -0.8661 g(g(g(g(-2))))       -0.8661 g(g(g(g(g(-2)))))       -0.8661 The first iteration brought us from the \\(x_0 = -2\\) starting point a little closer to the solution. A double iteration brought us further to the right. the successive iterations show little or now change from the output to the input, a good sign that we’ve done enough work and can declare ourselves done. Of course, you always want to make sure that the answer gotten, \\(x^\\star = -0.8661\\) is right. For this, we compute \\(f(x^\\star)\\), f(-0.8661108) # should be 5       5 Other than the exercises in this chapter, you will not have much need in this course to build your own functions for iterative improvement. But it is the mathematical technology behind the operators that perform the solving-task and the argmax-task. 27.4 Software for the tasks Evaluation of a function—number one in the list at the head of this chapter—is so central to the use of computer languages generally that every language provides a direct means for doing so. In R, as you know, the evaluation syntax involves following the name of the functions by a pair of parentheses, placing in those parenthesis the values for the various arguments to the function. Example: log(5) The other six operations on functions listed above, there is one (or sometimes more) specific R/mosaic functions. Every one of them takes, as a first argument, a tilde expression describing the function on which the operation is to be formed; on the left side is a formula for the function (which can be in terms of other, previously defined functions), on the right side is the with-respect-to variable. Differentiate D(). Returns a function. Anti-differentiate antiD(). Returns a function. Integrate: Integrate(). Returns a number. Solve Solve(). Returns a data frame with one row for each solution found. Argmax argM() Finds one argmax and one argmin in the domain. local_argM() looks for all the local argmaxes and argmins. Returns a data frame with one row for each argmax or argmin found. Iterate Iterate(). Returns a data frame with the value of the initial input and the output after each iteration. Each of operations 4-6 involves the specification of a domain. For Integrate(), this is, naturally, the domain of integration: the upper and lower bounds of the integral For Solve() and argM() the domain specifies where to search for the answer. Iterate() is slightly different. After the tilde expression comes an initial value \\(x_0\\) and then n= which you use to set the number of times to iterate. Another two R/mosaic functions that you frequently use generate graphics. slice_plot() contour_plot() These always take two main arguments: a tilde expression describing the function to be graphed and a plotting domain. Finally, there is one function for plotting variables from data frames: gf_point(). Again, the first argument is a tilde expression specifying two of the data frame’s variables: the variable on the left side will be assigned to the vertical axis, the variable on the right side to the horizontal axis. 27.5 Algorithmic techniques The key steps in optimization are setting up the objective function(s) and setting constraints as needed to represent the problem at hand. There are many ways to perform the work to extract the argmax once the objective function and constraints are set. Understandably, calculus textbooks tend to emphasize techniques based on finding an input where the derivative of the objective function is zero. For problems involving multiple inputs, the task is to find an input where the gradient vector is zero. Contemporary work often involves problems with tens, hundreds, thousands, or even millions of inputs. Even in such large problems, the mechanics of finding the corresponding gradient vector are straightforward. Searching through a high-dimensional space, however, is not generally a task that can be accomplished using calculus tools. Instead, starting in the 1940s, great creativity has been applied to develop algorithms with names like linear programming, quadratic programming, dynamic programming, etc. many of which are based on ideas from linear algebra such as the qr.solve() algorithm for solving the target problem, or ideas from statistics and statistical physics that incorporate randomness as an essential component. An entire field, operations research, focuses on setting up and solving such problems. Building appropriate algorithms requires deep understanding of several areas of mathematics. But using the methods is mainly a matter of knowing how to set up the problem and communicate the objective function, constraints, etc. to a computer. Purely as an example, let’s examine the operation of an early algorithmic optimization method: Nelder-Mead, dating from the mid-1960s. (There are better, faster methods now, but they are harder to understand.) Nelder-Mead is designed to search for maxima of objective functions with \\(n\\) inputs. The video shows an example with \\(n=2\\) in the domain of a contour plot of the objective function. Of course, you can simply scan the contour plot by eye to find the maxima and minima. The point here is to demonstrate the Nelder-Mead algorithm. Start by selecting \\(n+1\\) points on the domain that are not colinear. When \\(n=2\\), the \\(2+1\\) points are the vertices of a triangle. The set of points defines a simplex, which you can think of as a region of the domain that can be fenced off by connecting the vertices. Evaluate the objective function at the vertices of the simplex. One of the vertices will have the lowest score for the output of the objective. From that vertex, project a line through the midpoint of the fence segment defined by the other \\(n\\) vertices. In the video, this is drawn using dashes. Then try a handful of points along that line, indicated by the colored dots in the video. One of these will have a higher score for the objective function than the vertex used to define the line. Replace that vertex with the new, higher-scoring point. Now you have another simplex and can repeat the process. The actual algorithm has additional rules to handle special cases, but the gist of the algorithm is simple. 27.6 Newton’s method NEED A GRAPHIC OR SUCH to introduce the method. iterative technique for finding the zeros of a function \\(f(x)\\), that is, finding a value \\(x^\\star\\) such that \\(f(x^\\star) = 0\\). The method, called Newton’s method, involves making a starting guess \\(x_0\\) for the location of the zero and then refining this guess according to the famous formula \\[x_{i+1} = x_i - \\frac{f(x_i)}{\\partial_x f(x_i)}\\] We can use this method to find, for instance, \\(\\sqrt{10}\\). We do this by creating a function that’s easy to calculate which has a zero at \\(x^\\star = \\sqrt{10}\\): \\[f(x) \\equiv x^2 - 10\\] Finding \\(\\partial_x f(x)\\) is easy, so the famous formula becomes, for the special case of our \\(f()\\) \\[x_{i+1} \\equiv N(x_i) \\equiv x_i - \\frac{x_i^2 - 10}{2 x_i}\\] We’ve called the dynamics function \\(N()\\), in honor of Newton. Applying \\(N()\\) to \\(x_i\\) is called “taking a Newton step.” Start with a very rough guess for \\(x^\\star\\), say \\(x_0 = 1\\). Applying the formula once gives \\(x_1 = 1 - -9/2 = 5.5\\). Using the sandbox, implement the function \\(N(x)\\). Apply it once to \\(x_0=1\\) to make sure that you have the dynamics right. N &lt;- makeFun( 0 ~ x) x_0 &lt;- 1 N(x_0) # this is x_1 ## [1] 0 N(N(x_0)) # this is x_2 ## [1] 0 Question A Using \\(N()\\) as the dynamics and starting with \\(x_0 = 1\\), what is \\(x_5\\)? 5.5︎✘ That’s \\(x_1\\). 3.659091︎✘ That’s \\(x_2\\). 3.141593︎✘ That’s \\(\\pi\\), which incidentally is \\(\\neq \\sqrt{10}\\). 3.196005︎✘ That’s \\(x_3\\). 3.162456︎✘ That’s \\(x_4\\). 3.162354︎✘ 3.162278Good.  Question B Modify N() to find \\(\\sqrt{20}\\). Starting at \\(x_0=1\\), how many times do you have to apply your new N() to get an answer right to within 1% of the true number? After 2 steps we get 4.202︎✘ No, after 2 steps you would get 6.202, which is about 40% away from the true answer. After 3 steps we get 4.713.︎✘ That’s about 6% away from the true number. After 3 steps we get 4.472.︎✘ That’s not the right answer for \\(x_3\\). After 4 steps we get 4.472.︎✘ That’s not the right answer for \\(x_4\\). After 4 steps we get 4.478.Excellent! Right. A bit closer than 1% to the true answer. Question C Modify your N() once again to find \\(\\sqrt[3]{10}\\). (That is, the cube-root of 10.) Starting at \\(x_0 = 1\\), take 3 Newton steps. What is \\(x_3\\)?     2.154︎✘ That’s \\(x_{5}\\).       2.320\\(\\heartsuit\\ \\)       2.875︎✘ That’s \\(x_2\\).       2.912︎✘ 27.7 Exercises CONSTRUCT SOME NEWTON STEPS by hand Pick an initial guess by graphing the function. Dimension of \\(-f&#39;(x_0) / f&#39;&#39;(x_0)\\) Create a Newton step for optimization Know the arguments to the functions D(), antiD(), Solve(), argM(), and Iterate(), and be able to use them. We’re writing \\(f&#39;(x_0)\\) to stand for the more wordy version: \\(\\partial_x f(x=x_0)\\)/↩︎ A traditional name for such a person is “numerical analyst.”↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
