[{"path":"index.html","id":"vectors","chapter":"1 Vectors","heading":"1 Vectors","text":"Chapter yet released.now, presentation calculus featured functions, sometimes expressed formulas involving combinations basic modeling functions, sometimes generated directly data smoothing splines. Now turn new framework expressing functions, inputs operate, kind outputs generate.framework central technical work huge range fields. usual name given mathematicians linear algebra, although word “linear” conveys useful information subject. physicists developing first workable quantum theory called matrix mechanics. framework fundamental scientific computation often approach choice even non-linear problems. Application framework problems information access spark ignited modern era search engines.Although words “algebra” “quantum” may suggest conceptual difficulties store, fact human intuition well suited establishing useful understanding.introduce framework two different equivalent forms. geometric form readily grasped many students, even modest mathematical background. geometry easily grasped mathematicians antiquity, might even regarded trivial beneath notice. also possibly understood many important applications form became apparent last half 20th century emergence statistics, data science, computing generally. can fairly said important developments computer technology stem algorithms developed geometric properties form., equivalent way describing mathematics via simple arithmetic numbers. form lacks concreteness visibility geometric approach, makes ways makes mathematics amenable exact computation extending applications ways transcend limitations human, geometrical intuition.","code":""},{"path":"index.html","id":"length-direction","chapter":"1 Vectors","heading":"1.1 Length & direction","text":"vector mathematical idea deeply rooted everyday physical experience. vector simply object consisting length direction.pencil good physical metaphor vector, pencil , non-vector qualities diameter, color, eraser. , physical object, pencil always position: place ’s .\nFigure 1.1: Three pencils, just two vectors. yellow blue pencils length direction, exactly vector. Pencils position, vectors don’t. green pencil shares direction, different length, different vector blue/yellow vector.\ncourse, pencil tip tail; eraser affixed tail.\nFigure 1.2: Two different vectors. length parallel, point opposite directions.\nVectors always embedded vector space. physical stand-ins vectors, pencils, photographed table top: two-dimensional space. Naturally, pencil-vectors also embedded everyday three-dimensional space. table-top can thought representation two-dimensional subspace three-dimensional space.Pencils useful visualizing vectors, ’s helpful conceptualize step displacement sense “step left” “step forward.” individual vector step specific length particular direction. Much mathematics vectors can understood constructing instructions reaching target: “take three half steps along green vector, turn take two steps backwards along yellow vector.”Vectors embedded three-dimensional space central physics engineering. Quantities force, acceleration, velocity properly represented simple numerical quantities vectors magnitude (, length) direction. statement like, “plane’s velocity 450 miles per hour north-north-west” perfectly intelligible people, describing magnitude direction. Note vector velocity can understood without know plane located; vectors two qualities magnitude direction. Position irrelevant describing velocity, , matter, force acceleration.gradients studied partial differentiation (Chapter 24) vectors. gradient’s direction points directly uphill given point; ’s magnitude tells steep hill point.Vectors practical tool many situations relative motion. Consider problem finding aircraft heading speed intercept another plane ’s also moving. US Navy training movie 1950s shows calculations used done paper pencil.Nowadays relative motion calculations computerized. may well wonder computer able represent vectors, since pencils aren’t part computer hardware. answer disappointingly simple: properties direction magnitude can also represented set numbers. Two numbers vector embedded two-dimensional space, three vector embedded three-dimensions.Representing vector set numbers requires imposition framework: coordinate system. Figure 1.3, vector (, green pencil) placed coordinate system. Usually expect labels coordinate lines, labelling necessarily show vector even needed specify position. two coordinates assigned vector difference tip tail. figure, 20 units horizontally 16 units vertically, vector \\((20, 16)\\).\nFigure 1.3: Representing vector set numbers requires reference coordinate system, shown graph paper.\nconvention, write vector set coordinate numbers, write numbers column. instance, vector Figure 1.3, ’ll call \\(\\overset{\\longrightarrow}{\\text{green}}\\), written:\\[\\overset{\\longrightarrow}{\\text{green}} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\]\nnotation intended human reader, perhaps clearly distinguish vector coordinate description position \\((20,16)\\). ’s also useful carry idea “column” computer software, including computer data structure connecting numbers —20 16 —description “shape” vectors. convention say \\(\\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\) \\(2 \\times 1\\) shape, printed 2 rows one column. vector like velocity three numerical components, \n\\[\\vec{v} = \\left[\\begin{array}{c}v_x\\\\v_y\\\\v_z\\end{array}\\right]\\ .\\] components might function time, \\[\\partial_t \\vec{v}(t) = \\left[\\begin{array}{c}\\partial_t v_x(t)\\\\\\partial_t v_y(t)\\\\\\partial_t v_z(t)\\end{array}\\right] = \\vec{(t)}\\ ,\\]\n\\(\\vec{}(t)\\) vector acceleration.numerical representation vectors convenient need perform mathematical operations vectors addition, subtraction, reversal, scaling. instance, object velocity vector \\(\\vec{v}\\) subjected force vector \\(\\vec{f}\\), velocity vector change time: \\[\\partial_t \\vec{v} = \\frac{1}{m}\\vec{f}\\] (\\(m\\) mass object).Concretely, equation can seen shorthand description derivative three functions, one \\(x\\), \\(y\\), \\(z\\) components \\(\\vec{v}\\), :\n\\[\\partial_t v_x(t) = f_x/m\\\\\n\\partial_t v_y(t) = f_y/m\\\\\n\\partial_t v_z(t) = f_z/m\\ .\\]\ncan apply numerical techniques integration components, instance find velocity future time.numerical representation vectors also way people can understand tremendously important generalization vectors something first seem absurdly abstract: 4-dimensional space, 5-dimensional space, . general: \\(n\\)-dimensional space.","code":""},{"path":"index.html","id":"the-nth-dimension","chapter":"1 Vectors","heading":"1.2 The nth dimension","text":"Living palpably three-dimensional space, part species whose senses brains developed three dimensions, ’s hard maybe even impossible get grasp higher-dimensional spaces like.lovely 1884 book, Flatland features inhabitants two-dimensional world. central character, Square, receives visitor, Sphere, three-dimensional world Flatland embedded. difficulty can Square assemble conception Sphere appearing, growing, vanishing Sphere’s intersection flat world. Square’s attempt convince Sphere three-dimensional world might embedded four-dimensional one leads rejection disgrace.even spatial extent higher dimensions accessible, one-dimensional vector inhabitants space can readily perceived constructed alist numbers. device, allow us introduce vectors 4, 5, 6 dimensions, even \\(n\\) dimensional space.\\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\]Sensible people may consider mathematical tomfoolery promote everyday column numbers vector high-dimensional space, good reason. encourages us think arithmetic vectors terms familiar geometrical concepts: lengths, angles, alignment, . Perhaps unexpectedly, also guides us think data—consists columns numbers data frame—using powerful geometrical intuition.","code":""},{"path":"index.html","id":"geometry-arithmetic","chapter":"1 Vectors","heading":"1.3 Geometry & arithmetic","text":"Chapter 4 apply mathematical apparatus based vectors using data models deeper understanding technological systems radio spectroscopy based sinusoidal oscillations. applied work, may dozens thousands vectors involved. three simple geometric properties lay foundation even advanced techniques:length vector.scaling vector make longer shorter point opposite direction.angle two vectors.can measure length vector ruler. instance, Figure 1.3 shows green vector ruled paper; spacing adjacent lines graph paper 0.2 inches. place index finger eraser little finger tip, hold distance, can bring fingers alignment graph paper see vector roughly 5.2 inches long.Another way find length distance arithmetic. Recall counting squares tip tail, found:\n\\[\\overset{\\longrightarrow}{\\text{green}} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\ .\\]\nApplying Pythagorean theorem, can calculate vector length \\[\\| \\overset{\\longrightarrow}{\\text{green}} \\| =\n0.2\\, \\text {inches} \\sqrt{\\strut 20^2 + 16^2} = \\\\0.2\\, \\text{inches} \\times 25.6125 = \\\\\\strut5.12\\,\\text{inches}\\ .\\]\nnotation \\(\\| \\vec{v} \\|\\) read, “length \\(\\vec{v}\\).”simplicity, generally leave length units numbers specifying vector, \n\\[\\| \\overset{\\longrightarrow}{\\text{green}} \\| =\n\\sqrt{\\strut 20^2 + 16^2} =  25.6125\\ .\\]vector \\(n\\) components, \\[\\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\end{array}\\right]\\ ,\\]\nlength \\(\\|\\vec{w}\\| \\equiv \\sqrt{\\strut w_1^2 + w_2^2 + \\cdots + w_n^2}\\). arithmetic formula length allows us find length even vectors 4- higher-dimensional space, hard pressed find ruler.scale vector \\(\\vec{w}\\) means less change vector’s length. Perhaps better way think think vector step length \\(\\|\\vec{w}\\|\\) vector’s direction. Scaling vector 2 means take two steps, scaling 10 means take 10 steps. Fractional steps backward steps also allowed, scaling \\(\\vec{w}\\) -2.5 means take two half steps backward direction vector points.Arithmetically, scaling vector accomplished simply multiplying vector’s components number. instance, scaling \\(\\vec{w}\\) -2.5 gives:\n\\[-2.5\\, \\vec{w} \\equiv \\left[\\begin{array}{c}-2.5\\,w_1\\\\-2.5\\,w_2\\\\\\vdots\\\\-2.5\\,w_n\\end{array}\\right]\\]\nnumber scaling called scalar, multiplication scalar called scalar multiplication.","code":""},{"path":"index.html","id":"angles","chapter":"1 Vectors","heading":"1.4 Angles","text":"two vectors dimension distinct angle . easily seen two-dimensional vectors. Draw two vectors sheet paper. Since vectors two properties, length direction, mind’s eye can pick one vectors relocate “tail” meet tail vector. letters L V illustrate connection two vectors characters ^, >, <. angle L roughly 90 degrees, characters made vectors acute angles (, less 90 degrees). two vectors  /, brought together / subtend obtuse angle.Measure angle two vectors short way round: 0 180 degrees. larger angle, say 260 degrees, identified circular complement: 100 degrees complement 260 degree angle.2- 3-dimensional spaces, can measure angle two vectors using protractor: arrange vectors tail tail, align baseline protractor one vectors read angle marked second vector.’s also possible measure angle using arithmetic. Suppose vectors \\(\\vec{v}\\) \\(\\vec{w}\\) dimensional space. , \\(\\vec{v}\\) \\(\\vec{w}\\) number components:\\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{array}\\right]\\ \\ \\ \\text{}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\end{array}\\right]\\ ,\\]straightforward arithmetic formula cosine angle \\(\\theta\\) \\(\\vec{v}\\) \\(\\vec{w}\\):\\[\\cos(\\theta) = \\frac{v_1\\, w_1 \\ + \\ v_2\\, w_2 \\ + \\  \\cdots\\  + \\ v_n\\, w_n}{\\sqrt{\\strut v_1^2 + v_2^2 + \\cdots + v_n^2}\\  \\sqrt{\\strut w_1^2 + w_2^2 + \\cdots + w_n^2}}\\]\nmight recognize two quantities denominator ratio lengths \\(\\|\\vec{v}\\|\\) \\(\\|\\vec{w}\\|\\) respectively.’s also special notation name quantity numerator. dot product \\(\\vec{v}\\) \\(\\vec{w}\\) written \\(\\vec{v}\\cdot\\vec{w}\\) sum pairwise products vectors components:\n\\[\\text{dot product:}\\ \\ \\ \\ \\ \\vec{v}\\cdot\\vec{w} \\equiv v_1\\, w_1 + v_2\\, w_2 +  \\cdots + v_n\\, w_n\\ .\\]\nUsing dot-product length notation, can write formula cosine angle two vectors \n\\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\]insist knowing angle \\(\\theta\\) rather \\(\\cos(\\theta)\\), function conversion show Figure 1.4.\nFigure 1.4: \\(\\arccos()\\) function converts \\(\\cos(\\theta)\\) \\(\\theta\\).\nangle \\(\\theta\\) two vectors tell us?geometrical terms, angle tells us strongly aligned vectors . angle 0 tells us vectors point exactly direction, angle 180 degrees means vectors point exactly opposing directions. Either —0 180 degrees—indicates two vectors perfectly aligned. alignment means appropriate scalar multiplication, two vectors made exactly equal one another , consequently, scaled vectors one .Angles 5 175 degrees indicate two vectors mostly aligned, imperfectly. angle 90 degrees course—right angle—two vectors perpendicular.vector alignment particularly important meaning terms data. Suppose two vectors two columns data frame: two different variables. statistics important quantity called correlation coefficient, denoted \\(r\\). say two variables correlated means variables connected one another way. instance, among children, height age correlated. Since height tends increase along age (children), two variables said positively correlated. largest possible correlation \\(r=1\\).negative correlation means one variable increases tends decrease. Temperature elevation negatively correlated, pressure volume gas given temperature. negative possible correlation \\(r=-1\\).zero correlation indicates simple relationship two variables.terms vectors, , columns data frame, correlation coefficient \\(r\\) exactly quantity cosine angle vectors. time correlation coefficient invented 1880s, widely appreciated \\(r\\) simply cosine angle. Perhaps several generations statistics students studied correlation better grasp subject called alignment measured degrees.","code":""},{"path":"index.html","id":"orthogonality","chapter":"1 Vectors","heading":"1.5 Orthogonality","text":"Two vectors said orthogonal angle 90 degrees. everyday speech call 90 degree angle “right angle.” word “orthogonal” really just literal translation “right angle.” syllable “gon” indicates angle, five-angled pentagon six angled hexagon. “Ortho” means “right” “correct,” “orthodox” (right beliefs) “orthodontics” (right teeth) “orthopedic” (right feet).Two vectors right angles—prefer “orthogonal” since “right” many meanings related angles—dot product zero.Example 1.1  Find vector ’s orthogonal \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\).arithmetic trick reverse order components put minus sign front one , \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\).can confirm orthogonality calculating dot product: \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\).Example 1.2  Find vector orthogonal \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\).little scope . simple approach insert zero component new vector use two-dimensional trick fill remaining components.instance, starting \\(\\left[\\strut\\begin{array}{r}0\\\\ __\\\\ __\\end{array}\\right]\\) non-zero components dot product involve 2 3 original vector. \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) orthogonal. , start \\(\\left[\\strut\\begin{array}{r}1\\\\0\\\\3\\end{array}\\right]\\) construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\).addition two vectors constructed arithmetic trick, vector linear combination two vectors also orthogonal \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\).","code":""},{"path":"index.html","id":"exercises","chapter":"1 Vectors","heading":"1.6 Exercises","text":"Write (draw) answers sheet paper. can calculations need sandbox. Make sure write column vectors correct column format. Use SANDBOX numerical calculations.\\(5.42 \\left(\\begin{array}{r}7.3\\\\8.9\\\\-2.4\\end{array}\\right)\\)?\\(-2.67 \\left(\\begin{array}{r}-19.34\\\\0.23\\\\14.82\\end{array}\\right)\\)?Draw vector choice. Next , draw vector 2.5 times vector.Take original vector (c) draw vector -3 times .Write R code create vector named w \\[\\left(\\begin{array}{c}2\\\\5\\\\1\\\\5\\end{array}\\right)\\]Consider two vectors\n\\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\  \\  \\ \\mbox{}  \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right)\n\\]Question length vector \\(\\vec{u}\\)?2︎✘ two elements \\(\\vec{u}\\), ’s “length” vector means.5Good. 25︎✘ length-squared. Take square root find length.None ︎✘ Question B length vector \\(\\vec{w}\\)?2Good. Right: \\(\\sqrt{1^2 + 1^2 + 1^2 + 1^2}\\)4︎✘ four elements \\(\\vec{w}\\)$, square-length \\(\\vec{v}\\) happens 4 (\\(1^2 + 1^2 + 1^2 + 1^2\\)), neither “length” \\(\\vec{w}\\).6︎✘ None ︎✘ Question C \\(w^T \\circ u\\)?7︎✘ 14︎✘ doesn’t exist.Right! Two vectors dimension dot product make sense.\nSuppose \\[\\vec{\\mathbf u} \\equiv  \\left(\\begin{array}{c}4\\\\-3\\end{array}\\right)  \\  \\  \\mbox{}\\ \\ \\vec{\\mathbf v} \\equiv  \\left(\\begin{array}{c}2\\\\2\\end{array}\\right)\\ .\\]\nfollowing questions ask compute length vector given.\n(none available choices exactly right, choose closest.)Question \\(3 \\vec{u}\\)    3︎✘        4︎✘        5︎✘        6︎✘        10︎✘        15\\(\\heartsuit\\ \\)       16︎✘ Question B 42) \\(-2 \\vec{u}\\)    3︎✘        4︎✘        5︎✘        6︎✘        10\\(\\heartsuit\\ \\)       15︎✘        16︎✘ Question C \\(-\\vec{v}\\)    3\\(\\heartsuit\\ \\)Right. ’s \\(\\sqrt{8} = \\sqrt{2^2 + 2^2}\\) precise.       4︎✘        5︎✘        6︎✘        10︎✘        15︎✘        16︎✘ Question D \\(\\vec{u}+ \\vec{v}\\)    3︎✘        4︎✘        5︎✘        6\\(\\heartsuit\\ \\)       10︎✘        15︎✘        16︎✘ Question E \\(2 \\vec{u} - 5 \\vec{v}\\)Collision course?Consider diagram showing two straight-line tracks, dot track, vector.Let’s imagine dot 1 aircraft black vector attached aircraft’s velocity. ’ll call \\(\\vec{v}_1\\), Similarly dot 2, velocity vector called \\(\\vec{v}_2\\).’s third vector drawn red: difference position two aircraft exact moment depicted drawing.question want address whether aircraft collision course. Obviously, two courses cross. know two aircraft cross point. collision, aircraft cross point time.Copy drawing piece paper. don’t need get vectors positions exactly right; reasonable approximation .Now going visual vector addition subtraction answer collision question.relative velocity two planes difference velocities. Subtract \\(\\vec{v}_2\\) \\(\\vec{v}_1\\) draw resulting vector. Pay attention length direction relative velocity.relative velocity two planes difference velocities. Subtract \\(\\vec{v}_2\\) \\(\\vec{v}_1\\) draw resulting vector. Pay attention length direction relative velocity.displacement two planes red vector: position dot 2 subtracted dot 1. Compare directions relative velocity vector displacement vector. aligned, planes collision course.displacement two planes red vector: position dot 2 subtracted dot 1. Compare directions relative velocity vector displacement vector. aligned, planes collision course.picture drawn, relative velocity vector displacement vector aligned. Figure much need change length \\(\\vec{v}_2\\) relative velocity align displacement. (Keep direction .) Draw new vector label clearly “vector intercept.”picture drawn, relative velocity vector displacement vector aligned. Figure much need change length \\(\\vec{v}_2\\) relative velocity align displacement. (Keep direction .) Draw new vector label clearly “vector intercept.”(3) changed length \\(\\vec{v}_2\\) keeping direction . Now going keep \\(\\vec{v}_2\\) original length, change direction new relative velocity aligned displacement vector.(3) changed length \\(\\vec{v}_2\\) keeping direction . Now going keep \\(\\vec{v}_2\\) original length, change direction new relative velocity aligned displacement vector.Items (3) (4) two different ways designing intercept plane 1 plane 2.Bonus) can figure long takes plane reach intersection point finding many multiples velocity vector cover line segment plane’s position intersection point. example, original drawing \\(4 \\vec{v}_1\\) bring plane intersection point, takes 4 “time units” plane reach point. (time unit? velocity miles/hour, time unit hours. velocity feet/second, time unit seconds.) task: Figure aircraft 2 4 time units. tell separation aircraft 2 aircraft 1 1 reaches intersection point. Draw clearly label vector.Copy label vectors onto paper. good approximation ., paper, draw (label Roman numeral) following vector additions subtractions. (“show work” putting vectors added diagram along result addition.)\\(\\vec{} + \\vec{b}\\)\\(\\vec{c} + \\vec{d}\\)\\(\\vec{d} + \\vec{b} + \\vec{}\\)\\(\\vec{b} - \\vec{}\\)\nNote: radio-button multiple choice questions don’t allow us display column vector . Instead, use notation involving superscript T. instance, \\((2, -3)^T\\) column vector ordinarily write \\(\\left(\\begin{array}{c}2\\\\-3\\end{array}\\right)\\) vector written like \\((2, -3)\\), without \\(^T\\), row vector.Suppose \\[\\vec{\\mathbf{u}}  \\equiv \\left(\\begin{array}{c}2\\\\-3\\end{array}\\right) \\ \\ \\hbox{}\\  \\ \\vec{\\mathbf{v}}  \\equiv \\left(\\begin{array}{c}4\\\\1\\end{array}\\right)\\]\nCompute following linear combinations (arithmetically) …Question \\(- \\vec{\\mathbf{u}} -8 \\vec{\\mathbf{v}}\\)\\((-34, -5)^T\\)Right! \\((-34, -5)\\)︎✘ linear combination two column vectors column vector.\\((-34, 15)\\)︎✘ row vector, column vector.\\((34, 15)^T\\)︎✘ Watch arithmetic.Invalid combination︎✘ Question B \\(2.1 \\vec{\\mathbf{u}} -1.3 \\vec{\\mathbf{v}}\\)\\((-1, -7.6)^T\\)Excellent! \\((-1, -7.6)^T\\)Excellent! \\((0.8)\\)︎✘ linear combination two column vectors column vector.\\((0.8)\\)︎✘ linear combination two column vectors column vector.\\((-1, -7.6)\\)︎✘ row vector, column vector.\\((-1, -7.6)\\)︎✘ row vector, column vector.\\((2.1, -1.3)^T\\)︎✘ scalar multipliers linear combination, value linear combination.\\((2.1, -1.3)^T\\)︎✘ scalar multipliers linear combination, value linear combination.Invalid combination︎✘ \nQuestion C \\(-0.5 \\vec{\\mathbf{u}} + 3.2 \\vec{\\mathbf{v}}\\)Invalid combination︎✘ \nQuestion C \\(-0.5 \\vec{\\mathbf{u}} + 3.2 \\vec{\\mathbf{v}}\\)\\((11.8, 4.7)^T\\)Nice! \\((11.8, 4.7)^T\\)Nice! \\((2.7)\\)︎✘ linear combination two column vectors column vector.\\((2.7)\\)︎✘ linear combination two column vectors column vector.\\((13.8, 1.7)\\)︎✘ row vector\\((13.8, 1.7)\\)︎✘ row vector\\((34, 16)^T\\)︎✘ Watch arithmetic.\\((34, 16)^T\\)︎✘ Watch arithmetic.Invalid combination︎✘ \nQuestion D \\(7 \\vec{\\mathbf{u}} + 5 \\vec{\\mathbf{v}}\\)Invalid combination︎✘ \nQuestion D \\(7 \\vec{\\mathbf{u}} + 5 \\vec{\\mathbf{v}}\\)\\((34, -16)^T\\)Nice! \\((34, -16)^T\\)Nice! \\((32, -14)^T\\)︎✘ Check arithmetic!\\((32, -14)^T\\)︎✘ Check arithmetic!\\((34, -14)^T\\)︎✘ Check arithmetic!\\((34, -14)^T\\)︎✘ Check arithmetic!\\((32, -16)^T\\)︎✘ Check arithmetic!\\((32, -16)^T\\)︎✘ Check arithmetic!Invalid combination︎✘ Invalid combination︎✘ exercise, going check proposed solutions target problem. question poses one target problem. One answers correct. Use R SANDBOX select correct answer.vectors working :\n\\[\\vec{} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b}  \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c}  \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d}  \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T}  \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\]Question . linear combination \\(\\vec{}\\) \\(\\vec{b}\\) reach target \\(T\\)?\\(3\\vec{} - 7\\vec{b}\\)︎✘ \\(\\vec{} + 2\\vec{b}\\)︎✘ \\(-4 \\vec{} + 7 \\vec{b}\\)Correct. \\(- \\vec{} + 4\\vec{b}\\)︎✘ combination reach \\(\\vec{T}\\) exactly.︎✘ Two 2-dimensional vectors pointing different directions can reach point plane.None ︎✘ Question B ii. linear combination \\(\\vec{b}\\) \\(\\vec{c}\\) reach target \\(T\\)?\\(\\frac{4}{3}\\vec{b} - \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{7}{3}\\vec{c}\\)︎✘ \\(-\\frac{7}{3}\\vec{b} + \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{5}{3} \\vec{b} + \\frac{4}{3} \\vec{c}\\)Good. combination reach $ exactly.︎✘ Two 2-dimensional vectors pointing different directions can reach point plane.None ︎✘ Question C iii. linear combination \\(\\vec{b}\\) \\(\\vec{d}\\) reach target \\(T\\)?\\(0\\vec{b} - \\frac{3}{2}\\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ \\(0 \\vec{b} - \\frac{1}{2} \\vec{d}\\)Right! \\(\\frac{3}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ combination reach $ exactly.︎✘ Two 2-dimensional vectors pointing different directions can reach point plane.None ︎✘ Question D iv. linear combinations $ec{}$, \\(\\vec{b}\\) \\(\\vec{c}\\) reach target \\(T\\)?\\(\\vec{} -\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{} +\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{} -\\vec{b} + 2\\vec{c}\\)Good. \\(-2\\vec{} +\\vec{b} + 2\\vec{c}\\)︎✘ combination reach $ exactly.︎✘ Two 2-dimensional vectors pointing different directions can reach point plane. Adding third vector just increases number possibilities.None ︎✘ Referring vectors \\(\\vec{}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), \\(\\vec{d}\\) figure, construct linear combinations. diagram show scaled vectors added output linear combination.\\(2 \\vec{} + 1 \\vec{b}\\)\\(1.5 \\vec{c} - 2 \\vec{d}\\)\\(- \\vec{b} + 2\\vec{c} + 3\\vec{d}\\)\nphysics engineering, important operation two vectors \\((, b, c)^T\\) \\((e, f, g)^T\\) called “cross product.” (operation defined vectors 3-dimensional space.) output cross product another 3-dimensional vector can calculated arithmetically :\\[\\left(\\begin{array}{c}\\\\b\\\\c\\end{array}\\right) \\times \\left(\\begin{array}{c}e\\\\f\\\\g\\end{array}\\right) \\equiv\n\\left(\\begin{array}{c}b g - c f\\\\c e - g\\\\f - b e\\end{array}\\right)\\]DD16 Exercise 6 paper.Make coordinates two three-dimensional vectors \\(\\vec{u}\\) \\(\\vec{v}\\) point different directions. carry arithmetic find vector \\(\\vec{w}\\) cross product \\(\\vec{u} \\times \\vec{v}\\).Find angle \\(\\vec{w}\\) \\(\\vec{u}\\).Find angle \\(\\vec{w}\\) \\(\\vec{v}\\).Place dot piece paper. Hold pencil perpendicular paper eraser dot. Now, (using pen!) draw vector paper perpendicular pencil. Now draw 5 vectors, different , perpendicular pencil.Place dot piece paper. Hold pencil perpendicular paper eraser dot. Now, (using pen!) draw vector paper perpendicular pencil. Now draw 5 vectors, different , perpendicular pencil.Draw vector \\(\\vec{c}\\) paper. Now draw another vector paper perpendicular first. Label \\(\\vec{}\\). Find another vector, labeled \\(\\vec{b}\\) lies paper, orthogonal \\(\\vec{c}\\), different direction \\(\\vec{}\\). turn \\(\\vec{} = k \\vec{b}\\). Write numerical value scalar \\(k\\) \\(\\vec{}\\) \\(\\vec{b}\\).Draw vector \\(\\vec{c}\\) paper. Now draw another vector paper perpendicular first. Label \\(\\vec{}\\). Find another vector, labeled \\(\\vec{b}\\) lies paper, orthogonal \\(\\vec{c}\\), different direction \\(\\vec{}\\). turn \\(\\vec{} = k \\vec{b}\\). Write numerical value scalar \\(k\\) \\(\\vec{}\\) \\(\\vec{b}\\).Consider vector \\(\\vec{w} \\equiv (2, 5)^T\\). Using arithmetic, find vector \\(\\vec{z}\\) perpendicular \\(\\vec{w}\\). (Hint: perpendicular, \\(\\vec{z}^T \\cdot \\vec{w}\\) must zero. make two numbers \\(\\) \\(b\\) \\(2 + 5 b = 0\\).) Write \\(\\vec{z}\\) paper.Consider vector \\(\\vec{w} \\equiv (2, 5)^T\\). Using arithmetic, find vector \\(\\vec{z}\\) perpendicular \\(\\vec{w}\\). (Hint: perpendicular, \\(\\vec{z}^T \\cdot \\vec{w}\\) must zero. make two numbers \\(\\) \\(b\\) \\(2 + 5 b = 0\\).) Write \\(\\vec{z}\\) paper.Repeat (c) create another vector \\(\\vec{z}_2\\) different \\(\\vec{z}\\). Write \\(\\vec{z}_2\\) paper. addition, find scalar \\(k\\) \\(\\vec{z}_2 = k \\vec{z}\\).Repeat (c) create another vector \\(\\vec{z}_2\\) different \\(\\vec{z}\\). Write \\(\\vec{z}_2\\) paper. addition, find scalar \\(k\\) \\(\\vec{z}_2 = k \\vec{z}\\).following exercises, asked find numerical vectors perpendicular (“orthogonal”) stated vector. equivalent finding new vector whose dot product stated vector zero. (Except … new vector can’t zeros! -zero vector exceptional direction.) One effective strategy write vector consisting 1 position like zeros elsewhere. Find dot product, calling \\(d\\). select one zeros turn non-zero. value ? Just enough included dot product contributes \\(-d\\).Consider vector \\(\\vec{w} \\equiv (1, 2, -3, 2)^T\\).\nFind vector \\(\\vec{}\\) perpendicular \\(\\vec{w}\\).\nFind another vector \\(\\vec{b}\\) perpendicular \\(\\vec{w}\\) \\(\\vec{}\\). (Hint: can done.)\nCreate yet another vector \\(\\vec{c}\\) linear combination \\(\\vec{}\\) \\(\\vec{b}\\). (can choose whatever combination like, coefficients non-zero.) angle \\(\\vec{c}\\) \\(\\vec{w}\\)?\nConsider vector \\(\\vec{w} \\equiv (1, 2, -3, 2)^T\\).Find vector \\(\\vec{}\\) perpendicular \\(\\vec{w}\\).Find another vector \\(\\vec{b}\\) perpendicular \\(\\vec{w}\\) \\(\\vec{}\\). (Hint: can done.)Create yet another vector \\(\\vec{c}\\) linear combination \\(\\vec{}\\) \\(\\vec{b}\\). (can choose whatever combination like, coefficients non-zero.) angle \\(\\vec{c}\\) \\(\\vec{w}\\)?sent mission 5-dimensional space. task find vector perpendicular \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\). .sent mission 5-dimensional space. task find vector perpendicular \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\). .BONUS. Find many vectors can perpendicular \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\) . arithmetic necessarily easy. give (honor!), figure many mutually perpendicular vectors can 5-dimensional space. 10-dimensional space?BONUS. Find many vectors can perpendicular \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\) . arithmetic necessarily easy. give (honor!), figure many mutually perpendicular vectors can 5-dimensional space. 10-dimensional space?Find lengths vectors. square “graph paper” 1 unit. Assume vectors begin end exactly graph-paper intersections.Question Length vector \\(\\vec{}\\).    2︎✘        2.82︎✘        4︎✘        4.12︎✘        5\\(\\heartsuit\\ \\)       5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question B Length vector \\(\\vec{B}\\).    2︎✘        2.82︎✘        4︎✘        4.12︎✘        5\\(\\heartsuit\\ \\)       5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question C Length vector \\(\\vec{C}\\).    2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66\\(\\heartsuit\\ \\)       6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question D Length vector \\(\\vec{D}\\).    2︎✘        2.82\\(\\heartsuit\\ \\)       4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question E Length vector \\(\\vec{E}\\).    2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32\\(\\heartsuit\\ \\)       8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question F Length vector \\(\\vec{} + 2 \\vec{B}\\).    2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18\\(\\heartsuit\\ \\)       11.66︎✘        12︎✘        12.22︎✘ Question G Length vector \\(\\vec{C} - \\vec{B}\\).    2︎✘        2.82︎✘        4︎✘        4.12\\(\\heartsuit\\ \\)       5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question H Length vector \\(\\vec{C} + \\vec{E}\\).Question \\(\\vec{C}^T\\cdot\\vec{D}\\)?    -8︎✘        0\\(\\heartsuit\\ \\)       6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question B \\(\\vec{E}^T\\cdot\\vec{B}\\)?    -8︎✘        0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30\\(\\heartsuit\\ \\)       34︎✘        40︎✘        42︎✘ Question C \\(\\vec{}^T\\cdot\\vec{E}\\)?    -8︎✘        0︎✘        6︎✘        10\\(\\heartsuit\\ \\)       14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question D \\(\\vec{D}^T\\cdot\\vec{E}\\)?    -8\\(\\heartsuit\\ \\)       0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question E \\(\\left(\\vec{}+\\vec{B}\\right)^T\\cdot\\vec{C}\\)?    -8︎✘        0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40\\(\\heartsuit\\ \\)       42︎✘ Question F \\(\\left(\\vec{C}+\\vec{D}\\right)^T\\cdot\\vec{E}\\)?12 vectors, labeled “” “m.” (Letter “” left .) several quick questions, makes claim whether sum two vectors equals third. Answer true false claim. tricks exactitude, claim close true, answer true.Question 1) \\(\\vec{} + \\vec{b} = \\vec{L}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question B 2) \\(\\vec{b} + \\vec{J} = \\vec{}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question C 3) \\(\\vec{b} + \\vec{m} = \\vec{J}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question D 4) \\(\\vec{c} + \\vec{f} = \\vec{d}\\)    True\\(\\heartsuit\\ \\)       False︎✘ Question E 5) \\(\\vec{k} + \\vec{L} = \\vec{e}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question F 6) \\(\\vec{e} + \\vec{b} = \\vec{m}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question G 7) \\(\\vec{m} + \\vec{g} = \\vec{b}\\)","code":""},{"path":"index.html","id":"outline","chapter":"1 Vectors","heading":"1.7 Outline","text":"Dual representations vectors.\narithmetic: vector column numbers.\ngraphically: arrow.\ndirection magnitude.\nPosition isn’t feature. can draw vector whereever convenient long keep direction magnitude.\n\narithmetic: vector column numbers.graphically: arrow.\ndirection magnitude.\nPosition isn’t feature. can draw vector whereever convenient long keep direction magnitude.\ndirection magnitude.Position isn’t feature. can draw vector whereever convenient long keep direction magnitude.vector embedded (“lives ”) space. dimension space count rows column numbers (arithmetic representation). Graphically, dimension must figured context, since arrows look .Scalar multiplication addition representations.Dot product\ntakes two vectors, produces number\narithmetical operation: componentwise multiply add\ninterpreting graphically\n. proportional (cosine ) angle vectors\ntakes two vectors, produces numberarithmetical operation: componentwise multiply addinterpreting graphically\n. proportional (cosine ) angle vectorsLength vector\narithmetically: pythagorus\narithmetically: sqruare root dot product \ngraphically: ruler.\narithmetically: pythagorusarithmetically: sqruare root dot product itselfgraphically: ruler.","code":"\ngvec(from=c(-2,3), to=c(4,4), color=\"red\") %>%\ngraph_paper(xticks=-5:5, yticks=-5:5) %>%\n    gvec(from=c(0,0), to=c(-2,3))"},{"path":"linear-combs-vectors.html","id":"linear-combs-vectors","chapter":"2 Linear combinations of vectors","heading":"2 Linear combinations of vectors","text":"Chapter yet released.previous chapter, suggested think vector “step” given direction given magnitude: example 1 foot northeast. interpretation highlights mathematical structure vectors: just direction length, nothing else.“step”-interpretation also faithful important reason vectors useful. use steps get one place another. Similarly, central use formalism vectors guide thinking algorithms figuring best get one “place” another. ’ve used quotation marks around “place” necessarily referring physical destination. ’ll get else might mean “place” later chapter.now, let’s focus word “steps”: plural. Generally, get desired destination just one step. instance, pirate’s treasure map might contain instruction, “Dig 10 paces east tall tree.” East direction vector. “Pace” length vector (3 feet). 10 number steps take.pirates wrote vector notation, directions might given like : \\[\\text{Let}\\ \\vec{p} \\equiv \\left[\\begin{array}\\,1\\,\\text{pace}\\\\0\\end{array}\\strut\\right]\\ .\\ \\  \\text{}, \\ \n\\text{treasure spot} \\equiv \\text{tall tree} \\ + 10 \\vec{p}\\ .\\]Treating vector single step, mathematical representation multiple fractional steps scalar multiplies vector. scalar—17 0.36 -1.5—tells length journey reported units 1-step vector . previous paragraph, \\(\\vec{p}\\) single step; \\(10 \\vec{p}\\) 10 steps, taken direction \\(\\vec{p}\\).given vector may point direction want travel. Often, , journey consists stepping direction one vector, stepping direction another vector points different direction.typical journey, can one vector involved. instance, vector pointing hidden cove tall tree—’ll call \\(\\overset{\\longrightarrow}{\\text{tree}}\\)—might identified direction north-west length 1 nautical mile. full directions treasure might , \"Start hidden cove. march \\(3.2\\, \\overset{\\longrightarrow}{\\text{tree}} + 10\\, \\vec{p}\\).journey linear combination vectors. vectors \\(\\vec{p}\\) \\(\\overset{\\longrightarrow}{\\text{tree}}\\). vector \\(3.2\\, \\overset{\\longrightarrow}{\\text{tree}}\\) scaled version \\(\\overset{\\longrightarrow}{\\text{tree}}\\); points direction \\(\\overset{\\longrightarrow}{\\text{tree}}\\), length 3.2 nautical miles. Similarly, \\(10\\, \\vec{p}\\) vector length 10 paces, pointing direction \\(\\vec{p}\\). scale set vectors add , linear combination, just Block 1 created linear combination functions scaling adding results together.Pirates mariners use direction terms like “one point north north-north-east.” maps annotated compass roses translate words direction.\nMathematicians can replace compass rose just two vectors, say, \\(\\overset{\\longrightarrow}{\\text{North}}\\) \\(\\overset{\\longrightarrow}{\\text{East}}\\). directions can given linear combination. instance, compass rose’s “north-north-west” linear combination \\(0.9239\\,\\overset{\\longrightarrow}{\\text{North}} -0.3827\\,\\overset{\\longrightarrow}{\\text{East}}\\).Geometrically, , terms arrows pencils, scaling vector stretching shrinking, without changing direction. Scaling negative number means reverse tip tail vector. linear combination two vectors, choose one vectors start, move tail second vector tip first, Figure 2.1.\nFigure 2.1: Adding two vectors, yellow green, placing tail tip. result vector going tail yellow tip green. resultant equivalent blue vector.\nAdding vectors way takes advantage rootlessness vector. long keep direction length , can move vector whatever place convenient. adding vectors, convenient arrangement place tail second vector tip first. result—blue pencil picture —length direction tail first pencil (yellow) tip second (green). long maintain length direction, can put result (blue) anywhere want.Subtraction similar. \\(\\vec{v} - \\vec{w}\\),\n\\(\\vec{v}\\) \\(\\vec{w}\\) tail--tail. Read result vector running tip \\(\\vec{v}\\) tip \\(\\vec{w}\\). Figure 2.2, yellow vector \\(\\vec{v}\\), blue vector \\(\\vec{w}\\). result subtraction green vector.\nFigure 2.2: Subtracting blue yellow gives green.\n","code":""},{"path":"linear-combs-vectors.html","id":"scaling","chapter":"2 Linear combinations of vectors","heading":"2.1 Scaling","text":"know, scaling vector changes ’s length. scalar negative, vector tip tail swapped. Figure 2.3 shows vector \\(\\vec{v}\\) , dotted line, possible results scaling vector. Three locations dotted line labeled (), (b), (c). vectors create scalar multiplication can placed anywhere like. purpose illustration, imagine vectors create tails tail \\(\\vec{v}\\). vector \\(-\\frac{1}{2} \\vec{v}\\) half long \\(\\vec{v}\\) point exact opposite direction. tip \\(-\\frac{1}{2} \\vec{v}\\) therefore point marked (). Similarly, vector \\(\\frac{4}{5} \\vec{v}\\) tip (b) \\(2.5 \\vec{v}\\) tip (c). point dotted line, numerical scalar \\(\\alpha\\) scaled vector \\(\\alpha \\vec{v}\\) tip point.dotted line, , points can “reached” scaling \\(\\vec{v}\\) called subspace: subspace spanned \\(\\vec{v}\\). can think subspace “destinations” can reached stepping direction \\(\\vec{v}\\). (Steps backward steps also legitimate.)\nFigure 2.3: suitable scalar multiplying vector \\(\\vec{v}\\), point dotted line can reached. dotted line—extended infinity—subspace spanned vector.\nEvery vector associated subspace one-dimensional; can reach points line stepping direction vector.","code":""},{"path":"linear-combs-vectors.html","id":"sub-spaces","chapter":"2 Linear combinations of vectors","heading":"2.2 Sub-spaces","text":"Figure 2.4 includes second vector \\(\\vec{w}\\) along vector \\(\\vec{v}\\) seen Figure 2.3. vector subspace, shown dotted lines.\nFigure 2.4: subspaces spanned two vectors \\(\\vec{v}\\) \\(\\vec{w}\\).\nThings get interesting consider just subspace spanned vectors individually, subspace spanned jointly. Recall subspace spanned \\(\\vec{v}\\) vectors can created scalar multiplication \\(\\vec{v}\\), , vectors \\(\\alpha \\vec{v}\\) \\(-\\infty < \\alpha < \\infty\\).two vectors, subspace vectors can created linear combination two, \\[\\alpha \\vec{v} + \\beta \\vec{w}\\] \\(-\\infty < \\alpha < \\infty\\) \\(-\\infty < \\beta < \\infty\\). start trip, walk point \\(\\alpha \\vec{v}\\) subspace \\(\\vec{v}\\). Now, pick \\(\\vec{w}\\) place tail point just reached, \\(\\alpha \\vec{v}\\). Finally, walk point direction \\(\\vec{w}\\) \\(\\beta\\) steps.adjusting \\(\\alpha\\) \\(\\beta\\) appropriately, can get point plane. words, subspace spanned set vectors \\(\\{\\vec{}, \\vec{b}\\}\\) entire plane.CAN PLACE CALCPLOT3D picture .Figure 2.4, lying two-dimensional surface screen paper, fails indicate clearly even three- higher-dimensional space, two (non-aligned) vectors span plane particular orientation. see better, worthwhile experiment pick two pencils pointing different directions. Place eraser ends together, pinched thumb forefinger. can point whole rigid assembly direction like. angle remain .Place card top pencils, slipping pressed fingers hold tightly place. card another kind geometrical object: planar surface. orientation two vectors together determine orientation surface. simple fact extremely important later .replace pencils line segments drawn card underneath pencil. Now angle readily measurable two dimensions. angle two vectors three dimensions angle drawn two-dimension surface rests vectors.Notice can also lay card along single vector. ’s different can roll card around pencil; many different orientations card even vector stays fixed. single fixed vector determine uniquely orientation planar surface two vectors can reside. two fixed vectors, one surface.","code":""},{"path":"linear-combs-vectors.html","id":"functions-as-vectors","chapter":"2 Linear combinations of vectors","heading":"2.3 Functions as vectors","text":"Section 8.3 looked use exponential function describe temperature hot water cooling room temperature. exponential decreasing function course \\(e^{-kt}\\) can find \\(k\\) estimating half-life exponential decay, 36 minutes data used. (Figure ??).section, ’re going think functions terms vectors. huge advantages thinking way, take time see clearly.Recall data cooling water gave temperature (degrees C) versus time (minutes). Economy page argues showing rows CoolingWater data frame, access SANDBOX.temp column data frame set numbers, hence interpretable vector. vector 15 dimensions, ’s much said direction, length easy enough: take square root sum squares components. comes 251.3 degrees C. merely arithmetic fact, consequence adding together 15 numbers. dimension temperature T, nothing hot-oven-like 251 C temperature.temp vector playing role buried treasure; ’s destination want reach. got reach ?basic modeling function \\(e^{-kt}\\) comes . evaluating \\(e^{-kt}\\) values \\(t\\) time column, create another column, ’ll call expkt. Knowing half-life 36 minutes, ’ll use \\(k=\\ln(2)/36 \\approx 0.02\\). time, anticipating come, ’ll add another column ’ll call, following statistical practice, intercept column represents constant function (evaluated times).Confirm expkt column really match exponential decay half-life 36 minutes. can see time zero value expkt, just expected exponential. time 36, somewhere rows times 29 39, value 0.5. time 72—two half-lives start—value 0.25, closely matching recorded temperature time 74.two vectors expkt intercept, like two (non-aligned) vectors, span planar subspace. Since expkt intercept embedded 15-dimensional space—just saying 15 rows data frame—plane subspace 15-dimensional space. statement can seem hopelessly abstract, let’s try give concrete visualization. visualization, ’ll move onto familiar ground graph temperature versus time.domain temperature vs time, linear combinations \\(\\alpha\\)expkt\\(\\ + \\beta\\)intercept, appears set 15 dots. Figure 2.5 shows three sets 15 three different colors, along 15 points actual temperature data. show three sets dots visually confusing. Instead, ’ll add graph functions form \\(\\alpha\\, e^{-0.02 t} + \\beta\\, 1\\).\nFigure 2.5: possible linear combinations vectors expkt intercept. combination set 15 dots, many shown continuous functions connect dots particular linear combination.\nplot linear combinations expkt intercept, graphics frame completely covered ink. individual vector produced linear combination look much kind ten shown .functions shown Figure 2.5 inhabit two-dimensional subspace spanned expkt intercept. lot 15 dimensional space. functions look like inhabit space outside two-dimensional expkt&intercept subspace? Figure 2.6 shows handful . different kind functions shown Figure 2.5.\nFigure 2.6: handful vectors 15-dimensional space outside 2-dimensional expkt&intercept subspace.\nlot crazy-looking functions 15-dimensional space!Look back Figure 2.5 focus attention function drawn black. function reasonable match data (plotted black dots). question face now find function searching 15-dimensional space. ’s task take next chapter.’s pretty easy visualize length vector arithmetic straightforward even n-dimensional space. vector \\(\\vec{v}\\) components \\[\\vec{v} \\equiv \\left[\\strut \\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{array}\\right]\\] length \\(\\sqrt{\\strut v_1^2 + v_2^2 + \\cdots + v_n^2}\\). Similarly, dot product \\(\\vec{v}\\) \\(\\vec{w}\\) \\(\\vec{v} \\cdot \\vec{w} \\equiv v_1 w_1 + v_2 w_2 + \\cdots + v_n w_n\\). terms dot product, vector length \\(\\sqrt{\\strut\\vec{v} \\cdot \\vec{v}}\\). combining dot products, can calculate angle two vectors: \\[\\cos(\\theta_{v,w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{\\sqrt{\\strut (\\vec{v}\\cdot\\vec{v})\\ (\\vec{w}\\cdot\\vec{w}})}\\]know, vector two properties: length direction. can calculated using dot product.suggests way consider mathematical objects, functions, vectors. need reasonable definition dot product. Suppose two functions, \\(f(t)\\) \\(g(t)\\) defined domain, say \\(0 < t < 2\\pi\\). dot product multiplies objects together accumulates products. vectors sets numbers, accumulation add component--component products. analogy, take dot product \\(f(t)\\) \\(g(t)\\) domain, can integral: \\[f() \\cdot g() \\equiv \\int_0^{2\\pi} f(t) g(t) dt\\ .\\]illustrate, consider two functions: \\(f(t) \\equiv \\sin(t)\\) \\(g(t) \\equiv \\sin(2 t)\\), plotted .\n“long” \\(f(t)\\) \\(g(t)\\)? `       \\(1.7725\\)       \\(1.7725\\)“length”, might recognize \\(\\sqrt{\\strut\\pi}\\).’s cosine angle ?       \\(0\\)cosine angle zero—1.3e-16 just round-error. means functions \\(\\sin(t)\\) \\(\\sin(2t)\\) orthogonal domain \\(0 < t < 2\\pi\\).","code":"\nCW <- CW %>% \n  mutate(expkt = exp(-0.02*time),\n         intercept = 1)Integrate(sin(t) * sin(t) ~ t, domain(t = 0:(2 * pi))) %>% sqrt()Integrate(sin(2 * t) * sin(2 * t) ~ t, domain(t = 0:(2 * pi))) %>% sqrt()Integrate(sin(t) * sin(2 * t) ~ t, domain(t = 0:(2 * pi)))/pi"},{"path":"linear-combs-vectors.html","id":"matrices","chapter":"2 Linear combinations of vectors","heading":"2.4 Matrices","text":"writing linear combinations vectors format, \\(\\) \\(b\\)scalars: \\[\\vec{u} + b \\vec{v}\\ .\\]\n’s helpful standard notation makes evident things combined.matrix collection vectors, live dimensional space. instance, matrix containing \\(\\vec{u}\\) \\(\\vec{v}\\) looks like\n\\[\\left[\\begin{array}{cc}|&|\\\\\\vec{u} & \\vec{v} \\\\|&|\\\\ \\end{array}\\right]\\]\nvertical lines meant indicate \\(\\vec{u}\\) \\(\\vec{v}\\) column matrix. instance, supposing \\[\\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{}\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ ,\\]\nmatrix \\(\\mathbf \\) containing \\(\\vec{u}\\) \\(\\vec{v}\\) consists two columns:\n\\[ {\\mathbf } \\equiv \\left[\\overset{\\strut}{\\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]}\\ \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ \\right]\\]\nbook, write name matrix, ’ll use CAPITAL bold-face letter, M. Conventionally, without interior square brackets, writing\n\\[ {\\mathbf } \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\]scientific computing languages, almost always type object called “matrix” “array.” one shown , \\({\\mathbf }\\) 3 rows 2 columns. (Columns always run vertically, columns building.)Also, matter convention, linear combination vectors matrix indicated placing right matrix column containing coefficients combination \\(3 \\vec{u} + 1 \\vec{v}\\), \\[ {\\mathbf } \\left[\\begin{array}{c}3\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right] \\left[\\begin{array}{c}3\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}7\\\\11\\\\-9\\end{array}\\right]\\]\nR, make vectors rbind() command, places arguments row. ’ll make matrices cbind() command, places arguments column. course, r rbind() stands “row” c cbind() stands “column.”take linear combination vectors matrix writing coefficients style vector multiplying using %*%:Often, ’ll want make several different linear combinations vectors matrix. Since individual linear combination specified column coefficients, convention place several sets coefficients side--side columns, , matrix.example, consider three different linear combinations two vectors \\(\\vec{v}\\) \\(\\vec{w}\\):\n\\[\\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right] \\left[\\begin{array}{rr}3 & 0 & -1\\\\1 & 2 & 0\\end{array}\\right] = \\left[\\begin{array}{r}7 &2 &-2\\\\11 & -8 & -5\\\\-9 & 0 & 3\\end{array}\\right]\\]\nR, left-hand side equation involve constructing matrix three pairs coefficients:CATEGORICAL VARIABLE TRANSLATES SET VECTORS.Example 2.1  Talyor LagrangeIn Chapter 26 met method introduced Brook Taylor (1685–1731) construct polynomial order-\\(n\\) approximates smooth function \\(f(x)\\) close enough center \\(x_0\\). method made use ability differentiate \\(f(x)\\) \\(x_0\\) produced general formula:\n\\[f(x) \\approx f(x_0) + \\frac{f'(x_0)}{1} \\left[x-x_0\\right] + \\frac{f''(x_0)}{2!} \\left[x-x_0\\right]^2 + \\frac{f'''(x_0)}{3!} \\left[x-x_0\\right]^3 + \\cdots + \\frac{f^{(n)}(x_0)}{n!} \\left[x-x_0\\right]^n\\] \\(f'(x_0) \\equiv \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\) .Using polynomials approximating functions important theme mathematics history. Brook Taylor neither first last take problem.1795, Joseph-Louis Lagrange\n(1736 – 1813) published another method constructing approximating polynomial order \\(n\\). Whereas Taylor polynomial builds polynomial exactly matches first \\(n\\) derivatives center point \\(x_0\\), Lagrange polynomial different objective: match exactly values target function \\(f(x)\\) set knots (input values) \\(x_0\\), \\(x_1\\), \\(x_2\\), \\(\\ldots, x_n\\). Figure 2.7 shows situation knots shown orange dots.\nFigure 2.7: Lagrange polynomial order \\(n\\) arranged pass exactly \\(n+1\\) points graph function \\(f(x)\\).\nLagrange polynomial constructed linear combinations functions, one knots. example Figure 2.7, 6 knots, hence six functions combined. knot 2, instance, coordinates \\(\\left(\\strut x_2, f(x_2)\\right)\\) corresponding function :\\[p_2(x) = \\frac{(x-x_1)}{(x_2 -x_1)}\\left[\\strut\\cdot\\right]\\frac{(x-x_3)(x-x_4)(x-x_5)(x-x_6)}{(x_2 -x_3)(x_2 -x_4)(x_2 -x_5)(x_2 -x_6)}\\]\ngap indicated \\(\\left[\\strut\\cdot\\right]\\) marks term excluded. \\(p_2(x)\\) excluded term \\(\\frac{(x-x_2)}{(x_2 - x_2)}\\). various functions \\(p_1(x)\\), \\(p_2(x)\\), \\(p_3(x)\\) leave analogous term.Three important facts notice ingenious polynomial functions:polynomial order. \\(k\\) knots, order \\(k-1\\).Evaluated \\(x_i\\), value \\(p_i(x_i) = 1\\). instance, \\(p_2(x_2) = 1\\).Evaluated \\(x_j\\), \\(j\\neq \\), value \\(p_j(x_i) = 0\\). example, \\(p_2(x_3) = 0\\).overall polynomial linear combination \\[p(x) = y_1\\, p_1(x) + \ny_2\\, p_2(x) + \\cdots + y_k\\, p_k(x)\\ .\\]\nCan see ?","code":"\nu <- rbind(2, 5, -3)\nv <- rbind(1, -4, 0)\nA <- cbind(u, v)\nA##      [,1] [,2]\n## [1,]    2    1\n## [2,]    5   -4\n## [3,]   -3    0\nc <- rbind(3, 1)\nA %*% c##      [,1]\n## [1,]    7\n## [2,]   11\n## [3,]   -9\nX <- cbind(\n  rbind(3, 1),\n  rbind(0, 2),\n  rbind(-1, 0)\n)\nA %*% X##      [,1] [,2] [,3]\n## [1,]    7    2   -2\n## [2,]   11   -8   -5\n## [3,]   -9    0    3## Warning: Removed 38 row(s) containing missing values (geom_path).## Warning: Removed 8 row(s) containing missing values (geom_path)."},{"path":"linear-combs-vectors.html","id":"exercises-1","chapter":"2 Linear combinations of vectors","heading":"2.5 Exercises","text":"Locating WW aircraftThe photograph shows part aircraft detection system World War . concrete block “acoustic mirror.” purpose collect reflect sounds aircraft, concentrating point can picked microphone. Moving microphone point concentrated sound strongest allows aircraft’s bearing identified, helping observers acquire aircraft visually.SourceWith two acoustic mirrors, location aircraft can identified.Question Give position aircraft multiple \\(\\vec{}\\) sound mirror multiple \\(\\vec{b}\\) sound mirror B. (Choose closest answer)\\(3 \\vec{}\\) \\(4.5 \\vec{b}\\)Excellent! \\(4 \\vec{}\\) \\(6 \\vec{b}\\)︎✘ \\(3\\vec{}\\) \\(2 \\vec{b}\\)︎✘ \\(4\\vec{}\\) \\(4.5\\vec{b}\\)︎✘ diagrams consists two vectors target point (denoted bull’s eye). point, target definite location relative coordinate axes.“Solving” sort system amounts finding linear combination two vectors whose result vector can connect origin target. (call target problem.)Copy diagram paper. (reasonable approximation .) , diagram, find linear combination two vectors reach origin target. Show work, meaning draw scaled vectors proper position demonstrates indeed connect origin target. Underneath diagram, write numerical value scalars linear combination. (, reasonable approximation .)exercise, going use R find solutions target problem. question poses one target problem. solve problems eye, want get started computer solution ’ll ready solve harder problems must worked computer.vectors working :\n\\[\\vec{} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b}  \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c}  \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d}  \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T}  \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\]make life easier, commands defining vectors. One commands wrong. ’ll correct moving rest problem.NEED BREAK ONE THESEQuestion correct linear combination \\(\\vec{}\\) \\(\\vec{b}\\) reach target $ec{T}?\\(-4 \\vec{} + 7 \\vec{b}\\)Excellent! \\(-2 \\vec{} + 5 \\vec{b}\\)︎✘ \\(2 \\vec{} -7 \\vec{b}\\)︎✘ \\(4 \\vec{} - 5 \\vec{b}\\)︎✘ Question B correct linear combination \\(\\vec{b}\\) \\(\\vec{c}\\) reach target $ec{T}?\\(\\frac{5}{3}\\vec{b} + \\frac{4}{3} \\vec{c}\\)Excellent! \\(\\frac{4}{3}\\vec{b} + \\frac{7}{2} \\vec{c}\\)︎✘ \\(\\frac{5}{2}\\vec{b} + \\frac{4}{5} \\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{9}{4} \\vec{c}\\)︎✘ Question C correct linear combination \\(\\vec{c}\\) \\(\\vec{d}\\) reach target $ec{T}?\\(\\frac{0}{3}\\vec{c} + \\frac{4}{5} \\vec{d}\\)Correct. \\(\\frac{1}{3}\\vec{c} + \\frac{1}{3} \\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{c} + \\frac{5}{7} \\vec{d}\\)︎✘ \\(\\frac{0}{3}\\vec{c} + \\frac{1}{2} \\vec{d}\\)︎✘ Question D correct linear combination \\(\\vec{}\\) \\(\\vec{c}\\) reach target $ec{T}?\\(\\frac{5}{4}\\vec{} + \\frac{7}{4} \\vec{c}\\)Correct. \\(\\frac{5}{4}\\vec{} + \\frac{5}{4} \\vec{c}\\)︎✘ \\(\\frac{7}{4}\\vec{} + \\frac{9}{4} \\vec{c}\\)︎✘ \\(\\frac{3}{4}\\vec{} + \\frac{7}{4} \\vec{c}\\)︎✘ \n","code":"\na <- rbind(1, 2)\nb <- rbind(1, 1)\nc <- rbind(1, -1)\nd <- rbind(-6, 2)\nT <- rbind(3, -1)"},{"path":"projection-residual.html","id":"projection-residual","chapter":"3 Projection & residual","heading":"3 Projection & residual","text":"previous chapter, took linear combinations vectors. Using notation matrices, solved problem\n\\[\\text{previous chapter:}\\ \\ {\\mathbf }\\, \\vec{x} = \\vec{?}\\] \\(\\vec{x}\\) column coefficients, one coefficient columns matrix \\({\\mathbf }\\). Finding \\(\\vec{?}\\) simple, somewhat tedious, matter multiplication.\\[\\text{chapter:}\\ \\ {\\mathbf }\\, \\vec{?} = \\vec{b}\\]\nwords, task chapter show solve \\({\\mathbf } \\vec{x} = \\vec{b}\\) know result linear combination \\(\\vec{b}\\), set vectors combined, \\(\\mathbf M\\), don’t yet know \\(\\vec{x}\\).motivation “find \\(\\vec{x}\\)” problem, refer Figure 2.5 showed temperature-vs-time data CoolingWater data frame. figure shows several possible linear combinations vectors \\(u(t) \\equiv e^{-0.02 t}\\) (called expkt) \\(v(t) \\equiv 1\\) (called intercept). Suppose seek find particular linear combination \\(u(t)\\) \\(v(t)\\) comes close possible black dots figure. , know \\({\\mathbf }\\): two columns expkt intercept data frame, know \\(\\vec{b}\\): column temp data frame.sort problem extremely common important throughout quantitative fields sorts, astronomy zoology, one foundation techniques statistics data science. ’ll present approach graphically, algorithmically, computationally.Let’s highlight three different modes ’re going introduce topic projection residual. Ultimately, professional work, rely oncomputation, getting computer work. us chapter, basic R functions use rbind(), cbind(), qr.solve(), “matrix multiplication” %*%, well usual addition, subtraction, division, square root, scalar multiplication.1As introduce , want make sure know exactly rbind(), cbind(), qr.solve(), %*% used . world linear combinations, fundamental operations.algorithmic describes process carrying operation. R operators qr.solve() %*% (others!) implemented software. software describes series steps accomplish operation. linear combinations, steps arithmetic dot product vector length.show arithmetic steps vector operations simple case one, two, three vectors. generalization vectors requires organizational principles provided computer languages, course computer programming. rendering operations linear combinations arithmetic, hope provide better understanding ’s happening operations can aware consumer output computer programmers.geometrically two three dimensions. humans powerful intuitive abilities make sense simple geometric constructions. Since many people able assimilate simple geometry readily arithmetic, geometric presentation provides good alternative route understanding mathematics linear combinations, even though ’s restricted simple situations two three dimensional space., ultimately professionals use computational tools working tasks linear combinations. use tools better can think terms arithmetic geometry.","code":""},{"path":"projection-residual.html","id":"projecting-a-vector-onto-another-vector","chapter":"3 Projection & residual","heading":"3.1 Projecting a vector onto another vector","text":"’ll start one fundamental operations: projecting one vector onto another. geometric situation shown Figure 3.1. notation, always write vector projected \\(\\vec{b}\\) variation letter “b.” vectors onto \\(\\vec{b}\\) projected written \\(\\vec{u}\\), \\(\\vec{v}\\), . section, single vector projected onto, ’re calling \\(\\vec{u}\\).\nFigure 3.1: Two vectors. Vector \\(\\vec{b}\\) projected onto vector \\(\\vec{u}\\).\nphysical analogy projection casting shadows screen manner old-fashioned slide projector movie projector. vector \\(\\vec{u}\\) screen. light source arranged generate parallel rays arrive perpendicularly screen. situation, shadow cast \\(\\vec{b}\\) screen \\(\\vec{u}\\) “projection \\(\\vec{b}\\) onto \\(\\vec{u}\\).”terms vector-language, projection \\(\\vec{b}\\) onto \\(\\vec{u}\\) vector \\(\\alpha\\,\\vec{u}\\) scalar multiplier \\(\\alpha\\) selected place \\(\\alpha\\,\\vec{u}\\) close possible \\(\\vec{b}\\).mean exactly “close possible?” need render vector-language. ’ll combination length vector subtraction. scalar value \\(\\alpha\\), vector \\(\\vec{b} - \\alpha\\, \\vec{u}\\). Figure 3.2 shows situation handful candidates \\(\\alpha\\). always, vector \\(\\alpha\\,\\vec{u}\\) lies linear subspace defined \\(\\vec{u}\\). Supposing \\(\\alpha = 2\\), corresponding blue dot Figure 3.2, vector \\(\\vec{b} - \\alpha\\,\\vec{u}\\) vector extending blue dot tip \\(\\vec{b}\\).\nFigure 3.2: vector \\(\\alpha\\,\\vec{u}\\) shown several values \\(\\alpha\\)$, value marked colored dot. (values shown roughly $lpha = $1.1 (orange), 1.3 (yellow), 1.6 (red), 2.0 (blue).) vector \\(\\vec{b} - \\alpha\\,\\vec{u}\\) colors indicated dashed line connecting tip \\(\\alpha\\,\\vec{u}\\) tip \\(\\vec{b}\\).\ncontext, “close possible” means value \\(\\alpha\\) makes length \\(\\| \\vec{b} - \\alpha\\,\\vec{u} \\|\\) small possible. ’s possible many people find argmin \\(\\alpha^\\star\\) eye. Figure, orange blue dots clearly best. Yellow red length, yellow winning hair.vector \\(\\vec{b} - \\alpha^\\star\\,\\vec{u}\\) always perpendicular \\(\\vec{u}\\). words, three vectors \\(\\vec{b}\\), \\(\\alpha^\\star\\,\\vec{u}\\), \\(\\vec{b}-\\alpha^\\star\\,\\vec{u}\\) constitute sides right triangle, \\(\\vec{b}\\) hypothenuse. gives important clue finding \\(\\alpha^\\star\\) arithmetically. length \\(\\alpha^\\star\\, u\\) , using trigonometry right triangles, \\(\\|\\vec{b}\\| \\cos(\\theta)\\), \\(\\theta\\) angle \\(\\vec{u}\\) \\(\\vec{b}\\). Recall \n\\[\\cos(\\theta) = \\frac{\\vec{u}\\cdot \\vec{b}}{\\|\\vec{u}\\|\\ \\|\\vec{b}\\|}\\ .\\]Consequently, \\[\\|\\vec{b}\\| \\cos(\\theta) = \\frac{\\vec{u}\\cdot \\vec{b}}{\\|\\vec{u}\\|}\\]quantity length. projection \\(\\vec{b}\\) onto \\(\\vec{u}\\) vector. Since know \\(\\vec{u}\\), know direction result projection. previous formula gives length. can create unit-length vector pointing direction \\(\\vec{u}\\) appropriate scaling: \\(\\vec{u} / \\|\\vec{u} \\|\\). projection \\(\\vec{b}\\) onto \\(\\vec{u}\\), ’ll denote \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) \n\\[\\overset{\\longrightarrow}{{b}\\|_{u}} = \\underbrace{\\frac{\\vec{u}\\cdot\\vec{b}}{\\|\\vec{u}\\|}}_\\text{length}\\underbrace{\\frac{\\vec{u}}{\\|\\vec{u} \\|}}_\\text{direction} = \\frac{\\vec{u}\\cdot\\vec{b}}{\\vec{u}\\cdot\\vec{u}} \\vec{u} = \\alpha^\\star\\,\\vec{u}\\]Read \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) “component \\(\\vec{b}\\) parallel \\(\\vec{u}\\),” “\\(\\vec{b}\\) projected onto \\(\\vec{u}\\).”Remember vector \\(\\vec{b} - \\alpha^\\star\\, \\vec{u}\\) perpendicular \\(\\vec{u}\\) connects \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) \\(\\vec{b}\\). vector, called residual vector statistics, denote \n\\[\\overset{\\longrightarrow}{b\\! \\perp_u}\\]\ncan pronounce “component \\(\\vec{b}\\) perpendicular \\(\\vec{u}\\),” “residual projecting \\(\\vec{b}\\) onto \\(\\vec{u}\\).” Adding projection \\(\\vec{b}\\) onto \\(\\vec{u}\\) residual vector projection reproduces original \\(\\vec{b}\\):\\[\\overset{\\longrightarrow}{b\\|_u}\\ +\\ \\overset{\\longrightarrow}{b\\! \\perp_u} = \\vec{b}\\ .\\]MOVE FUNCTIONS mosaicCalc. ’re going give students training wheelsUsing geometry vectors little bit trigonometry, now created arithmetic formulas needed :Project vector \\(\\vec{b}\\) onto another vector \\(\\vec{u}\\), andFind residual vector projection.help make transition arithmetic computation, provide two training-wheel functions, %onto% %perp. Use like :Example 3.1  Using %dot%, %perp% %onto% two vectors \\(\\vec{b}\\) \\(\\vec{u}\\) choosing, demonstrate \n\\[\\overset{\\longrightarrow}{b\\|_u}\\ +\\ \\overset{\\longrightarrow}{b\\! \\perp_u} = \\vec{b}\\|\\] \\(\\overset{\\longrightarrow}{b\\|_u}\\) perpendicular \\(\\overset{\\longrightarrow}{b\\! \\perp_u}\\).instance,       \\(0\\)Example 3.2  Show \\(\\overset{\\longrightarrow}{b\\|_u}\\) parallel \\(\\vec{u}\\).Two vectors parallel one scalar multiple .\ninstance, suppose three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}\\), like :Notice \\(\\vec{v} = 2 \\vec{u}\\), , \\(\\vec{u}\\) \\(\\vec{v}\\) parallel. ’s get divide \\(\\vec{v}\\) \\(\\vec{u}\\) component--component basis:Every component \\(\\vec{v}\\) multiple corresponding component \\(\\vec{u}\\).hand, \\(\\vec{b}\\) scalar multiple \\(\\vec{u}\\). Consequently:component--component multipliers .Now show \\(\\overset{\\longrightarrow}{b\\|_u}\\) parallel \\(\\vec{u}\\). First, ’ll compute \\(\\overset{\\longrightarrow}{b\\|_u}\\) divide , component--component, \\(\\vec{u}\\)contrast, \\(\\overset{\\longrightarrow}{b\\!\\perp_u}\\) , obviously, parallel \\(\\vec{u}\\):Example 3.3  Find scalar multiplier \\(\\alpha^\\star\\) \\[\\overset{\\longrightarrow}{b\\!\\perp_u} = \\alpha^\\star\\,\\vec{u}\\ .\\]just matter dividing \\(\\overset{\\longrightarrow}{b\\!\\perp_u}\\) \\(\\vec{u}\\) component--component basis:see \\(\\alpha^\\star = 0.7857143\\).commit commands used previous demonstrations memory, making sure understand commands demonstration .","code":"\n`%dot%` <- function(u, v) {\n  sum(u * v)\n}\n`%onto%` <- function(b, A) {\n  A %*% qr.solve(A, b)\n}\n`%perp%` <- function(b, A) {\n  b - (b %onto% A)\n}\nas_unit <- function(A) {\n  helper <- function(v) { v / sqrt(sum(v^2)) }\n  apply(A, 2, helper)\n}\nas_mag <- function(A, method=c(\"2\", \"O\", \"I\", \"F\", \"M\")) {\n  method <- match.arg(method)\n  helper <- function(v) {Matrix::norm(v, type=method)}\n  \n  apply(A, 2, helper)\n}\nb <- rbind(3, 1, 7, 4)\nu <- rbind(1,1,1,1)b %onto% ub %perp% u\n# create your own vectors!\nb <- rbind(8, -3, 2, 1, 4)\nu <- rbind(1,  2, 1, 2, 3)\n# demonstrate properties\nprojected <- b %onto% u\nresidual <- b %perp% uprojected + residualprojected %dot% residual\nu <- rbind(1, 2, 3)\nv <- rbind(2, 4, 6)\nb <- rbind(3, 1, 2)v/ub/u(b %onto% u)/u(b %perp% u)/u(b %onto% u)/u"},{"path":"projection-residual.html","id":"projection-onto-a-set-of-vectors","chapter":"3 Projection & residual","heading":"3.2 Projection onto a set of vectors","text":"just seen, projecting target \\(\\vec{b}\\) onto single vector matter arithmetic:\n\\[\\overset{\\longrightarrow}{b\\|_u} = \\frac{\\vec{u}\\cdot\\vec{b}}{\\vec{u}\\cdot\\vec{u}} \\vec{u}\\]\nNow take problem projecting target \\(\\vec{b}\\) onto two vectors simultaneously. Following previous convention, vectors projected onto \\(\\vec{u}\\) \\(\\vec{v}\\). ’ll package vectors matrix \\(\\mathbf \\), ,\n\\[{\\mathbf } \\equiv \\left[\\vec{u}, \\vec{v}\\right]\\ .\\]\n’ll refer space spanned vectors \\({\\bf }\\) \\(span(\\mathbf )\\). Since two vectors \\({\\bf }\\), space plane.Finding linear combination vectors \\(\\mathbf \\) comes close possible \\(\\vec{b}\\) involves two tasks.\nOne task project \\(\\vec{b}\\) onto \\(span(\\mathbf )\\). resulting vector, denote \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\), lives \\(span(\\mathbf )\\). (course , since \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\) created projecting \\(\\vec{b}\\) onto \\(\\mathbf \\).)task two express \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\) linear combination vectors \\(\\mathbf \\). know can exactly \\(span(\\mathbf )\\) defined vectors can created linear combination vectors \\(\\mathbf \\). vector \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\) created specifically live \\(span(\\mathbf )\\).’ll start description technique approximating \\(\\vec{b}\\) linear combination two vectors \\(\\mathbf \\) reduced problem \\(\\vec{b}\\) happens already \\(span(\\mathbf )\\).may already encountered technique childhood reading. problem appears Robert Louis Stevenson’s famous novel, Treasure Island. story discovery treasure map indicating location buried treasure eponymous Island. red X map labelled “bulk treasure ,” hardly sufficient guide dig treasure. , every buried treasure needs secret protect . back map written cryptic clue precise location:Tall tree, Spy-glass shoulder, bearing point N. N.N.E.\nSkeleton Island E.S.E. E.\nTen feet.Skeleton Island clearly marked map, Spy-glass Hill. plateau marked red X “dotted thickly pine-trees varying height. Every , one different species rose forty fifty feed clear neighbours.” “tall tree” mentioned clue.\nFigure 3.3: map Treasure Island. heading ‘E.S.E. E.’ marked solid black line starting Skeleton Island. heading ‘N. N.N.E.’ marked dotted lines, one positioned point shoulder Spy-glass Hill. bearing Skeleton Island meets bearing Spy-glass Hill Tall tree.\nLong John Silver, obviously accomplished mathematician, starts near Skeleton Island, moving along vector keeps Skeleton Island compass bearing one point east east-south-east. march, keeps telescope trained shoulder Spy-glass Hill. goal telescope points one point north north-north-east, vicinity tall tree. ’s tree matching clue.vectors Treasure Island embedded Earth’s surface, set positions spanned latitude longitude. Figure 3.4 shows slightly general problem two-dimensional space, vectors \\(\\vec{u}\\) \\(\\vec{v}\\) orthogonal. task, still, find linear combination \\(\\vec{u}\\) \\(\\vec{v}\\) match \\(\\vec{b}\\). diagram shows \\(\\vec{u}\\) vector subspace aligned \\(\\vec{u}\\), similarly \\(\\vec{v}\\)\nFigure 3.4: telescope method solving projection onto two vectors.\nalgorithm based Long John Silver’s technique. Pick either \\(\\vec{u}\\) \\(\\vec{v}\\), doesn’t matter . diagram, ’ve picked \\(\\vec{v}\\). Align telescope vector. Now march along vector, \\(\\vec{u}\\), carefully keeping telescope bearing aligned \\(\\vec{v}\\). diagram, can see ’ve marched \\(\\frac{1}{2} \\vec{u}\\), telescope yet \\(\\vec{b}\\) view. Similarly, \\(1 \\vec{u}\\), target \\(\\vec{b}\\) isn’t yet visible. Marching little , \\(1.6 \\vec{u}\\) brings point \\(\\vec{u}\\)-subspace target falls view. tells us coefficient \\(\\vec{u}\\) 1.6.find coefficient \\(\\vec{v}\\), ’ll need march along line telescope, taking steps size \\(\\|\\vec{v}\\|\\). diagram, ’ve marked march copies \\(\\vec{v}\\) make counting easier. ’ll need march opposite direction \\(\\vec{v}\\), coefficient negative. Taking 2.8 steps size \\(\\|\\vec{v}\\|\\) brings us target. Thus:\\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\]\nable find linear combination \\(\\vec{u}\\) \\(\\vec{v}\\)—vectors making matrix \\(\\mathbf \\)—, Treasure Island, \\(\\vec{b}\\) happened live \\(span(\\mathbf )\\). complete algorithm, project \\(\\vec{b}\\) onto \\(span(\\mathbf )\\) started march \\(span(\\mathbf )\\). Also, make algorithm general, need render form work dimensional space. means telescopes dotted lines , ’ll work arithmetic.Figure 3.5 three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) \\(\\vec{b}\\) three dimensional space. turns algorithm develop case perfectly general, work -dimensional space. , seek linear combination \\(\\vec{u}\\) \\(\\vec{v}\\) match \\(\\vec{b}\\) closely possible. acquaint geometry, rotate diagram perform experiments.\\(\\vec{u}\\) \\(\\vec{v}\\) fixed length. However, lengths appear change rotate space. might called “gun-barrel” effect; tube looks short look ’s longitudinal axis, looks longer look side. Rotate space \\(\\vec{u}\\) \\(\\vec{v}\\) reach maximum apparent length. viewpoint accomplishes looking downward perpendicularly onto \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Vector \\(\\vec{b}\\) plane, downward perpendicular viewpoint, can see \\(\\widehat{b}\\), point plane projection \\(\\vec{b}\\) fall. viewpoint, ’s easy find linear combination \\(\\vec{u}\\) \\(\\vec{v}\\) reachesAgain rotate space vector \\(\\vec{u}\\) pointing straight toward . ’ll see arrowhead \\(\\vec{u}\\), ’ll able figure many steps along \\(\\vec{v}\\) projection \\(\\vec{b}\\) onto \\(\\vec{v}\\). plane looking downward onto plane spanned \\(\\left[\\vec{v}, \\vec{b}\\right]\\).rotate space look straight vector \\(\\vec{v}\\). perspective, can calculate many steps along \\(\\vec{v}\\) projection \\(\\vec{b}\\) onto \\(\\vec{v}\\).\n\n\n\nFigure 3.5: Showing relative orientation three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) \\(\\vec{b}\\). Drag image rotate .\nMany people difficulty bare vector diagram 3-space. Much visual ability see three dimensions shape, shadow, , extent, stereo vision (doesn’t apply projection image onto flat space screen page book).Figure 3.6 shows diagram previous, translucent plane placed onto onto \\(\\vec{u}\\) \\(\\vec{v}\\) vectors mark \\(span(\\mathbf )\\). Try rotations , time subspace refer visually.\n\n\n\nFigure 3.6: three vectors Figure 3.5, now space spanned \\(\\left[\\vec{u}, \\vec{v}\\right]\\) marked translucent plane.\nFigure 3.4 shows general case vector \\(\\vec{b}\\) approached linear combination \\(\\vec{u}\\) \\(\\vec{v}\\).two vectors \\(\\vec{u}\\) \\(\\vec{v}\\) two-dimensional space, linear combination match \\(\\vec{b}\\) space.general situation \\(n\\)-dimensional space \\(p < n\\) vectors form linear combination. Typically, target \\(\\vec{b}\\) subspace spanned p vectors. can illustrate using 3-dimensional space two vectors \\(\\vec{u}\\) \\(\\vec{v}\\) linearly combined.\nFigure 3.7: diagram needs show b projected residual vector.\n\n\n\ncourse, set vectors simply matrix, ’ll cast problem one projecting \\(\\vec{b}\\) onto matrix \\(\\mathbf \\).one case extremely simple: vectors \\(\\mathbf \\) mutually orthogonal. Let’s make sure understand case well. geometry simple, Figure 3.8.\nFigure 3.8: Projecting \\(\\vec{b}\\) onto two orthogonal vectors. REPLACE ACTUAL IMAGE.\ndemonstrate projection bit generally, let’s set 3 orthogonal vectors four-dimensional space \\(\\mathbf \\):able confirm simple arithmetic \\(\\vec{u}\\) orthogonal \\(\\vec{v}\\), \\(\\vec{u}\\) orthogonal \\(\\vec{w}\\), \\(\\vec{w}\\) orthogonal \\(\\vec{v}\\). (Hint: Use dot product.) can also see \\(\\vec{b}\\) parallel one three columns \\({\\mathbf }\\).’ll compute correct answer see simple arithmetic.telling us \n\\[\\overset{\\longrightarrow}{b\\|_{\\mathbf }} = \\left[\\begin{array}{r}0.9783914\n\\\\0.9871949\n\\\\1.0196078\n\\\\1.0136054\\end{array}\\right] = 0.2244898 \\vec{u} - 0.3333333 \\vec{v} + 0.1764706 \\vec{w}\\]can find coefficients linear combination simple, independent uses formula projecting \\(\\vec{b}\\) onto columns \\({\\mathbf }\\) one time:       \\(0.2245\\)       \\(-0.3333\\)       \\(0.1765\\)Example 3.4  Let’s return moment Lagrange polynomials introduced Chapter 2. Recall Lagrange polynomial set pass exactly \\(k\\) knot points \\(k-1\\)-order polynomial. Lagrange polynomial linear combination \\(k\\) simple ingenious functions, one knot point. functions combined similar form:\\[p_i(x) = \\frac{(x-x_1)(x-x_2)}{(x_i -x_1)(x_i-x_2)}\\left[\\strut\\cdots\\right]\\frac{(x-x_k)}{(x_i -x_k)}\\] \\(\\left[\\strut\\cdots\\right]\\) means include intermediate terms sequence except term \\(\\frac{(x-x_i)}{(x_i - x_i)}\\).Evaluate \\(p_i(x)\\) set knot inputs \\((x_j, y_j)\\) gives vector consisting single 1 zeros otherwise. instance:\n\\[p_1\\left(\\begin{array}{c}x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_k\\end{array}\\right) = \\left[\\begin{array}{c}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{array}\\right]\\ \\ \\ \\ \\ \\ \\ \\ \\ p_2\\left(\\begin{array}{c}x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_k\\end{array}\\right) = \\left[\\begin{array}{c}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ p_3\\left(\\begin{array}{c}x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_k\\end{array}\\right) = \\left[\\begin{array}{c}0\\\\0\\\\1\\\\\\vdots\\\\0\\end{array}\\right]\\ \\ \\text{.}\\]\nNotice functions \\(p_i()\\), applied set \\(x\\)-coordinates knot points, produce vector output orthogonal every one functions. , whole set \\(p_1(x_{knots}), p_2(x_{knots}), \\cdots, p_k(x_{knots})\\) creates set vectors (matrix) whose columns combined linearly reach produce \\(y\\)-values knots. system six knot points shown Figure 2.7, matrix, coefficients, target related way:\\[\\!\\!\\!\\begin{array}{rrrrrr}p_1\\!\\!&\\!p_2&\\!\\!p_3&\\!\\!\\!p_4&\\!\\!\\!p_5&\\!\\!p_6& & & & & & \\end{array}\\\\\\left[\\begin{array}{rrrrrr}1&0&0&0&0&0\\\\\n0&1 &0& 0 & 0 & 0\\\\0&0&1&0&0&0\\\\\n0&0&0&1&0&0\\\\\n0&0&0&0&1&0\\\\\n0&0&0&0&0&1\\end{array}\\right]\\cdot\n\\left[\\begin{array}{c}\\strut a_1\\\\a_2\\\\a_3\\\\a_4\\\\a_5\\\\a_6\\end{array}\\right] = \\left[\\begin{array}{c}\\strut y_1\\\\y_2\\\\y_3\\\\y_4\\\\y_5\\\\y_k\\end{array}\\right]\\ .\\]\nSince vectors matrix mutually orthogonal, find \\(a_i\\) need project target onto corresponding \\(\\)th column matrix.MAYBE ILLUSTRATE COMPUTER CODE?simple procedure independent projections work columns \\(\\mathbf \\) mutually orthogonal*. instance:different “one-projection---time” coefficients:       \\(0.3333\\)       \\(0.2593\\)Since independent projections won’t solve target problem (columns \\(\\mathbf \\) mutually orthogonal), solve ?strategy two simplify problem constructing \\(\\mathbf \\) another matrix ’ll call \\(\\mathbf Q\\) spans exactly subspace \\(\\mathbf \\) mutually orthogonal columns.start, well set first column \\(\\mathbf Q\\) one vectors \\(\\mathbf \\). ’ll use \\(\\vec{u}\\) example.second column \\(\\mathbf Q\\) based one remaining vectors, say \\(\\vec{v}\\). \\(\\vec{v}\\) orthogonal \\(\\vec{u}\\). second column \\(\\mathbf Q\\) ’ll insert \\(\\vec{v}\\) , component \\(\\vec{v}\\) orthogonal \\(\\vec{u}\\), :\n\\[\\overset{\\longrightarrow}{v\\!\\perp_u} = \\vec{v} - \\frac{\\vec{u}\\cdot\\vec{v}}{\\vec{u}\\cdot\\vec{u}} \\vec{u}\\]\ncomputer notation, ’ll refer \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) name v_perp_u.       \\(1.2\\)Now \\({\\mathbf Q}\\) matrix \n\\[{\\mathbf Q} \\equiv \\left[\\begin{array}{cc}|&|\\\\ \\vec{u}&\\overset{\\longrightarrow}{v\\!\\perp_u}\\\\|&|\\end{array}\\right]\\]\nCaution: next paragraphs rough going. suffices follow flow argument note operations used scalar multiplication, addition, subtraction, simple dot-product form coefficient produced projecting one vector onto another vector.Since \\(\\vec{u}\\) \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) orthogonal, can easily calculate coefficients two vectors projecting \\(\\vec{b}\\) onto subspace spanned \\(\\mathbf Q\\).       \\(0.3333\\)       \\(0.1852\\)coefficients—\\(\\alpha_1 =\\ \\) 0.3333333 \\(\\alpha_2 =\\ \\) 0.1851852 respectively—multiplied \\(\\vec{u}\\) \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) give us projection \\(\\vec{b}\\) onto subspace spanned \\({\\mathbf Q}\\). Since subspace spanned \\({\\mathbf Q}\\) exactly subspace spanned \\({\\mathbf }\\), answer \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\) consequently \\[\\overset{\\longrightarrow}{b\\!\\perp_{\\mathbf }} = \\vec{b} - \\overset{\\longrightarrow}{b\\|_{\\mathbf }}\\ .\\]terms coefficients, projection \\(\\vec{b}\\) onto \\(\\mathbf \\) \n\\[\\overset{\\longrightarrow}{b\\|_{\\mathbf }} = 0.3333333 \\vec{u} + 0.1851852 \\overset{\\longrightarrow}{v\\!\\perp_u}\\]\ncoefficients \\(\\vec{u}\\) \\(\\vec{v}\\) originally sought. recognizing \\(\\overset{\\longrightarrow}{v\\!\\perp_u} = v - 1.2 \\vec{u}\\), \n\\[\\overset{\\longrightarrow}{b\\|_{\\mathbf }} = 0.3333333 \\vec{u} + 0.1851852 \\left[\\strut \\vec{v} - 1.2 \\vec{u}\\right] \\\\= [0.3333333 - 1.2\\times0.1851852] \\vec{u} + 0.1851852 \\vec{v}\\\\= 0.1111111 \\vec{u} + 0.1851852 \\vec{v}\\]\ncoefficients \\(\\vec{u}\\) \\(\\vec{v}\\) ones sought ones produced professional software.course, nobody want undertake process described step--step fashion ’ve followed. addition hard follow, ’s hard avoid making mistakes along way. Fortunately, expert programmers done work us encapsulated process software function. us using R, function qr.solve().result now way solve target problem, finding coefficients linear combination set vectors bring us close possible target \\(\\vec{b}\\).next chapter, ’ll use capability solve real-world modeling problems.","code":"b %onto% Ab %perp% A\nu <- rbind(2, 3, 0, 6)\nv <- rbind(0,-2,-2, 1)\nw <- rbind(3,-2, 2, 0)\nb <- rbind(1, 1, 1, 1)\nA <- cbind(u, v, w)b %onto% Aqr.solve(A, b)(b %dot% u)/(u %dot% u)(b %dot% v)/(v %dot% v)(b %dot% w)/(w %dot% w)\nu <- rbind(1,2,3,4)\nv <- rbind(4,3,2,5)\nA2 <- cbind(u, v)qr.solve(A2, b)(b %dot% u)/(u %dot% u)(b %dot% v)/(v %dot% v)\ncoef <- ((u %dot% v) / (u %dot% u))\nv_perp_u <- v - coef * ucoef\nalpha1 <- (u %dot% b) / (u %dot% u)\nalpha2 <- (v_perp_u %dot% b) / (v_perp_u %dot% v_perp_u)alpha1alpha2qr.solve(A2, b)"},{"path":"projection-residual.html","id":"exercises-2","chapter":"3 Projection & residual","heading":"3.3 Exercises","text":"Refer vectors \\(\\vec{}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), \\(\\vec{d}\\) figure. Carry following projections graphically. show result projection, also original vector projected original vector projected onto.Project \\(\\vec{}\\) onto \\(\\vec{b}\\)Project \\(\\vec{c}\\) onto \\(\\vec{}\\)Project \\(\\vec{b}\\) onto .direction, \\(\\vec{\\mathbf D}\\):\\[\\vec{\\mathbf D} \\equiv  \\left(\\begin{array}{c}2\\\\5\\end{array}\\right)\\]Question Find projection \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\).\\(0.59 \\vec{\\mathbf D}\\)Excellent! \\(\\left(5, 6\\right)^T\\)︎✘ projection \\(\\vec{\\mathbf V}\\) onto direction \\(\\vec{\\mathbf D}\\) always scalar multiple \\(\\vec{\\mathbf D}\\).\\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ \\(\\left(5,6\\right)^T\\) residual vector, true.Question B Find residual vector \\(\\vec{\\mathbf R}\\) projection \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\).\\(\\vec{\\mathbf R} = \\left(0.83, 2.07\\right)\\)Right! \\(\\vec{\\mathbf R} = \\left(0.31, 1.28\\right)\\)︎✘ residual projecting column vector onto another column vector column vector\\(\\vec{\\mathbf R} = \\left(1.28, 0.31\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.83, 0.31\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) see get $Question C Find projection \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\).\\(0.379 \\vec{\\mathbf D}\\)Nice! \\(\\left(5, 6\\right)^T\\)︎✘ projection \\(\\vec{\\mathbf V}\\) onto direction \\(\\vec{\\mathbf D}\\) always scalar multiple \\(\\vec{\\mathbf D}\\).\\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ \\(\\left(5,6\\right)^T\\) residual vector, true.Question D Find residual vector \\(\\vec{\\mathbf R}\\) projection \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\).\\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)^T\\)Excellent! \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)\\)︎✘ residual projecting column vector onto another column vector column vector\\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) see get $Exercises confirming qr.solve() produces results : residual orthogonal every vector , projected + residual = \\(\\vec{b}\\).Exercises confirming adding columns produces smaller residual.Demonstration even random vectors can combined exactly equal \\(\\vec{b}\\), long enough .twelve labeled vectors, M. thirteenth vector, labeled “Null vector.” ’s vector length zero, can’t drawn arrow. Note direction null vector doesn’t matter, since vector length zero.following statements form, \"\\(\\vec{v}\\) projected onto \\(\\vec{u}\\) gives \\(\\vec{w}\\). Say whether statement true false.Question \\(\\vec{}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{K}\\)TrueRight! False︎✘ projected onto D direction D. K direction D, points right direction (downwards), equals vertical component .Question B \\(\\vec{D}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{L}\\)True︎✘ D projected onto E direction E. L direction E L right length.FalseGood. Question C \\(\\vec{J}\\) projected onto \\(\\vec{E}\\) gives null vector.True︎✘ J E orthogonal. projection one onto null vector.FalseGood. Question D \\(\\vec{H}\\) projected onto \\(\\vec{}\\) gives null vector    True\\(\\heartsuit\\ \\)       False︎✘ Question E \\(\\vec{J}\\) projected onto \\(\\vec{K}\\) gives \\(\\vec{D}\\)True︎✘ J K parallel, projecting J onto K produce vector J. J much shorter D.FalseExcellent! Question F \\(\\vec{C}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{L}\\)True︎✘ C projected onto D direction D. L direction D.FalseGood. Question G \\(\\vec{L}\\) projected onto \\(\\vec{B}\\) gives null vectorTrue︎✘ ’s vectors orthogonal projection one onto produces null vector. L B orthogonal. FalseRight! Question H \\(\\vec{E}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{E}\\)    True\\(\\heartsuit\\ \\)       False︎✘ Question \\(\\vec{G}\\) projected onto \\(\\vec{C}\\) gives null vector.    True\\(\\heartsuit\\ \\)       False︎✘ Question J \\(\\vec{E}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{J}\\)    True\\(\\heartsuit\\ \\)       False︎✘ Question K \\(\\vec{}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{K}\\)    True︎✘ onto B direction B. K orthogonal B.       False\\(\\heartsuit\\ \\)Question L \\(\\vec{H}\\) projected onto \\(\\vec{}\\) gives null vector    True\\(\\heartsuit\\ \\)       False︎✘ Question M \\(\\vec{F}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{H}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question N \\(\\vec{C}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{E}\\)    True︎✘        False\\(\\heartsuit\\ \\)Question O \\(\\vec{E}\\) projected onto \\(\\vec{G}\\) gives null vector    True\\(\\heartsuit\\ \\)       False︎✘ Question P \\(\\vec{G}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{C}\\)Vectors \\(\\vec{}, \\vec{B}, \\cdots, \\vec{H}\\) defined , B, …, H can use sandbox. (see one vectors, give command sandbox consisting just vector name.)sequence commands project vector C onto B.Question ) Solve \\((\\vec{}, \\vec{C}) \\cdot \\vec{x} = \\vec{B}\\). length \\(\\vec{x}\\)?    Exactly 0︎✘        1.6︎✘        2.5︎✘        3.43︎✘        12\\(\\heartsuit\\ \\)       Can’t solved︎✘ Question B B) Solve \\((\\vec{}) \\cdot \\vec{x} = \\vec{B}\\). ’s length residual?    Exactly 0︎✘        0.3︎✘        1.6︎✘        2.5\\(\\heartsuit\\ \\)       3.43︎✘        12︎✘        Can’t solved︎✘ Question C D) Solve \\((\\vec{D}, \\vec{E}) \\cdot \\vec{x} = \\vec{F}\\). length residual?    Exactly 0︎✘        0.3︎✘        1.6︎✘        2.5︎✘        3.43\\(\\heartsuit\\ \\)       12︎✘        Can’t solved︎✘ Question D E) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}) \\cdot \\vec{x} = \\vec{G}\\). length residual?    Exactly 0︎✘        0.3︎✘        1.6\\(\\heartsuit\\ \\)       2.5︎✘        3.43︎✘        12︎✘        Can’t solved︎✘ Question E F) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}, \\vec{G}) \\cdot \\vec{x} = \\vec{H}\\). length residual?Calculate angle two vectors\narithmetically, using dot product\ngraphically, using protractor\narithmetically, using dot productgraphically, using protractorFind nonzero vector orthogonal given vectorCalculate projection vector onto another vector\narithmetically dot product\ngraphically\narithmetically dot productgraphicallyDecompose vector residual component directed along second vector\ngraphically\narithmetically\ngraphicallyarithmeticallyOur goal scale expkt vector scaled numbers close possible destination, namely, temp. Comparing two columns numbers, might anticipate scalar 100. ’ll see calculate exactly next chapter. result turns 99.23. resulting model \\[T(t) = 99.23\\, e^{-0.02 t}\\ .\\]judge whether good model ? Common sense suggests plotting model function along data, Figure 3.9.\nFigure 3.9: Comparing model \\(99.23\\, e^{-0.02 t}\\) recorded data `CoolingWater.\nJudge whether good model. obvious deficiency model falls, decaying exponentials , toward temperature 0, whereas water cooling room temperature 25 degrees.Let’s return model seen terms vectors. advantage develop general procedure can use interpreting models sorts, rather just particular situation cooling-water data.geometric facts? know temp vector length 251.3 deg C. Similarly can calculate length expkt vector: 2.46 deg C.might seem “direction” vector meaningless, ’s direction abstract, hard--envision 15-dimensional space. (15 components temp expkt.) Even , can calculate angle two vectors, using formula \\(\\cos(\\theta) = \\frac{\\vec{v}\\cdot \\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\). arithmetic gives \\[\\cos{\\theta} = \\frac{599.8}{251.3 \\times 2.46} = 0.9708\\ \\ \\implies \\ \\ \\ \\theta = 13.88^\\circ\\]\nFigure 3.10: vectors temp expkt angle 13.88 deg . , expkt drawn 10x ’s actual size.\ngeometrical facts, can draw picture. Figure 3.10 shows temp black expkt magenta. (’ve drawn 10 times long really can see well.) expkt vector good model temp, need scale result, must dotted line picture, close possible tip temp. can count many expkt steps bring close temp. (Remember multiply result 10, since picture drew expkt ten times longer arithmetic length.)One reasonable way quantify good model temp can made properly scaled version vector expkt angle : 13.88 degrees.Likewise, can scale vector intercept make match temp well possible. angle intercept temp works 75.7 degrees; vectors well aligned. Scaling 58.2 bring intercept close ever going get temp, close .idea linear combination scale add multiple vectors. rough start, let’s look combination 58.2 intercept + 99.23 expkt, combination two individual models constructed vector analogy. CAUTION: model poor. ’s vector analogy poor still work , next two chapters, properly work vectors.resulting model … well, terrible! Figure ?? shows linear combinationEXERCISE: Repeat calculations entire CoolingData data frame.Adding vectors. result vectorScaling adding vectors: linear combination vectors.","code":"\nA <- rbind(2.1,  -3.4)\nB <- rbind(0.3, 4.2)\nC <- rbind(-2, 3.8)\nD <- rbind(1.1,  -3,  2.2,  -1)\nE <- rbind(6,  4,  -2,  -1)\nF <- rbind(0,  3,  2,  1)\nG <- rbind(1, -2,  1,  1)\nH <- rbind(-2, 0, 3, 4)\nM <- cbind(A, B)    # Problem setup: Matrix packaging up vectors\nb <- C              # Problem setup: Target vector \nx <- qr.solve(M, b) # Solve the target problem M %*% x = b for x\nbhat <- M %*% x     # Result: projected  vector\nresid <- b  - bhat  # Result: residual vector\nt(A)  %*%  A        # square length of a vector\nCW <- CW %>% \n  mutate(model = 99.23*expkt + 58.2*intercept)"},{"path":"projection-residual.html","id":"moved-from-vectors","chapter":"3 Projection & residual","heading":"3.4 MOVED FROM VECTORS","text":"pencil physical object good job representing vector three dimensional space. Three-dimensional space around us ’s easy—indeed, inevitable—situate pencil . physical access 4-dimensional space higher-dimensional spaces. Instead physical representation, need rely mathematical one: column numbers. can’t use protractor measure angle two vectors 4- higher-dimensional space. Instead, calculate angle using arithmetic. define numerical process calculating angle, need make sure result follows familiar conventions angles, specifically 0 180 degrees, angle two vectors orientation 0, angle two opposite-pointing vectors 180 degrees.arithmetic formula computing angle two vectors simple. , rather, ’s simple allow calculate cosine angle rather angle \\(\\theta\\) . underlying quantity can calculated using dot products:\n\\[\\cos(\\theta) \\equiv \\frac{\\vec{v}^T \\cdot \\vec{w}}{\\sqrt{\\strut (\\vec{v}^T\\cdot \\vec{v})(\\vec{w}^T\\cdot \\vec{w})}}\\]starting point creating vector computer. sense vector just collection numbers, ’s helpful disciplined remember , purposes, vector column numbers. R knows columns handle appropriately.One way create column vector rbind() function.use rbind() function applied individual arguments. , instance, command makes three-dimensional vector calling b.Notice printing vector, R includes series indices (e.g. [1,] [3,]) help reader identify location element vector. also prints header ([,1]) helpful later work collections vectors.","code":"\nb <- rbind(4, -2, 6)\nb##      [,1]\n## [1,]    4\n## [2,]   -2\n## [3,]    6"},{"path":"projection-residual.html","id":"matrix","chapter":"3 Projection & residual","heading":"3.5 Matrix","text":"going hear word “matrix” lot. Later tutorial use term “matrix multiplication.” matrix collection vectors, dimension. ’ll get good time.","code":""},{"path":"projection-residual.html","id":"scalar-multiplication","chapter":"3 Projection & residual","heading":"3.6 Scalar multiplication","text":"can multiply vector times number. result new vector exactly direction original, different length. arithmetic simple: ordinary multiplication number elements vector. Examples:\\[2 \\left(\\begin{array}{c}4\\\\7\\end{array}\\right) = \\left(\\begin{array}{c}8\\\\14\\end{array}\\right)\\]simple multiplication called “scalar multiplication” two reasons:result “scale” vector, sense “scale model”, , make vector bigger smaller.another important form vector arithmetic called “matrix multiplication.” saying “scalar multiplication,” avoid confusion might arise used “multiplication” alone.R, scalar multiplication vector done *, just like ordinary multiplication numbers:Example 3.5  SANDBOX, Write R code create vector named w components 4, -1, -3.5. scalar multiply w 6.3.","code":"\nb <- rbind(4, -2, 6)\n2.3 * b##      [,1]\n## [1,]  9.2\n## [2,] -4.6\n## [3,] 13.8w <- _______\n6.3 _______ w\nw <- rbind(4,  -1,  -3.5)\n6.3 *  w##        [,1]\n## [1,]  25.20\n## [2,]  -6.30\n## [3,] -22.05"},{"path":"projection-residual.html","id":"dot-product-and","chapter":"3 Projection & residual","heading":"3.7 Dot product and %*%","text":"Now introduce new R arithmetic function, written %*%. symbol pronounced “matrix multiply.” traditional mathematical notation, matrix multiplication indicated putting two quantities next one another, like : \\(\\vec{\\mathbf m}^T \\vec{\\mathbf x}\\), sometimes dot \\(\\vec{\\mathbf m}T\\cdot \\vec{\\mathbf x}\\). superscript \\(^T\\) means “transpose.” us, merely book-keeping convention.operation %*% several different types arithmetic vectors. one work called dot product. (also “matrix products” “outer products”.)R notation dot product much echoes traditional matrix notation, least respect \\(^T\\). ’ll illustrate creating two vectors u v calculating dot product.Notice output dot product single number: scalar. (R prints output scalar vector one-dimension.)Arithmetically, dot product calculated multiplying corresponding components two vectors (e.g. \\(6 \\times 2\\) \\(-3 \\times 1\\) \\(7 \\times 3\\)) adding result. can see dot product always involves two vectors number elements.R t() function corresponds mathematical notation transpose: \\(^T\\). t(u) written, mathematically, \\(\\vec{u}^T\\). purpose t() turn columns (like vector u) rows, vice versa. like, try command t(u) sandbox see printed.us, purpose writing t(u) signal %*% matrix multiplication operation want particular operation: dot product.dot product always involves transpose column vector left side %*% column vector right side.can also write command u %*% t(v), dot product. called “outer product” need course. Try sandbox.dot product outer product written similar ways produce completely different results. won’t much use outer products course, aware look like can diagnose problem attempt dot product goes wrong.Dot productsOuter product","code":"\nu <- rbind(6, -3,  7)\nv <- rbind(2,  1,  3)\nt(u) %*%  v##      [,1]\n## [1,]   30\nt(u) %*% v##      [,1]\n## [1,]   30\nt(v) %*% u##      [,1]\n## [1,]   30\nu %*% t(v)##      [,1] [,2] [,3]\n## [1,]   12    6   18\n## [2,]   -6   -3   -9\n## [3,]   14    7   21\nv %*% t(u)##      [,1] [,2] [,3]\n## [1,]   12   -6   14\n## [2,]    6   -3    7\n## [3,]   18   -9   21"},{"path":"projection-residual.html","id":"vector-lengths","chapter":"3 Projection & residual","heading":"3.8 Vector lengths","text":"vector \\(\\vec{v}\\), length denoted \\(|| \\vec{v} ||\\). Vector length can measured ruler … long physical access vector. often, numerical representation. , use arithmetic—dot product—calculate vector length:\n\\[|| \\vec{v}  || \\equiv \\sqrt{\\ \\vec{v}^T \\cdot \\vec{v}}\\]::: {.example data-latex=\"\"}\nConsider two vectors\n\\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\  \\  \\ \\mbox{}  \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right)\n\\]length \\(\\vec{u}\\) \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\).length \\(\\vec{w}\\) \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\).Using SANDBOX, use R commands create vectors \\(\\vec{u}\\) \\(\\vec{w}\\) find lengths using dot-product operator%*%` operator.","code":"u <- rbind( ____ )\n# length of u\nsqrt( ____ %*% ____ )\n\nv <- rbind( ______ )\n# length of v\nsqrt( ____ %*% ____ )\nu <- rbind(3, 4)\nsqrt(t(u) %*% u)##      [,1]\n## [1,]    5\nw <- rbind(1, 1, 1, 1)\nsqrt(t(w) %*% w)##      [,1]\n## [1,]    2"},{"path":"projection-residual.html","id":"outline-1","chapter":"3 Projection & residual","heading":"3.9 Outline","text":"Write system linear equations “target problem” involving linear combination vectors.Definition linear combination vectors.Matrix collection vectors\nLinear combination written matrix multiplied vector\nLinear combination written matrix multiplied vectorSolve target problem algebraically graphically two dimensions.Subtracting vectors algebraically graphically","code":""},{"path":"target-problem.html","id":"target-problem","chapter":"4 The target problem","heading":"4 The target problem","text":"Chapter 3, solve problem finding scalar multiple \\(\\alpha^\\star\\) \\(\\alpha^\\star\\, \\vec{u}\\) close possible another vector \\(\\vec{b}\\). chapter, ’ll generalize method let us finding particular linear combination set vectors \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}, \\ldots\\) close possible \\(\\vec{b}\\). call target problem. clarity, ’ll always write target \\(\\vec{b}\\): place want get .","code":""},{"path":"target-problem.html","id":"properties-of-the-solution","chapter":"4 The target problem","heading":"4.1 Properties of the solution","text":"might expect, known solution target problem. ’ll start using computer implementation solution demonstrate simple properties solution. example, ’ll use three vectors 5-dimensional space “screen” projected onto, another vector \\(\\vec{b}\\) object projected.notation \\[\\overset{\\longrightarrow}{{b}\\|_{u,v,w}}\\] expressive solution seek. ’s concise place \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}\\) matrix \\({\\mathbf }\\):\n\\[{\\mathbf } \\equiv \\left[\\strut \\begin{array}{ccc}|&|&|\\\\\\vec{u} & \\vec{v} & \\vec{w}\\\\|&|&|\\end{array}\\right]\\] can write solution \n\\[\\overset{\\longrightarrow}{{b}\\|_{\\mathbf }}\\ .\\]notation, lovely expressive though may , hardly suited computer expression, constructed typewriter characters. , ’ll adopt another convention writing b_hat computer stand \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf }}\\).spirit, ’ll write b_resid stand \\(\\overset{\\longrightarrow}{b\\!\\perp_{\\mathbf }}\\).can confirm really solution target problem set vectors? One check b_hat + b_resid identical original b:far, good.Another check b_hat perpendicular b_resid. , dot product b_hat b_resid 0.       \\(0\\)Close enough! (Round-error computer arithmetic created slight deviation 0.)interesting consequence residual perpendicular b_hat residual also perpendicular every one vectors projected onto, linear combination vectors.       \\(0\\)       \\(0\\)       \\(0\\)Examples linear combinations       \\(0\\)       \\(0\\)Check!Although know b_hat b_resid, still don’t know particular linear combination columns produces b_hat. ’s find :x contains coefficients linear combination. demonstrate, let’s compare b_hat linear combination:Ideally, result ones. ’s round-error copied first several digits coefficients calculation. better copy. Instead, can calculate linear combination matrix multiplication:add qr.solve() computational toolbox R functions. carries projection calculations, instead returning b_hat \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf }}\\) whatever want call shadow \\(\\vec{b}\\) onto \\({\\mathbf }\\), returns coefficients linear combination vectors \\({\\mathbf }\\) produce \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf }}\\).","code":"\n# the three vectors\nu <- rbind(6, 4, 9, 3, 1)\nv <- rbind(1, 5,-2, 0, 7)\nw <- rbind(3,-5, 2, 8, 4)\nA <- cbind(u, v, w)\n# the target\nb <- rbind(8, 2,-5, 7, 0)\n# the solution and residual\nb_hat <- b %onto% A\nb_resid <- b %perp% Ab_hatb_residb - (b_hat + b_resid)b_hat %dot% b_residb_resid %dot% ub_resid %dot% vb_resid %dot% wb_resid %dot% (2 * u - 4.7 * v + pi * w)b_resid %dot% (-5.4 * u - 0.7 * v + 8.3 * w)x \n       \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.0384\\\\ 0.3348\\\\ 0.4885\\\\\\end{array}\\right]\\)b_hat - (0.03835171 * u + 0.33478133 * v + 0.48849968 * w)b_hat/(0.03835171 * u + 0.33478133 * v + 0.48849968 * w)b_hat - (A %*% x)b_hat/(A %*% x)"},{"path":"target-problem.html","id":"the-geometry-of-qr.solve","chapter":"4 The target problem","heading":"4.2 The geometry of qr.solve()","text":"","code":""},{"path":"target-problem.html","id":"finding-the-target-with-qr","chapter":"4 The target problem","heading":"4.3 Finding the target with QR","text":"Start simple example identity matrix reaching target .Show vectors identity matrix mutually orthogonal.Show can find mutually orthogonal basis set matrix.Write target problem form x = bRelationship \\(\\vec{b}\\), \\(\\hat{b}\\), residual.Compose linear combination problems solve using R/qr.solve()Applications projectionRelationship projection method least squaresComputations results qr.solve()\nb-hat\nresidual\northogonality b-hat residual\nb-hatresidualorthogonality b-hat residual","code":""},{"path":"least-squares.html","id":"least-squares","chapter":"5 Least Squares","heading":"5 Least Squares","text":"Chapter yet released.often happens model needed help organize complex, multivariate data, purposes prediction. example use Daily Digital measurement body-fat percentage. ’s thought quantity useful monitoring long-term health related measures weight body-mass index (BMI).’s possible measure body-fat percentage elaborate procedure involving submerging person water measure total body volume, calculating persons mass density converting reading body-mass percentage. helpful construct way estimating body-fat percentage easy--make measurements weight, height, waist circumference, etc.overall strategy developing model involves making easy--make body measurements hard--make submergence measurement. model function constructed submergence measurement output easy--make measurements inputs.form function take? One widely used techniques throughout science, industry, medicine, economics, many disciplines involves constructing linear combination simple functions individual variables sometimes pairs variables. instance, input variables \\(x\\), \\(y\\), \\(z\\), simple functions might include\\[f_0(x,  y, z) \\equiv 1\\\\\nf_1(x,y,z)\\equiv x\\\\\nf_2(x, y, z) \\equiv y\\\\\nf_3(x, y, z) \\equiv z\\\\\nf_4(x, y, z) \\equiv x \\cdot y\\\\\nf_5(x, y, z) \\equiv x \\cdot z\\\\\n\\mbox{...  .}\n\\]’re going use strategy linear combinations simple functions investigate problem finding body-fat percentage function easy--make measurements.","code":""},{"path":"least-squares.html","id":"the-data","chapter":"5 Least Squares","heading":"5.1 The data","text":"data hand data frame called Body_fat containing measurements 252 males. variables Body_fat include body-fat percentage (bodyfat), age, weight, height, body-mass index (bmi) well series circumferences: around neck, chest, abdomen, hip, thigh, knee, angle, biceps, forearm, wrist.First, quick demonstration body-fat percentage obvious simple function weight BMI. commands use Daily Digital almost demonstrated :lm() finds linear combinations inputs best approximate outputmakeFun() reformats linear combinations function suitable plottinggf_point() slice_plot() already familiar yoursquared() calculates close predictions actual values. R\\(^2\\) statistic always range 0 1. closer one, predictive model.graphs show correlation bodyfat weight bmi individually. , given weight given BMI, big range body-fat percentage, clearly bodyfat just alias weight BMI. R\\(^2\\) models shows bmi better predictor body-fat percentage weight.’ve seen enough vector mathematics now understand exactly R\\(^2\\) . output-variable vector exactly perpendicular subspace spanned input vectors linear combination, R\\(^2\\) zero. output-variable vector completely aligned subspace spanned input vectors, R\\(^2\\) one. two extremes, R\\(^2\\) cosine-squared angle output vector subspace spanned input vectors. two important consequences definition:include new input vector linear combination, R\\(^2\\) generally go (certainly never go ). reason subspace spanned expanded model usually larger (never smaller) subspace spanned original model.data \\(n\\) rows, using \\(n\\) random-number vectors produce R\\(^2\\) 1, subspace spanned vectors entire vector space. Consequently, regardless output vector , entirely contained within input-vector subspace.small increase R\\(^2\\) happen even new vector nothing output vector. statistics, learn “small” means much R\\(^2\\) go order plausible claim new vector meaningfully contributing model.Let’s see well can predict bodyfat use weight bmi.can see contour graph weight 150 lbs BMI 25, predicted body-fat percentage just hair 20%. can see R\\(^2\\) two-input model better model just BMI input.slice plots show, name suggests, slice contour plot. first slice, weight held constant 150 lbs. range BMI shown, body-fat percentage changes substantially: 0 40%. , BMI strong relationship body fat percentage even holding weight constant.second slice holds bmi 25. range weights, body-fat percent changes slightly, 18% 21%. Thus, knowing BMI, additional knowledge weight tells almost nothing body fat percentage.strategy build model multiple inputs, hold inputs constant examining model output respect remaining inputs. “hold inputs constant” powerful technique interpreting complex models.","code":"\nmod1 <- lm(bodyfat ~ weight, data = Body_fat)\nfun1 <- makeFun(mod1)\nmod2 <- lm(bodyfat ~ bmi,  data =  Body_fat)\nfun2 <- makeFun(mod2)\ngf_point(bodyfat ~ weight,  data = Body_fat) %>% \n  slice_plot(fun1(weight) ~ weight)\ngf_point(bodyfat ~ bmi, data  = Body_fat) %>% \n  slice_plot(fun2(bmi) ~ bmi)\nrsquared(mod1)## [1] 0.3722742\nrsquared(mod2)## [1] 0.5547538\nmod3 <- lm(bodyfat ~ weight + bmi,  data  = Body_fat)\nfun3  <-  makeFun(mod3)\ncontour_plot(fun3(weight,  bmi) ~ weight + bmi, \n             domain(weight =  c(100,200), bmi =  c(15, 35)))\nrsquared(mod3)  # mod3 is bodyfat ~ weight + bmi## [1] 0.5585916\nslice_plot(fun3(weight = 150, bmi)  ~ bmi, domain(bmi =  c(15, 35)))  %>%\n  gf_labs(title = \"Holding weight at 150 lbs\")\nslice_plot(fun3(weight, bmi = 30)  ~  weight,  domain(weight   = c(100,200))) %>%\n  gf_labs(title = \"Holding BMI at 30\")"},{"path":"least-squares.html","id":"the-fitting-problem","chapter":"5 Least Squares","heading":"5.2 The fitting problem","text":"","code":""},{"path":"least-squares.html","id":"the-residual-vector","chapter":"5 Least Squares","heading":"5.3 The residual vector","text":"Residual connects Mx b. (Review B5-projection.) Idea fitting, make residual small possible. Using sum squares let’s us translate problem projection problem.","code":""},{"path":"least-squares.html","id":"solving-m-x-b-part-ii","chapter":"5 Least Squares","heading":"5.4 Solving M x ≈ b: Part II","text":"previous found x given M b using R function qr.solve(M, b). operation solving important quantitative work many fields, numerous software programs package ways make easy use.One example re-packaging lm() function R. “lm” stands “linear modeling,” finds x solves M x ≈ b, also calculates numerous indicators statistical reliability result. lm() function one widely used R, R one widely used software systems data science, ’re going spend little time .mtcars data frame shown records several aspects design performance classic cars 50 years ago.variables include things fuel economy (mpg), cylinder displacement (disp), whether car automatic transmission () V-shaped engine (vs), number carburetors. (carb. Ask grandparent carburetor .) won’t concerned mechanics cars worked. (point poor fuel economy compared today’s cars. Datsun 710 small, low-performance car got just 22.8 mpg, much better Maserati Bora.) interest just mechanics meaning solving \\(\\overrightarrow{\\mathbf M}\\cdot \\vec{x} \\approx \\vec{b}\\).example, let’s consider time takes car go one-quarter mile standing start: variable qsec. let’s suppose hypothesize engine power (hp: horsepower) major factor. starting analysis might look like : graph data sketch straight-line model, quantitative calculation model using lm(). see run code following sandbox qsec tends go (, get faster) increasing horsepower. coefficients give slope intercept least-squares straight-line model: every 100 extra horsepower, quarter-second time goes (average) 1.8 seconds.::: {.example data-latex=\"\"}Turn question example.\nQuestion F coefficients indicates qsec goes (average) hp?sign hp coefficient.Right! Nothing coefficients. ’s graph shows .︎✘ graph indeed show going-pattern. fact, straight line shown graph exactly represented coefficients. coefficients contain information need draw line gf_lm() layerThat intercept greater hp coefficient.︎✘ “R-squared” simple statistical summary model. typical description R-squared might , “hp explains 47% variation qsec.” (’s much said statistics model. can see standard statistical report using command summary(model).)","code":"\ngf_point(qsec ~ hp, data = mtcars) %>%\n  gf_lm(interval = \"confidence\")\nmodel <- lm(qsec ~ hp, data = mtcars)\ncoef(model) # report model coefficients## (Intercept)          hp \n## 20.55635402 -0.01845831\nrsquared(model) # report R-squared## [1] 0.5015804"},{"path":"least-squares.html","id":"lm-and-qr.solve","chapter":"5 Least Squares","heading":"5.5 lm() and qr.solve","text":"section, ’re going see qr.solve() essential engine lm(). following sandbox repeats model-building command omits graphic (save space). ’re going add additional commands translate model qr.solve() equivalent.inputs qr.solve() matrix M target vector b.linear modeling, matrix M called “model matrix.” ’s convenient function, model.matrix() construct matrix tilde expression data frame. Notice arguments model.matrix() exactly corresponding call lm(). Run code see resulting M.::: {.example data-latex=\"\"}\nTurn example names given coefficients.\nQuestion G model.matrix() function gives names individual vectors M. names?(Intercept) hpExcellent! Merc 230 Pontiac Firebird︎✘ \\(\\vec{u}\\) \\(\\vec{v}\\)︎✘ [[1] 0 1︎✘ Block 1, variety functions something common, …\\[f(x) \\equiv m x + B\\\\\ng(x) \\equiv \\sin(\\frac{2\\pi}{P}t) + C\\\\\nh(x) \\equiv e^{k t} + C\\]Question: Explain (Intercept) vector relates one coefficients functions.Now \\(\\vec{b}\\). create target vector \\(\\vec{b}\\), lm() function looks variable named left side tilde expression (namely, qsec). simply grabs variable, unaltered, data frame. can directly adding following command end sandbox:Question: constructed M b described , use find \\vec{x} satisfies M\\vec{x}≈\\vec{b}. Explain x matches model created lm().","code":"\nmodel <- lm(qsec ~ hp, data = mtcars)\nM <- model.matrix(qsec ~ hp, data = mtcars)\nM##                     (Intercept)  hp\n## Mazda RX4                     1 110\n## Mazda RX4 Wag                 1 110\n## Datsun 710                    1  93\n## Hornet 4 Drive                1 110\n## Hornet Sportabout             1 175\n## Valiant                       1 105\n## Duster 360                    1 245\n## Merc 240D                     1  62\n## Merc 230                      1  95\n## Merc 280                      1 123\n## Merc 280C                     1 123\n## Merc 450SE                    1 180\n## Merc 450SL                    1 180\n## Merc 450SLC                   1 180\n## Cadillac Fleetwood            1 205\n## Lincoln Continental           1 215\n## Chrysler Imperial             1 230\n## Fiat 128                      1  66\n## Honda Civic                   1  52\n## Toyota Corolla                1  65\n## Toyota Corona                 1  97\n## Dodge Challenger              1 150\n## AMC Javelin                   1 150\n## Camaro Z28                    1 245\n## Pontiac Firebird              1 175\n## Fiat X1-9                     1  66\n## Porsche 914-2                 1  91\n## Lotus Europa                  1 113\n## Ford Pantera L                1 264\n## Ferrari Dino                  1 175\n## Maserati Bora                 1 335\n## Volvo 142E                    1 109\n## attr(,\"assign\")\n## [1] 0 1b <- mtcars$qsec"},{"path":"least-squares.html","id":"r2-residuals-etc.","chapter":"5 Least Squares","heading":"5.6 R\\(^2\\), Residuals, etc.","text":"Little - rThe sandbox recapitulates commands used create M b qsec ~ hp model calculate x.Add sandbox command calculate \\(\\widehat{\\ {b}\\ }\\). Call vector bhat.Question H square length bhat? (Hint: square length even easier calculate length.)3780.4Good. 12︎✘ ’s many components bhat. length refers sizes components.3805.5︎✘ ’s length b. ’re asking bhat.bhat vector.︎✘ True. length vector?statistics, amount b explained model length vector closely related bhat. Add command sandbox:Similarly, amount explain length vector:simple relationship R-squared (47.1%) squared lengths explained_vec to_explain_vec. Figure say .","code":"\nM <- model.matrix(qsec ~ hp, data = mtcars)\nb <- mtcars$qsec\nx <- qr.solve(M, b)explained_vec <- bhat - mean(bhat)to_explain_vec <- b - mean(b)"},{"path":"least-squares.html","id":"modern-fitting-the-ridge-and-the-lasso-optional","chapter":"5 Least Squares","heading":"5.7 Modern fitting: The ridge and the lasso (optional)","text":"","code":""},{"path":"functions-as-vectors-1.html","id":"functions-as-vectors-1","chapter":"6 Functions as vectors","heading":"6 Functions as vectors","text":"chapter available current release MOSAIC Calculus","code":""}]
