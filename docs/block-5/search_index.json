[["vectors.html", "Chapter 41 Vectors 41.1 Length &amp; direction 41.2 The nth dimension 41.3 Geometry &amp; arithmetic 41.4 Vector lengths 41.5 Angles 41.6 Orthogonality 41.7 Exercises", " Chapter 41 Vectors Until now, our presentation of calculus has featured functions, sometimes expressed as formulas involving combinations of the basic modeling functions, sometimes generated directly from data by splines. Now we turn to a new framework for expressing functions, the inputs on which they operate, and the kind of outputs they generate. This framework is central to technical work in a huge range of fields. The usual name given to it by mathematicians is linear algebra, although only the word “linear” conveys useful information about the subject. The physicists developing the first workable quantum theory called it matrix mechanics. The framework is fundamental to scientific computation and is often the approach of choice even to non-linear problems. Application of the framework to problems of information access was the spark that ignited the modern era of search engines. Although the words “algebra” and “quantum” may suggest that conceptual difficulties are in store, in fact human intuition is well suited to establishing a useful understanding. We will use two formats to introduce linear algebra: (1) geometric and visual; (2) via simple arithmetic, numbers, and algorithms. 41.1 Length &amp; direction A vector is a mathematical idea that is deeply rooted in everyday physical experience. Geometrially, a vector is simply an object consisting only of length and direction. A pencil is a good physical metaphor for a vector, but a pencil has other, non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil always has position: the place it’s at. Figure 41.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are exactly the same vector. Pencils have position, but vectors don’t. The green pencil shares the same direction, but it has a different length, so it is a different vector from the blue/yellow vector. You can move in a given direction either forward or in reverse. To eliminate this ambiguity, it’s helpful to imagine vectors having a tip and a tail. For the pencil illustrations, the writing end it the tip and the eraser is the tail. Figure 41.2: Two different vectors. They have the same length and are parallel, but they point in opposite directions. Vectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a table top: a two-dimensional space. Naturally, the pencil-vectors are also embedded in our everyday three-dimensional space. The table-top can be thought of as a representation of a two-dimensional subspace of three-dimensional space. Often, we will use vectors to represent a change in position, that is, a step or displacement in the sense of “step to the left” or “step forward.” An individual vector describes a step of a specific length in a particular direction. Much of the useful mathematics of vectors can be understood as constructing instructions for reaching a target: “take three and a half steps along the green vector, then turn and take two steps backwards along the yellow vector.” Vectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are properly represented not as simple numerical quantities but as vectors with magnitude (that is, length) and direction. The statement, “The plane’s velocity is 450 miles per hour to the north-north-west” is perfectly intelligible to most people, describing magnitude and direction. Note that the vector velocity can be understood without having to know where the plane is located; vectors have only the two qualities of magnitude and direction. Position is irrelevant to describing velocity, or, for that matter, force or acceleration. The gradients that we studied with partial differentiation (Chapter 24) are vectors. A gradient’s direction points directly uphill from a given point; it’s magnitude tells how steep the hill is at that point. Vectors are a practical tool in many situations such as relative motion. Consider the problem of finding an aircraft heading and speed to intercept another plane that’s also moving. The US Navy training movie from the 1950s shows how such calculations used to be done with paper and pencil. Nowadays such relative motion calculations are computerized. You may well wonder how the computer is able to represent vectors, since pencils aren’t part of computer hardware. The answer is disappointingly simple: the properties of direction and magnitude can also be represented by a set of numbers. Two numbers will do for a vector embedded in two-dimensional space, three for a vector embedded in three-dimensional space. Representing a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 41.3, the vector (that is, the green pencil) has been placed in a coordinate system. Usually you would expect there to be labels for each of the coordinate lines, but this labeling is not necessarily to show a vector (even if it is needed to specify a position). The two coordinates to be assigned to the vector are the difference between the tip and the tail. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\). Figure 41.3: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper. By convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure 41.3, which we’ll call \\(\\vec{green}\\), is written numerically as: \\[\\vec{green} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] In more advanced linear algebra, the distinction between a column vector (like \\(\\vec{green}\\)) and a row vector (like \\(\\left[20 \\ 16\\right]\\)) is important. For our purposes in this block, we will only have need of column vectors. Column vectors can be constructed with the rbind() function, as in rbind(1,3,-4) ## [,1] ## [1,] 1 ## [2,] 3 ## [3,] -4 Note that the elements are separated by commas in the same way as any other R function. Later in this block, we will be using data frames to define vectors. We’ll introduce the R syntax for that when we need it. If you have previous experience with R, say in a statistics course, or if you regard yourself as an expert, please note the following. The R language has a data structure called a “vector,” which is a set of elements without the information needed to consider it a row or column vector. As such, the native R “vector” is not suited for linear-algebra computations in R. Many people construct mathematical vectors using the matrix() function. Such a matrix() command to produce a column vector would look like matrix(c(1, 3, -4), ncol=1). This is a professional practice, but we regard it as too verbose for our purposes in this book. We will use rbind() instead. Note that combining rbind() with c() or other preconstructed R “vectors” will not produce a mathematical row vector rbind(c(1,2,3)) ## [,1] [,2] [,3] ## [1,] 1 2 3 rbind(1:5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 Eventually, we will be constructing matrices. We’ll do this by concatenating column vectors, e.g. cbind( rbind( 1, 2, 3), rbind(10, 11, 12) ) ## [,1] [,2] ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 In physics and engineering vectors are used to describe positions, velocities, acceleration, forces, momentum, and other such functions of time or space. In mathematical notation, such a velocity can be written \\(\\vec{v}(t)\\). It’s common to perform calculus operations such a differentiation, writing it as \\(\\partial_t \\vec{v}(t)\\). The vector-valued function of time \\(\\vec{v}(t)\\) can also be written in terms of scalar-valued components assembled into a vector. For instance, a subscript is often used to identify which component is which, so that \\[\\vec{v}(t) = \\left[\\begin{array}{c}v_x(t)\\\\v_y(t)\\\\v_z(t)\\end{array}\\right]\\] where the \\(x\\), \\(y\\), and \\(z\\) refer to the axes of the coordinate system. Since the physics and engineering vectors are typically 2- or 3-dimensional, when working numerically there’s not much lost by keeping track of the components with a set of scalar quantities rather than as a vector. You saw this already in Chapter 33 when we represented the instantaneous vector position of a robot arm as a pair of scalar-valued functions \\(x(t)\\) and \\(y(t)\\). In linear algebra, vectors often have many more than 3 components. In this book, we will work with vectors with hundreds of components. Services like Google search rely on vector calculations with millions of components. When programming such systems, representing the vectors as individual scalar components is unwieldy. The programming must rely on handling the whole vector as a single entity. 41.2 The nth dimension Living as we do in a palpably three-dimensional space, and being part of a species whose senses and brains developed in three dimensions, it’s hard and maybe even impossible to get a grasp on what higher-dimensional spaces would be like. A lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character, Square, receives a visitor, Sphere, from the three-dimensional world in which Flatland is embedded. Only with difficulty can Square assemble a conception of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace. Even if the spatial extent of higher dimensions is not accessible, the one-dimensional vector inhabitants of any such space can be readily perceived and constructed as a list of numbers. With this device, allow us to introduce vectors from 4, 5, and 6 dimensions, and even \\(n\\) dimensional space. \\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\] Sensible people may consider it mathematical ostentation to promote an everyday column of numbers into a vector in high-dimensional space. The utility of doing so is to help us think about the arithmetic we are about to do on vectors in terms of familiar geometrical concepts: lengths, angles, alignment, and so on. Perhaps unexpectedly, it also guides us to think about data—which consists of columns of numbers in a data frame—using our powerful geometrical intuition. There’s nothing science-fiction-like about so-called “high-dimensional” spaces; they are not usually intended to correspond to a physical space. Vectors with many components are often used in advanced physics to represent the state of a particle. For instance, the state vector could contain both the position and velocity and might be written \\((x, y, z, v_x, v_y, v_z)\\), but you can easily see this as the concatenation of a position vector and a velocity vector, each of which is 3-dimensional. In statistics, engineering, and statistical mechanics, the term degrees of freedom is used as an alternative to “dimension.” Another example: computer-controlled machine tools are often described as having 5 degrees of freedom (or more). There is a cutting head whose \\(x, y, z\\) position can be set as well as the head’s orientation (tilt) as an azimuth and inclination. If ever you start to freak out about the idea of a 10-dimensional space, just close your eyes and remember that this is only shorthand for the set of arrays with 10 elements. 41.3 Geometry &amp; arithmetic There are three common mathematical tasks involving vectors that can be understood with simple geometry. Given a set of vectors drawn on paper, you can carry out these tasks by pencil assisted by a simple ruler and a protractor. Measure the length of a vector. Measure the angle between two vectors. Create a new vector by scaling a vector. Scaling makes the new vector longer or shorter or point in the opposite orientation, but the direction remains identical to the original. The geometrical perspective is helpful for many purposes, but often we need to work with vectors using computers. For this, we use the numerical representation of vectors. This section introduces the arithmetic of vectors. With this arithmetic in hand, we can carry out the above tasks (and more!) on vectors that consist of a column of numbers. Especially noteworthy is that the arithmetic enables to to apply simple geometrical concepts to vectors in three or more dimensions. To scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. A good mental image for scaling is based on thinking about the vector as a step or displacement in the direction of \\(\\vec{w}\\). Scaling means to go on a simple walk, taking one step after the other in the same direction as the \\(\\vec{w}\\). We write a scaled vector by placing a number in front of the name of the vector. \\(3 \\vec{w}\\) is a short walk of three steps; \\(117 \\vec{w}\\) is a considerably longer walk; \\(-5 \\vec{w}\\) means to take five steps backwards. You can also take fraction steps: \\(0.5 \\vec{w}\\) is half a step, \\(19.3 \\vec{w}\\) means to take 19 steps followed by a 30% step. Scaling a vector by \\(-1\\) means flipping the vector tip-for-tail; this doesn’t change the length, just the orientation. Arithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. Suppose that we are working with a vector \\(\\vec{v}\\) that has \\(n\\) components. (We’ll also define another vector \\(\\vec{w}\\) to use in examples.) \\[\\vec{v} \\equiv \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]\\] To scale a vector by 3 is accomplished by multiplying each component by 3 \\[3\\, \\vec{v} = 3\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right] = \\left[\\begin{array}{r}18\\\\6\\\\-12\\\\\\vdots\\\\3\\\\24\\end{array}\\right]\\] This is perfectly ordinary multiplication applied component by component, that is, componentwise. Scaling involves a number (the “scalar”) and a single vector. There are other sorts of multiplication however, that involve two or more vectors. The dot product is one approach to multiplication of one vector with another. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\[\\vec{v} \\bullet \\vec{w}\\]. The arithmetic of the dot product involves two steps: Multiply the two vectors component-wise. For instance: \\[\\underset{\\Large \\vec{v}}{\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]}\\ \\underset{\\Large \\vec{w}}{\\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]} = \\left[\\begin{array}{r}-18\\\\2\\\\20\\\\\\vdots\\\\2\\\\40 \\end{array}\\right]\\] Sum the elements in the component-wise product. For the component-wise product of \\(\\vec{v}\\) and \\(\\vec{w}\\), this will be \\(-18 + 2 + 20 + \\cdots +2 + 40\\). The resulting sum, which is an ordinary quantity, that is, a scalar, is the output of the dot product. That is, the dot product takes two vectors as inputs and produces a scalar as an output. R/mosaic provides a beginner-friendly function for computing a dot product. To mimic the use of the dot, as in \\(\\vec{v} \\bullet \\vec{w}\\), the function will be invoked using infix notation. You have a huge amount of experience with infix notation, even if you never heard the term. Some examples: 3 + 2 7 / 4 6 - 2 9 * 3 2 ^ 4 Infix notation is distinct from the functional notation that you are also familiar with, for instance sin(2) or makeFun(x^2 ~ x). In principle, you could invoke the +, -, *, /, and ^ operations using functional notation. Nobody does this because the commands are so ugly: `+`(3, 2) ## [1] 5 `/`(7, 4) ## [1] 1.75 `-`(6, 2) ## [1] 4 `*`(9, 3) ## [1] 27 `^`(2, 4) ## [1] 16   The R language makes it possible to define new infix operators, but there is a catch. The new operators must always have a name that begins and ends with the % symbol, for example %in% or %*% or %dot%. You’ll be using %*% and %dot% a lot in this block and the next. Here’s an example of using %dot% to calculate the dot product of two vectors: a &lt;- rbind(1, 2, 3, 5, 8, 13) b &lt;- rbind(1, 4, 2, 3, 2, -1) a %dot% b ## [1] 33 The vectors being combined with %dot% must both have the same number of elements. Otherwise, an error message will result: rbind(2, 1) %dot% rbind(3, 4, 5) ## Error in rbind(2, 1) %dot% rbind(3, 4, 5): Vector &lt;u&gt; must have the same number of elements as vector &lt;b&gt;. To the student encountering the dot product for the first time, a natural response is to wonder what such a two-step operation might be good for. As we progress through this block, you’ll see the dot product playing a central role. You will be seeing a lot of the dot product, so it’s important to have it firmly in mind that a dot product is not ordinary multiplication. 41.4 Vector lengths The arithmetic used to calculate the length of a vector is based on the Pythagorean theorem. For a vector \\(\\vec{u} = \\left[\\begin{array}{c}4\\\\3\\end{array}\\right]\\) the vector is the hypotenuse of a right triangle with legs of length 4 and 3 respectively. Therefore, \\[\\|\\vec{u}\\| = \\sqrt{4^2 + 3^2} = 5\\ .\\] For vectors with more than two components, follow the same pattern: sum the squares of the components then take the square root. The length of a vector \\(\\vec{u}\\) can also be computed using the dot product: \\[\\|\\vec{u}\\| = \\sqrt{\\strut\\vec{u} \\bullet \\vec{u}}\\ .\\] Although length has an obvious physical interpretation, in many areas of science including statistics and quantum physics, the square length is a more fundamental quantity. The square length of \\(\\vec{u}\\) is simply \\(\\|\\vec{u}\\|^2 = \\vec{u}\\bullet \\vec{u}\\). Example 41.1 Consider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) \\] The length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\). The length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\). In statistics, the many applications of linear algebra often involve a simple constant vector, which we’ll write \\(\\vec{1}\\). It is simply a column vector of 1s, \\[\\vec{1} \\equiv \\left[\\begin{array}{c}1\\\\1\\\\1\\\\\\vdots\\\\1\\\\1\\\\ \\end{array}\\right]\\ .\\] Common statistical calculations can be expressed compactly in vector notation. For example, if \\(\\vec{x}\\) is an \\(n\\)-dimensional vector, then the mean of the components of \\(\\vec{x}\\), which is often written \\(\\bar{x}\\), is \\[\\bar{x} \\equiv \\frac{1}{n}\\ \\vec{x} \\bullet \\vec{1}\\ .\\] The symbol \\(\\bar{}\\) is pronounced “bar”, and \\(\\bar{x}\\) is pronounced “x-bar.”. Another commonly used statistic is the variance of the components of a vector \\(\\vec{x}\\). This is only slightly more complicated than the mean: \\[\\text{var}(x) \\equiv \\frac{1}{n-1}\\ (\\vec{x} - \\bar{x}) \\bullet (\\vec{x} - \\bar{x})\\ .\\] The quantity \\(\\vec{x} - \\bar{x}\\) is an example of scalar subtraction, which is done on a component-wise basis. For instance, with \\[\\vec{x} = \\left[\\begin{array}{r}1\\\\2\\\\3\\\\4\\\\\\end{array}\\right]\\] then \\(\\bar{x} = 2.5\\). This being the case, \\[\\vec{x} - \\bar{x} = \\left[\\begin{array}{c}-1.5\\\\-0.5\\\\\\ 0.5\\\\\\ 1.5\\\\\\end{array}\\right]\\ ,\\] with the variance of \\(\\vec{x}\\) being \\[\\frac{1}{4-1} \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] \\bullet \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] = \\frac{5}{3}\\ .\\] 41.5 Angles Any two vectors of the same dimension have a distinct angle between them. This is easily seen for two-dimensional vectors. Draw two vectors on a sheet of paper. Since vectors have only two properties, length and direction, in your mind’s eye you can pick up one of the vectors and relocate its “tail” to meet the tail of the other vector. Measure the angle between two vectors the short way round: between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260 degree angle. In 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the vectors so they are tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector. It’s also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) that are in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components: \\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\\\\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\\\\\end{array}\\right]\\ ,\\] Using the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\] Remember that the dot-product-based formula above gives the cosine of the angle between the two vectors. It turns out that in many applications, the cosine is exactly what’s needed. If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), the trigonometric function \\(\\arccos()\\) will do the job. For instance, if \\(\\theta\\) is such that \\(\\cos(\\theta) = 0.6\\), compute the angle in degrees with acos(0.6)*180/pi ## [1] 53.1301 The trigonometric functions in R (and in most other languages) do calculations with angles in units of radians. The 180/pi is the factor that converts radians to degrees. Figure 41.4 shows a graph of converting \\(\\cos(\\theta)\\) to \\(\\theta\\) in degrees. Figure 41.4: The \\(\\arccos()\\) function (acos() in R) converts \\(\\cos(\\theta)\\) to \\(\\theta\\). What does the angle \\(\\theta\\) between two vectors tell us? In geometrical terms, the angle tells us how strongly aligned the vectors are. An angle of 0 tells us the vectors point in exactly the same direction, and angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that by appropriate scalar multiplication, the two vectors could be made exactly equal to one another and, consequently, that the scaled vectors would be one and the same. Angles such as 5 or 175 degrees indicate that the two vectors are mostly aligned, but imperfectly. When the angle is 90 degrees of course—a right angle—the two vectors are perpendicular. The vector alignment has a particularly important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics there is an important quantity called the correlation coefficient, denoted \\(r\\). To say that two variables are correlated means that the variables are connected to one another in some way. For instance, among children, height and age are correlated. Since height tends to increase along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\). A negative correlation means that as one variable increases the other tends to decrease. Temperature and elevation are negatively correlated, as are the pressure and volume of a gas at a given temperature. The most negative possible correlation is \\(r=-1\\). A zero correlation indicates that there is no simple relationship between the two variables. This occurs when the variables are orthogonal, a term described in Section 41.6. In terms of vectors, that is, the columns in the data frame, the correlation coefficient \\(r\\) is exactly the same quantity as the cosine of the angle between the vectors. At the time the correlation coefficient was invented in the 1880s, it was not widely appreciated that \\(r\\) is simply the cosine of an angle. Perhaps the several generations of statistics students who have studied correlation would have had a better grasp on the subject if it had been called alignment and measured in degrees. 41.6 Orthogonality Two vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech we call a 90 degree angle a “right angle.” The word “orthogonal” is really just a literal translation of “right angle.” (The syllable “gon” indicates an angle, as in the five-angled pentagon or six angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet).) Two vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero. Example 41.2   Find a vector that is orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\). The arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\). We can confirm the orthogonality by calculating the dot product: \\(\\left[\\begin{array}{c}-2\\\\\\ 1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\). In R, this can be written u &lt;- rbind( 1, 2) v &lt;- rbind(-2, 1) u %dot% v ## [1] 0 Example 41.3   Find a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\). We have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components. For instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\ \\text{__}\\\\ \\text{__}\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}\\text{__}\\\\0\\\\\\text{__}\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\). 41.7 Exercises Exercise 41.01 – dot products: pQLrbc Using these vectors \\[\\vec{u} \\equiv \\left[\\begin{array}{r} -100\\\\\\ 98\\\\\\ 9\\\\ -81\\\\\\ 90\\end{array}\\right]\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}\\ 21\\\\\\ 37\\\\\\ 93\\\\ -41\\\\ -93\\end{array}\\right]\\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r} -97\\\\ -11\\\\\\ 33\\\\ -49\\\\\\ 91\\end{array}\\right]\\] calculate the length of each of these vectors. (Hint: Save yourself trouble by doing it in R.)         \\(2 \\vec{u}\\)       \\(- 2 \\vec{u}\\)         \\(4 \\vec{u}\\)         \\(17 \\vec{v}\\)      \\(- 1.5 \\vec{w}\\) Exercise 41.03 – Vector lengths: QPShi2 Using R/mosaic and the %dot% and sqrt() functions, calculate the length of each of these vectors: \\[\\vec{u} \\equiv \\left[\\begin{array}{r}\\ 35\\\\ -93\\\\ -99\\end{array}\\right]\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r} -59\\\\\\ 41\\\\ -41\\end{array}\\right]\\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r} -76\\\\ -71\\\\ -48\\end{array}\\right]\\ \\ \\ \\vec{x} \\equiv \\left[\\begin{array}{r} -96\\\\\\ 83\\\\\\ 35\\end{array}\\right]\\] Exercise 41.05 – Scalar multipliers: UsntjP Consider this vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}\\ 470\\\\\\ 210\\\\ -430\\end{array}\\right]\\ .\\] For each of the following vectors, calculate the scalar multiplier \\(\\alpha\\) such that \\(\\alpha \\vec{x}\\) equals the vectors. If there is no such multiplier, say why. \\[\\vec{a} \\equiv \\left[\\begin{array}{r}\\ 282\\\\\\ 126\\\\ -258\\end{array}\\right]\\ \\ \\ \\vec{b} \\equiv \\left[\\begin{array}{r} -376\\\\ -169\\\\\\ 344\\end{array}\\right]\\ \\ \\ \\vec{c} \\equiv \\left[\\begin{array}{r}\\ 752\\\\\\ 336\\\\ -688\\end{array}\\right]\\ \\ \\ \\vec{d} \\equiv \\left[\\begin{array}{r} -1128\\\\ -504\\\\\\ 1032\\end{array}\\right]\\] Hint: Try componentwise division. Exercise 41.07 – Orthogonality: JPtnJM Here are four vectors. \\[\\vec{u} \\equiv \\left[\\begin{array}{r}\\ 18\\\\\\ 79\\\\\\ 33\\\\ -41\\end{array}\\right]\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r} -35\\\\ -62\\\\ -32\\\\ -7\\end{array}\\right]\\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r} -44\\\\\\ 81\\\\\\ 74\\\\ -4\\end{array}\\right]\\ \\ \\ \\vec{x} \\equiv \\left[\\begin{array}{r} -9\\\\\\ 71\\\\ -69\\\\\\ 33\\end{array}\\right]\\] Which one of the above vectors is orthogonal to \\[\\vec{z} \\equiv \\left[\\begin{array}{r} -1\\\\ -10\\\\\\ 20\\\\\\ 5\\end{array}\\right]\\ ?\\] Exercise 41.09 – Angles: Ihil7U A. Calculate the cosine of the angle between this vector \\[\\vec{z} \\equiv \\left[\\begin{array}{r} -1\\\\ -10\\\\\\ 20\\\\\\ 5\\end{array}\\right]\\ ?\\] and each of the following vectors: \\[\\vec{u} \\equiv \\left[\\begin{array}{r}\\ 99\\\\\\ 35\\\\ -25\\\\\\ 92\\end{array}\\right]\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r} -49\\\\ -92\\\\\\ 84\\\\ -65\\end{array}\\right]\\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r} -13\\\\ -39\\\\ -55\\\\\\ 65\\end{array}\\right]\\ \\ \\ \\vec{x} \\equiv \\left[\\begin{array}{r} -35\\\\\\ 73\\\\\\ 33\\\\\\ 14\\end{array}\\right]\\] B. Translate each of the cosines from part (A) into degrees of the angle. (Rounding to the nearest degree is fine.) Exercise 41.11: 5zmmBu Consider this set of vectors Find the lengths of each of these vectors. Assume that the vectors begin and end exactly on the graph-paper intersections. Two pairs of vectors in this set are orthogonal. Which ones? Just using your eye, say whether the dot product between every pair of vectors is positive, zero, or negative. A &amp; B A &amp; C A &amp; D A &amp; E B &amp; C B &amp; D B &amp; E C &amp; D C &amp; E D &amp; E Exercise 41.15 – Collision course?: MBTGQt Collision course? Consider the diagram showing two straight-line tracks, a dot on each track, and a vector. Let’s imagine that dot 1 is an aircraft and that the black vector attached to it is the aircraft’s velocity. We’ll call this \\(\\vec{v}_1\\), Similarly for dot 2, where the velocity vector will be called \\(\\vec{v}_2\\). There’s a third vector drawn in red: the difference in position of the two aircraft at the exact moment depicted in the drawing. The question we want to address is whether the aircraft are on a collision course. Obviously, the two courses cross. So we know that the two aircraft will cross the same point. For a collision, the aircraft have to cross that point at the same time. Copy over the drawing to your own piece of paper. You don’t need to get the vectors and positions exactly right; any reasonable approximation will do. Now you are going to do visual vector addition and subtraction to answer the collision question. The relative velocity of the two planes is the difference between their velocities. Subtract \\(\\vec{v}_2\\) from \\(\\vec{v}_1\\) and draw the resulting vector. Pay attention to both the length and direction of the relative velocity. The displacement between the two planes is the red vector: the position of dot 2 subtracted from dot 1. Compare the directions of the relative velocity vector and the displacement vector. If they are aligned, then the planes are on a collision course. In the picture as drawn, the relative velocity vector and the displacement vector are not aligned. Figure out how much you would need to change the length of \\(\\vec{v}_2\\) so that the relative velocity does align with the displacement. (Keep the direction the same.) Draw this new vector and label it clearly “vector for intercept.” In (3) you changed the length of \\(\\vec{v}_2\\) keeping the direction the same. Now you are going to keep \\(\\vec{v}_2\\) at the original length, but change its direction so that the new relative velocity is aligned with the displacement vector. Items (3) and (4) are two different ways of designing an intercept of plane 1 by plane 2. Bonus) You can figure out how long it takes for each plane to reach the intersection point by finding out how many multiples of the velocity vector will cover the line segment between the plane’s position and the intersection point. For example, in the original drawing \\(4 \\vec{v}_1\\) will bring the plane to the intersection point, so it takes 4 “time units” for the plane to reach the point. (What is the time unit? If velocity is in miles/hour, then the time unit is hours. If the velocity is in feet/second, then the time unit is seconds.) Your task: Figure out where aircraft 2 will be in 4 time units. This will tell you the separation between aircraft 2 and aircraft 1 when 1 reaches the intersection point. Draw and clearly label this vector. Exercise 41.17: PCfYDo In physics and engineering, there is a very important operation called the cross product and written \\(\\vec{u}\\times\\vec{v}\\). The operation is only defined for vectors in 3-dimensional space. The output of the cross product is another 3-dimensional vector which can be calculated arithmetically as: \\[\\left(\\begin{array}{c}a\\\\b\\\\c\\end{array}\\right) \\times \\left(\\begin{array}{c}e\\\\f\\\\g\\end{array}\\right) \\equiv \\left(\\begin{array}{c}b\\, g - c\\, f\\\\c\\, e - a\\, g\\\\a\\, f - b\\, e\\end{array}\\right)\\] Make up coordinates for two three-dimensional vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) that point in different directions. Calculate the cross product \\(\\vec{w} \\equiv \\vec{u} \\times \\vec{v}\\). Find the angle between \\(\\vec{w}\\) and \\(\\vec{u}\\). Find the angle between \\(\\vec{w}\\) and \\(\\vec{v}\\). Other than this brief description, we will not use cross products at all in this course. But keep them in mind for your upcoming physics and engineering courses. "],["linear-combs-vectors.html", "Chapter 42 Linear combinations of vectors 42.1 Scaling vectors 42.2 Adding vectors 42.3 Linear combinations 42.4 Functions as vectors 42.5 Matrices and linear combinations 42.6 Sub-spaces 42.7 Exercises", " Chapter 42 Linear combinations of vectors In this chapter, we introduce linear combinations of vectors. As you recall, a linear combination is a sum of basic elements each of which has been scaled. For instance, in Block 1 we looked at linear combinations of functions such as \\[g(t) = A + B e^{kt}\\] which involves the basic functions \\(\\text{one}(t)\\) and \\(e^{kt}\\) scaled respectively by \\(A\\) and \\(B\\). Linear combinations of vectors involve scaling and addition, which are simple seen either as numerical operations or a geometric ones. A useful concept will be the set of all vectors that can be constructed as linear combinations of given vectors. This set of all possibilities, called the subspace spanned by the given vectors is key to understanding how to find the “best” scalars for a given purpose. 42.1 Scaling vectors To scale a vector means to change its length without altering its direction. Scaling by a negative number flips the vector tip-for-tail. Figure 42.1 shows two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) together with several scaled versions of each. Figure 42.1: Vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) and some scaled versions of them. The vectors we create by scalar multiplication can be placed anywhere we like. Arithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g. \\[\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\ -\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ \\] Every vector is associated with a subspace that is one-dimensional; you can only reach the points on a line by stepping in the direction of a vector. 42.2 Adding vectors To add two vectors, choose either one of the vectors as a start, then move the tail of the second vector to the tip of the first, as in Figure 42.2. Figure 42.2: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector. Adding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to whatever place is convenient. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result—the blue pencil in the picture above—has the length and direction from the tail of the first pencil (yellow) to the tip of the second (green). But so long as we maintain this length and direction, we can put the result (blue) anywhere we want. Arithmetically, vector addition is simply a matter of applying addition component-by-component, that is, componentwise. For instance, consider adding two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\): \\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}\\] Unlike our pencil exemplars of vectors, which must of physical necessity always be in the three-dimensional space we inhabit, mathematical vectors can be embedded in any-dimensional space. Addition is applicable to vectors embedded in the same space. Arithmetically, this means that the two vectors to be added must have the same number of components. Arithmetic subtraction of one vector from another is a simple component-wise operation. For example: \\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .\\] From a geometrical point of view, many people like to think of \\(\\vec{v} - \\vec{w}\\) in terms of placing the two vectors tail to tail as in Figure 42.3. Read out the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 42.3, the yellow vector is \\(\\vec{v}\\), the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector. Figure 42.3: Subtracting blue from yellow gives green. 42.3 Linear combinations In the previous chapter, we suggested that you think of a vector as a “step” or displacement in a given direction and of a given magnitude as in, “1 foot to the northeast.” This interpretation highlights the mathematical structure of vectors: just a direction and a length, nothing else. The “step”-interpretation is also faithful to an important reason why vectors are useful. We use steps to get from one place to another. Similarly, a central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We’ve used quotation marks around “place” because we are not necessarily referring to a physical destination. We’ll get to what else we might mean by “place” later in this Block. As a fanciful example of getting to a “place,” consider a treasure hunt. You are given these instructions to get there: On June 1, go to the flagpole before sunrise. At 6:32, walk 213 paces away from the sun. At 12:19, walk 126 paces toward the sun. The sun position varies over the day, so the direction to the sun on June 1 at 6:32 will be different than at 12:19. Figure 42.4 the Sun vectors at 6:32 and 12:19 on June 1. Figure 42.4: For June 1: \\(\\color{magenta}{ ext{Sun&#39;s direction at 6:32}}\\) and $$. (Location: latitude 38.0091, /longitude -104.8871). Source: suncalc.org The treasure-hunt directions are in the form of a linear combination of vectors. For each of the two vectors described in the treasure instructions, the length of the vector is 1 pace. (Admittedly, not a scientific unit of length.) Scaling \\(\\color{magenta}{\\text{the magenta vector}}\\) by -213 and \\(\\color{blue}{\\text{the blue vector}}\\) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure. A stickler for details might point out tht the “direction of the sun” has an upward component. Common sense will guide you to walk in the direction of the Sun as projected onto Earth’s surface. Chapter 43 deals with projections of vectors. 42.4 Functions as vectors This is a calculus book, and calculus is about functions. So you can imagine that there is going to be some connection between functions and vectors. We’ll start with the idea that a vector is a column of numbers. Recall from Block 1 (Section 4.2) the idea of representing a function as a table of inputs and the corresponding outputs. Here is such a table with some of our pattern-book functions. t one(t) identity(t) exp(t) sin(t) pnorm(t) 0.0 1 0.0 1.000000 0.0000000 0.5000000 0.1 1 0.1 1.105171 0.0998334 0.5398278 0.2 1 0.2 1.221403 0.1986693 0.5792597 0.3 1 0.3 1.349859 0.2955202 0.6179114 0.4 1 0.4 1.491825 0.3894183 0.6554217 … and so on … 4.6 1 4.6 99.48432 -0.9936910 0.9999979 4.7 1 4.7 109.94717 -0.9999233 0.9999987 4.8 1 4.8 121.51042 -0.9961646 0.9999992 4.9 1 4.9 134.28978 -0.9824526 0.9999995 5.0 1 5.0 148.41316 -0.9589243 0.9999997 In this representation, each of the pattern-book functions is a column of numbers, that is, a vector. Functions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function \\(g(t) \\equiv 3 - 2 t\\) is \\(3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)\\) t one(t) identity(t) g(t) 0.0 1 0.0 3.0 0.1 1 0.1 2.8 0.2 1 0.2 2.6 0.3 1 0.3 2.4 0.4 1 0.4 2.2 … and so on … 4.6 1 4.6 -6.2 4.7 1 4.7 -6.4 4.8 1 4.8 -6.6 4.9 1 4.9 -6.8 5.0 1 5.0 -7.0 The table above is a collection of four vectors: \\(\\vec{\\mathtt t}\\), \\(\\vec{\\mathtt{ one(t)}}\\), \\(\\vec{\\mathtt{identity(t)}}\\), and \\(\\vec{\\mathtt{g(t)}}\\). Each of those vectors has 51 components. In math-speak, we can say that the vectors are “embedded in a 51-dimensional space.” The functions in the table are being represented as discrete values. Still, a table, combined with interpolation (Chapter 33) can produce a continuum. 42.5 Matrices and linear combinations A collection of vectors, such as the one displayed in the previous table, is called a matrix. Each of the vectors in a matrix must have the same number of components. As mathematical notation, we will use bold-faced, capital letters to stand for matrices, for example \\(\\mathit{M}\\). The symbol \\(\\rightleftharpoons\\) is a reminder that a matrix can contain multiple vectors, just as the symbol \\(\\rightharpoonup\\) in \\(\\vec{v}\\) reminds us that the name “\\(v\\)” refers to a vector. In the conventions for data, we give a name to each column of a data frame so that we can refer to it individually. In the conventions used in vector mathematics, single letter are used to refer to the individual vectors. As a case in point, let’s look at a matrix \\(\\mathit{M}\\) containing the two vectors which we’ve previously called \\(\\vec{\\mathtt{one(t)}}\\) and \\(\\vec{\\mathtt{identity(t)}}\\): \\[\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 &amp; 0\\\\ 1 &amp; 0.1\\\\ 1 &amp; 0.2\\\\ 1 &amp; 0.3\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; 4.9\\\\ 1 &amp; 5.0\\\\ \\end{array}\\right]\\ .\\] The linear combination which we might previous have called \\(3\\cdot \\vec{\\mathtt{t}} - 2\\,\\vec{\\mathtt{identity(t)}}\\) can be thought of as \\[\\left[\\overbrace{\\begin{array}{r} 1\\\\ 1 \\\\ 1 \\\\ 1 \\\\ \\vdots &amp;\\\\ 1 \\\\ 1 \\end{array}}^{3 \\times} \\stackrel{\\begin{array}{r} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{array}}{\\Large + \\ } \\overbrace{\\begin{array}{r} 0\\\\ 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ \\vdots\\\\ 4.9 \\\\ 5.0 \\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r} \\\\ \\\\ 3\\\\ 2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\ \\end{array}\\right]\\ ,\\] but this is not conventional notation. Instead, we would write this more concisely as \\[\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 &amp; 0\\\\ 1 &amp; 0.1\\\\ 1 &amp; 0.2\\\\ 1 &amp; 0.3\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; 4.9\\\\ 1 &amp; 5.0\\\\ \\end{array}\\right]} \\ \\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}\\] In symbolic form, the linear combination of the columns of \\(\\mathit{M}\\) using respectively the scalars in \\(\\vec{w}\\) is simply \\(\\mathit{M} \\, \\vec{w}\\). This is called matrix multiplication. Naturally, the operation only makes sense if there are as many components to \\(\\vec{w}\\) as there are columns in \\(\\mathit{M}\\). “Matrix multiplication” might better have been called “\\(\\mathit{M}\\) linearly combined by \\(\\vec{w}\\).” But “matrix multiplication” is the standard term for such linear combinations. In R, you can make vectors with the rbind() command, short for “bind rows,” as in rbind(2, 5, -3) ## [,1] ## [1,] 2 ## [2,] 5 ## [3,] -3 with the components of the vector presented as successive arguments to the function. One way to make a matrix is with the cbind() command, short for “bind columns”. The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands. u &lt;- rbind(2, 5, -3) v &lt;- rbind(1, -4, 0) A &lt;- cbind(u, v) A ## [,1] [,2] ## [1,] 2 1 ## [2,] 5 -4 ## [3,] -3 0 To compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that’s easy to read: x &lt;- rbind(3, 1) A %*% x ## [,1] ## [1,] 7 ## [2,] 11 ## [3,] -9 It’s a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with: A * x ## Error in A * x: non-conformable arrays The phrase “non-conformable arrays” is R-speak for saying \"I don’t know how to do component-wise multiplication with two differently shaped objects. In chapters to come, we will sometimes make several different linear combinations of the vectors in a matrix. Of course the result of each individual linear combination will be a vector, so the “several different linear combinations” can be thought of as a collection of vectors, that is, a matrix. For example, consider the possible linear combinations of the two vectors in a matrix \\[\\mathit{A} = \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ .\\] The combinations we have in mind are: \\[ \\mathit{A}\\left[\\begin{array}{r}3\\\\1\\end{array}\\right]= \\left[\\begin{array}{r}7\\\\11\\\\-9\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathit{A} \\left[\\begin{array}{r}-0\\\\2\\end{array}\\right]= \\left[\\begin{array}{r}2\\\\-8\\\\0\\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathit{A} \\left[\\begin{array}{r}-1\\\\0\\end{array}\\right] = \\left[\\begin{array}{r}-2\\\\-5\\\\3\\end{array}\\right] \\] A more concise way to write this collects the vectors with the values for the scalars into a matrix, which we’ll call \\[\\mathit{X} \\equiv \\left[\\begin{array}{rr}3 &amp; 0 &amp; -1\\\\1 &amp; 2 &amp; 0\\end{array}\\right]\\ .\\] \\[\\mathit{A} \\ \\mathit{X} = \\left[\\begin{array}{r}7 &amp;2 &amp;-2\\\\11 &amp; -8 &amp; -5\\\\-9 &amp; 0 &amp; 3\\end{array}\\right]\\] In R, to create the set of linear combinations, we create the matrices \\(\\mathit{A}\\) and \\(\\mathit{X}\\) and combine them with matrix multiplication. A &lt;- cbind( rbind( 2, 5, -3), rbind( 1, -4, 0) ) A is a collection of two vectors. Therefore, each vector in X must have two components: one for each vector in A X &lt;- cbind( rbind( 3, 1), rbind( 0, 2), rbind(-1, 0) ) The overall result will be a new matrix, containing three vectors, one for each vector in X: A %*% X ## [,1] [,2] [,3] ## [1,] 7 2 -2 ## [2,] 11 -8 -5 ## [3,] -9 0 3 42.6 Sub-spaces Recall that a vector with \\(n\\) components can be said to be embedded in an \\(n\\)-dimensional space. You might like to think of the embedding space as a kind of club with restricted membership. A vector with 2 elements is entitled to join the 2-dimensional club, but a vector with more or fewer than 2 elements cannot be admitted to the club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on. The clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded. Now imagine the clubhouse arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace. A subspace has its own rules for admission. Vectors belong to the subspace only if they can be constructed as a linear combination of the sponsoring members. Mathematically, although the subspace is defined by the founding vectors, the subspace itself consists f all the possible vectors can be constructed by a linear combination of the sponsors. As an example, consider the clubhouse that is open to any and all vectors with three components. The diagram in Figure ?? shows the clubhouse with just two members present, \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\). Any vector can sponsor its own subspace. In Figure ?? the subspace sponsored by \\(\\color{blue}{\\vec{u}}\\) is the extended line through \\(\\color{blue}{\\vec{u}}\\), that is, all the possible scaled versions of \\(\\color{blue}{\\vec{u}}\\). Similarly, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) is the extended line through \\(\\color{magenta}{\\vec{v}}\\). Each of these subspaces is one-dimensional. Multiple vectors can sponsor a subspace. The subspace sponsored by both \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) contains all the vectors that can be constructed as linear combinations of \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\). In Figure 42.5, this subspace is shown in gray. Figure 42.5: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown as a gray surface. On the other hand, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) is not the entire clubhouse. \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) lie in a common plane, but not all the vectors in the 3-dimensional clubhouse lied in that plane. In fact, if you rotate Figure ?? to “look down the barrel” of either \\(\\color{magenta}{\\vec{v}}\\) or \\(\\color{blue}{\\vec{u}}\\), the plane will entirely disappear from view. A subspace is an infinitesimal slice of the embedding space. “Sponsored a subspace” is metaphorical. In technical language we speak of the subspace spanned by a set of vectors in the same embedding space. Usually, we refer to a “set of vectors” as a matrix. For instance, letting \\[\\mathit{M} \\equiv \\left[{\\Large \\strut}\\color{blue}{\\vec{u}}\\ \\ \\color{magenta}{\\vec{v}}\\right]\\ ,\\] the gray plane in Figure 42.5 is the subspace spanned by \\(\\mathit{M}\\) or, more concisely, \\(span(\\mathit{M})\\). For a more concrete, everyday representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. You can point the whole rigid assembly in any direction you like. The angle between them will remain the same. Place a card on top of the pencils, slipping it between your pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determine the orientation of the surface. This simple fact will be extremely important later on. You could replace the pencils with line segments drawn on the card underneath each pencil. Now you have the angle readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors. Notice that you can also lay a card along a single vector. What’s different here is that you can roll the card around the pencil; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not determine uniquely the orientation of the planar surface in which the two vectors can reside. But with two fixed vectors, there is only one such surface. 42.7 Exercises Exercise 42.01 – Adding vectors: k5u7hG Here are 12 vectors, labeled “a” through “m.” (Letter “i” has been left out.) There are several quick questions, each of which makes a claim about whether the sum of two vectors equals a third. Answer true or false to the claim. There are no tricks about exactitude, so if the claim is close to being true, answer true. \\(\\vec{a} + \\vec{b} = \\vec{L}\\) \\(\\vec{b} + \\vec{J} = \\vec{a}\\) \\(\\vec{b} + \\vec{m} = \\vec{J}\\) \\(\\vec{c} + \\vec{f} = \\vec{d}\\) \\(\\vec{k} + \\vec{L} = \\vec{e}\\) \\(\\vec{e} + \\vec{b} = \\vec{m}\\) \\(\\vec{m} + \\vec{g} = \\vec{b}\\) Exercise 42.03: Q2ars0 Copy over and label these vectors onto your paper. Any good approximation will do. Then, on paper, draw (and label with the Roman numeral) the following vector additions and subtractions. (You should “show your work” by putting the vectors being added in the diagram along with the result of the addition.) \\(\\vec{a} + \\vec{b}\\) \\(\\vec{c} + \\vec{d}\\) \\(\\vec{d} + \\vec{b} + \\vec{a}\\) \\(\\vec{b} - \\vec{a}\\) Exercise 42.05: N98zli Referring to the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), and \\(\\vec{d}\\) in the figure, construct these linear combinations. Your diagram should show both the scaled vectors being added and the output of the linear combination. \\(\\ \\ \\ \\ 2 \\vec{a} + 1 \\vec{b}\\) \\(\\ \\ \\ \\ 1.5 \\vec{c} - 2 \\vec{d}\\) \\(\\ \\ \\ \\ - \\vec{b} + 2\\vec{c} + 3\\vec{d}\\) Exercise 42.07: xG8ePG Figure 42.4 is a kind of treasure map. The starting point (the flagpole) is in the center of the map. Find the treasure according to the instructions given in the text: 213 paces away from the sun at 6:32, then 126 paces toward the sun at 12:19. Assume that each square of the grid is 15 paces on a side. Exercise 42.09 – linear combinations: ffHdu3 Consider these vectors: \\[\\vec{u} \\equiv \\left[\\begin{array}{r} -81382\\\\ -9645\\\\ -5099\\end{array}\\right]\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}\\ 61713\\\\ -45063\\\\ -51427\\end{array}\\right]\\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}\\ 90176\\\\\\ 57269\\\\ -27150\\end{array}\\right]\\\\ \\vec{x} \\equiv \\left[\\begin{array}{r} -93470\\\\ -85812\\\\\\ 93693\\end{array}\\right]\\ \\ \\ \\vec{y} \\equiv \\left[\\begin{array}{r}\\ 14751\\\\\\ 65466\\end{array}\\right]\\ \\ \\ \\vec{z} \\equiv \\left[\\begin{array}{r}\\ 30370\\\\\\ 645\\\\ -45350\\end{array}\\right]\\] Construct R versions of these vectors, named u through z. Then, using R, construct these linear combinations. If the operation is invalid, say why. \\(\\vec{u} + 2 \\vec{v}\\) \\(6 \\vec{y} - 4\\vec{z}\\) \\(-2 \\vec{w} + 0 \\vec{u}\\) \\(- \\vec{z} - \\vec{u}\\) \\(\\vec{u} + \\vec{y}\\) \\(2 \\vec{u} - 4\\vec{} + 6 \\vec{z}\\) Exercise 42.11: 9bAVr2 Locating WW I aircraft The photograph shows part of an aircraft detection system from World War I. The concrete block is an “acoustic mirror.” Its purpose is to collect and reflect sounds from an aircraft, concentrating them at a point where they can be picked up by a microphone. Moving the microphone to a point where the concentrated sound is strongest allows the aircraft’s bearing to be identified, helping observers acquire the aircraft visually. Source With two or more such acoustic mirrors, the location of the aircraft can be identified. Question A Give the position of the aircraft as a multiple of \\(\\vec{a}\\) from sound mirror A and as a multiple of \\(\\vec{b}\\) from sound mirror B. (Choose the closest answer) \\(3 \\vec{a}\\) and \\(4.5 \\vec{b}\\)Nice!  \\(4 \\vec{a}\\) and \\(6 \\vec{b}\\)︎✘ \\(3\\vec{a}\\) and \\(2 \\vec{b}\\)︎✘ \\(4\\vec{a}\\) and \\(4.5\\vec{b}\\)︎✘ Exercise 42.13: l5UR71 Define the vector \\[\\vec{v} \\equiv \\left[\\begin{array}{r}4\\\\1\\\\3\\\\-2\\end{array}\\right]\\ .\\] Your task is to construct different vectors that are orthogonal to \\(\\vec{v}\\). You can use the trick presented in Section 41.6 of creating templates with zero in all but two of the positions, e.g. \\[\\left[\\begin{array}{r}0\\\\0\\\\\\text{__}\\\\\\text{__}\\end{array}\\right] \\ \\ \\text{or}\\ \\ \\ \\left[\\begin{array}{r}0\\\\\\text{__}\\\\0\\\\\\text{__}\\end{array}\\right] \\ \\ \\text{and}\\ \\ \\ \\left[\\begin{array}{r}\\text{__}\\\\\\text{__}\\\\0\\\\0\\end{array}\\right]\\] To construct a vector orthogonal to \\(\\vec{v}\\), fill in the blanks by taking the corresponding elements of \\(\\vec{v}\\), swapping them, and negating one of them. For example, taking the first template will produce \\[\\left[\\begin{array}{c}0\\\\0\\\\\\underline{\\ 2\\ }\\\\\\underline{\\ 3\\ }\\end{array}\\right]\\ .\\] Fill in the blanks of the other two tempates to create vectors orthogonal to \\(\\vec{v}\\). Construct 3 new vectors by taking different linear combinations of the vectors you created in (A). Are any of these new vectors orthogonal to \\(\\vec{v}\\)? Show your work. Prove algebraically that any linear combination of a set of vectors orthogonal to \\(\\vec{x}\\) will itself be orthogonal to \\(\\vec{x}\\). Exercise 42.15: K8hC6q ::: {.underconstruction} This will be an exercise about translating compass directions into linear combinations. Some initial ramblings … Pirates and other mariners use direction terms like “one point north of north-north-east.” Their maps are annotated with compass roses that translate the words into a direction. Mathematicians can replace a compass rose with just two vectors, say, \\(\\overset{\\longrightarrow}{\\text{North}}\\) and \\(\\overset{\\longrightarrow}{\\text{East}}\\). Other directions can be given as a linear combination. For instance, the compass rose’s “north-north-west” is the linear combination \\(0.9239\\,\\overset{\\longrightarrow}{\\text{North}} -0.3827\\,\\overset{\\longrightarrow}{\\text{East}}\\). ::: "],["projection-residual.html", "Chapter 43 Projection &amp; residual 43.1 Projection terminology 43.2 Projection onto a single vector 43.3 Projection onto a set of vectors 43.4 A becomes Q 43.5 Exercises", " Chapter 43 Projection &amp; residual Many problems in physics and engineering involve the task of decomposing a vector \\(\\vec{b}\\) into two perpendicular component vectors \\(\\hat{b}\\) and \\(\\vec{r}\\), such that \\(\\hat{b} + \\vec{r} = \\vec{b}\\) and \\(\\hat{b} \\cdot \\vec{r} = 0\\). There is an infinite number of ways to accomplish such a decomposition, one for each way or orienting \\(\\hat{b}\\) relative to \\(\\vec{b}\\). Figure 43.1 shows a few examples. This is the first time that we are encountering a symbol like \\(\\hat{b}\\), pronounced “b-hat.” You will see it especially in statistics and machine learning. Figure 43.1: A few ways of decomposing \\(\\vec{b}\\) into perpendicular components \\(\\hat{b}\\) and \\(\\vec{r}\\) Example 43.1 Gravitational force, as you know, always points downward. The effective acceleration due to gravity of a mass depends, however, on how that mass is situated with respect to other elements of the structure. The figure below shows several diagrams that might well be found on the pages of a physics textbook. In each diagram, there is a mass and a constraining structure: a ramp, a pendulum, an inclined plane. The force of gravity on the mass always points directly downward. In each diagram, \\(\\hat{b}\\) is the effective gravitational force on the mass, pointing down the ramp, or perpendicular to the pendulum strut, or aligned with the gradient vector of the inclined plane. The \\(\\vec{r}\\) in each diagram gives the component of gravitational force that will be counter-acted by the structure: the pull downward into the ramp, the pull along the pendulum strut, or the pull into the inclined plane. The task of decomposition is important also outside of physics and engineering. Our particular interest will be in finding how best to take a linear combination of the columns of a matrix \\(\\mathit{A}\\) in order to make the best approximation to a given vector \\(\\vec{b}\\). This problem solves all sorts of problems: finding a linear combination of functions to match a relationship laid out in data, constructing statistical models such as those found in machine learning, effortlessly solving sets of simultaneous linear equations with any number of equations and any number of unknowns. 43.1 Projection terminology The problem of decomposition can be considered to be a special case of projection. The word “projection” may bring to mind the casting of shadows on a screen in the same manner as an old-fashioned slide projector or movie projector. The light source is arranged to generate parallel rays which arrive perpendicularly to the screen. A movie screen is two-dimensional, a subspace defined by two vectors. Imagining those two vectors to be collected into matrix \\(\\mathit{A}\\), the idea is to decompose \\(\\vec{b}\\) into a component that lies in the subspace defined by \\(\\mathit{A}\\) and another component that is perpendicular to the screen. That perpendicular component is what we have been calling \\(\\vec{r}\\) while the vector \\(\\hat{b}\\) is the projection of \\(\\vec{b}\\) onto the screen. To make it easier to keep track of the various roles played by \\(\\vec{b}\\), \\(\\hat{b}\\), \\(\\vec{r}\\) and \\(\\mathit{A}\\), we’ll give these vectors English-language names. The motivation for these names will become apparent in later chapters, but for now, here they are. You will want to memorize them. \\(\\vec{b}\\) the target vector \\(\\hat{b}\\) the model vector \\(\\vec{r}\\) the residual vector \\(\\mathit{A}\\) the model space (or “model subspace”) Projection is the process of finding the model vector that is as close as possible to the target vector \\(\\vec{b}\\). Another way to see this is as finding the model vector that makes the residual vector as short as possible. Example 43.2 Figure 43.2 shows a a solved projection problem in 3-dimensional space. The figure can be rotated or set spinning, which makes it much easier to interpret the diagram as a three dimensional object. In addition to \\(\\vec{b}\\) and the vectors \\(\\vec{u}\\) and \\(\\vec{b}\\) that constitute the matrix \\(\\mathit{A}\\), the diagram includes a translucent plane marking \\(span(\\mathit{A})\\). The goal of projection is, from these givens, to find the model vector (shown in light green). Once the model vector \\(\\vec{x}\\) is known, the residual vector is easy to calculate \\[\\vec{r} \\equiv \\vec{b} - \\hat{b}\\ .\\] Another approach to the problem is to find the residual vector \\(r\\) first, then use that to find the model vector as \\[\\hat{b} \\equiv \\vec{b} - \\vec{r}\\ .\\] Figure 43.2: A three-dimensional diagram showing the target vector $ ec{b}$ and the vectors $ ec{u}$ and $ ec{v}$. The subspace spanned by $ ec{u}$ and $ ec{v}$ is indicated with a translucent plane. The model vector (green) is the result produced in solving the projection problem. Interpreting such three dimensional diagrams can be difficult. But there are tricks involving watching the diagram as it is rotated. For instance, how do we know that the translucent plane in Figure 43.2 contains \\(\\vec{u}\\) and \\(\\vec{v}\\)? As the diagram rotates, from time to time you will be looking edge on at the plane, so that the plane appears as a line on the screen. At such times, you can see that vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) disappear. There is no component to \\(\\vec{u}\\) and \\(\\vec{v}\\) that sticks out from the plane. 43.2 Projection onto a single vector As we said, projection involves a vector \\(\\vec{b}\\) and a matrix \\(\\mathit{A}\\) that defines the model space. We’ll start with the simplest case, where \\(\\mathit{A}\\) has only one column. That column is, of course, a vector. We’ll call that vector \\(\\vec{a}\\), so the projection problem is to project \\(\\vec{b}\\) onto the subspace spanned by \\(\\vec{a}\\). Geometrically, the situation of projecting the target vector \\(\\vec{b}\\) onto the model space \\(\\vec{a}\\) is diagrammed in Figure @ref{fig:b-onto-a}. Figure 43.3: The geometry of projecting \\(\\vec{b}\\) onto \\(\\vec{a}\\) to produce the model vector \\(\\hat{b}\\). The angle between \\(\\vec{a}\\) and \\(\\vec{b}\\) is labelled \\(\\theta\\). You already know how to calculate \\(\\theta\\) from \\(\\vec{b}\\) and \\(\\vec{a}\\) by using the dot product: \\[\\cos(\\theta) = \\frac{\\vec{b} \\bullet \\vec{a}}{\\len{b}\\, \\len{a}}\\ .\\] Knowing \\(\\theta\\) and \\(\\len{b}\\), you can calculate the length of the model vector \\(\\hat{b}\\): \\[\\len{s} = \\len{b} \\cos(\\theta) = \\vec{b} \\bullet \\vec{a} / \\len{a}\\ .\\] Scaling \\(\\vec{a}\\) by \\(\\len{a}\\) would produce a vector oriented in the model subspace, but it would have the wrong length: length \\(\\len{a} \\len{s}\\). So we need to divide \\(\\vec{a}\\) by \\(\\len{a}\\) to get a unit length vector oriented along \\(\\vec{a}\\): \\[\\text{model vector:}\\ \\ \\hat{b} = \\left[\\vec{b} \\bullet \\vec{a}\\right] \\,\\vec{a} / {\\len{a}^2} = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\ \\vec{a}.\\] . In R/mosaic, you can calculate the projection of \\(\\vec{b}\\) onto \\(\\vec{a}\\) using %onto%. For instance b &lt;- rbind(-1, 2) a &lt;- rbind(-2.5, -0.8) s &lt;- b %onto% a s ## [,1] ## [1,] -0.3265602 ## [2,] -0.1044993 Having found \\(\\hat{b}\\), the residual vector \\(\\vec{r}\\) can be calculated as \\(\\vec{b}- \\hat{b}\\). r &lt;- b - s r ## [,1] ## [1,] -0.6734398 ## [2,] 2.1044993 The two properties that a projection satisfies are: The residual vector is perpendicular to each and every vector in \\(\\mathit{A}\\). Since in this example, \\(\\mathit{A}\\) contains only the one vector \\(\\vec{a}\\), we need only look at \\(\\vec{r} \\cdot \\vec{a}\\) and confirm that it’s zero. r %dot% a ## [1] -2.220446e-16 The residual vector plus the model vector exactly equal the target vector. Since we computed r &lt;- b - s, we know this must be true, but still … (r+s) - b ## [,1] ## [1,] 0 ## [2,] 0 If the difference between two vectors is zero for every coordinate, the two vectors must be identical. 43.3 Projection onto a set of vectors As we have just seen, projecting a target \\(\\vec{b}\\) onto a single vector is a matter of arithmetic. Now we will expand the technique to project the target vector \\(\\vec{b}\\) onto multiple vectors collected into a matrix \\(\\mathit{A}\\). Whereas in the chapter we used trigonometry to find the component of \\(\\vec{b}\\) aligned with the single vector \\(\\vec{a}\\), now we have to deal with multiple vectors at the same time. The result will be the component of \\(\\vec{b}\\) aligned with the subspace sponsored by \\(\\mathit{A}\\). There is one situation where the projection is easy: when the vectors in \\(\\mathit{A}\\) are mutually orthogonal. In this situation, carry out several one-vector-at-a-time projections: \\[\\vec{p_1} = \\modeledby{\\vec{b}}{\\vec{v_1}}\\\\ \\vec{p_2} = \\modeledby{\\vec{b}}{\\vec{v_2}}\\\\ \\vec{p_3} = \\modeledby{\\vec{b}}{\\vec{v_3}}\\\\ \\text{and so on}\\] The projection of \\(\\vec{b}\\) onto \\(\\mathit{A}\\) will be the sum \\(\\vec{p_1} + \\vec{p2} + \\vec{p3}\\). Example 43.3 To illustrate the method of projection when the vectors in \\(\\mathit{A}\\) are mutually orthogonal, we can construct such a matrix. b &lt;- rbind( 1, 1, 1, 1) v1 &lt;- rbind( 1, 2, 0, 0) v2 &lt;- rbind(-2, 1, 3, 1) v3 &lt;- rbind( 0, 0, -1, 3) A &lt;- cbind(v1, v2, v3) You can verify using a dot product that v1, v2, and v3 are mutually orthogonal. Now construct the one-at-a-time projections: p1 &lt;- b %onto% v1 p2 &lt;- b %onto% v2 p3 &lt;- b %onto% v3 To find the projection of \\(\\vec{b}\\) onto the subspace spanned by \\(\\mathit{A}\\), add up the one-at-a-time projections: b_on_A &lt;- p1 + p2 + p3 Now we’ll confirm that b_on_A really is the projection of b onto A. The strategy is to construct the residual from the projection. resid &lt;- b - b_on_A All that’s needed is to confirm that the residual is perpendicular to each and every vector in A: resid %dot% v1 ## [1] 7.771561e-16 resid %dot% v2 ## [1] -2.220446e-16 resid %dot% v3 ## [1] 6.661338e-16 43.4 A becomes Q Now that we have a satisfactory method for projecting \\(\\vec{b}\\) onto a matrix \\(\\mathit{A}\\) consisting of mutually orthogonal vectors, we need to develop a method for the projection when the vectors in \\(\\mathit{A}\\) are not mutually orthogonal. The big picture here is that we will construct a new matrix \\(\\mathit{Q}\\) that spans the same space as \\(\\mathit{A}\\) but whose vectors are mutually orthogonal. We’ll construct \\(\\mathit{Q}\\) out of linear combinations of the vectors in \\(\\mathit{A}\\), so we can be sure that \\(span(\\mathit{Q}) = span(\\mathit{A})\\). We introduce the process with an example, involving a vectors in a 4-dimensional space. \\(\\mathit{A}\\) will be a matrix with two columns, \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\). Here’s the setup for the example vectors and model matrix: b &lt;- rbind(1,1,1,1) v1 &lt;- rbind(2,3,4,5) v2 &lt;- rbind(-4,2,4,1) A &lt;- cbind(v1, v2) We start the construction of the \\(\\mathit{Q}\\) matrix by pulling in the first vector in \\(\\mathit{A}\\). We’ll call that vector \\(\\vec{q_1}\\) q1 &lt;- v1 The next \\(\\mathit{Q}\\) vector will be constructed to be perpendicular to \\(\\vec{q_1}\\) but still in the subspace spanned by \\(\\left[{\\Large\\strut}\\vec{v_1}\\ \\ \\vec{v_2\\)]$. We can guarantee this will be the case by making the \\(\\mathit{Q}\\) vector entirely as a linear combination of \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\). q2 &lt;- v2 %perp% v1 since \\(\\vec{q_1}\\) and \\(\\vec{q_2}\\) are orthogonal and define the same subspace as \\(\\mathit{A}\\), we can construct the projection of \\(\\vec{b}\\) onto \\(\\vec{A}\\) by adding up the projections of \\(\\vec{b}\\) onto the individual vectors in \\(\\mathit{Q}\\), like this: bhat &lt;- (b %onto% q1) + (b %onto% q2) To confirm that this calculation of \\(\\hat{\\vec{b}}\\) is correct, construct the residual vector and confirm that it is perpendicular to every vector in \\(\\mathit{Q}\\) (and therefore in \\(\\mathit{A}\\), which spans the same space). r &lt;- b - bhat r %dot% v1 ## [1] 1.110223e-15 r %dot% v2 ## [1] 2.220446e-16 Note that we defined \\(\\vec{r} = \\vec{b} - \\hat{\\mathbf{b}}\\), so it’s guaranteed that \\(\\vec{r} + \\hat{\\mathbf{b}}\\) will equal \\(\\vec{b}\\). This process can be extended to any number of vectors in \\(\\mathit{A}\\). Here’s the algorithm for constructing \\(\\mathit{Q}\\): Take the first vector from \\(\\mathit{A}\\) and call it \\(\\vec{q_1}\\). Take the second vector from \\(\\mathit{A}\\) and find the residual from projecting it onto \\(\\vec{q_1}\\). This residual will be \\(\\vec{q_2}\\). At this point, the matrix \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\) consists of mutually orthogonal vectors. Take the third vector from \\(\\mathit{A}\\) and project it onto \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\). We can do this because we already have an algorithm for projecting a vector onto a matrix with mutually orthogonal columns. Call the residual from this projection \\(\\mathit{q_3}\\). It will be orthogonal to the vectors in \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\), so all three of the q vectors we’ve created are mutually orthogonal. Continue onward, taking the next vector in \\(\\mathit{A}\\), projecting it onto the q-vectors already assembled, and finding the residual from that projection. Repeat step (iv) until all the vectors in \\(\\mathit{A}\\) have been handled. Example 43.4 Project a \\(\\vec{b}\\) that lives in 10-dimensional space onto the subspace sponsored by five vectors that are not mutually orthgonal: b &lt;- rbind(3,2,7,3,-6,4,1,-1, 8, 2) # or any set of 10 numbers v1 &lt;- rbind(4, 7, 1, 0, 3, 0, 6, 1, 1, 2) v2 &lt;- rbind(8, 8, 4, -3, 3, -2, -4, 9, 6, 0) v3 &lt;- rbind(12, 0, 4, -2, -6, -4, -1, 4, 6, -7) v4 &lt;- rbind(0, 3, 9, 6, -4, -5, 4, 0, 5, -4) v5 &lt;- rbind(-2, 5, -4, 8, -9, 3, -5, 0, 11, -4) A &lt;- cbind(v1, v2, v3, v4, v5) You can confirm using dot products that the v-vectors are not mutually orthogonal. Now to construct the vectors in \\(\\mathit{Q}\\). q1 &lt;- v1 q2 &lt;- v2 %perp% q1 q3 &lt;- v3 %perp% cbind(q1, q2) q4 &lt;- v4 %perp% cbind(q1, q2, q3) q5 &lt;- v5 %perp% cbind(q1, q2, q3, q4) Q &lt;- cbind(q1, q2, q3, q4, q5) Since Q consists of mutually orthogonal vectors, the projection of b onto Q can be done one vector at a time. p1 &lt;- b %onto% q1 p2 &lt;- b %onto% q2 p3 &lt;- b %onto% q3 p4 &lt;- b %onto% q4 p5 &lt;- b %onto% q5 # put together the components b_on_A &lt;- p1 + p2 + p3 + p4 + p5 # check the answer: resid should be perpendicular to A resid &lt;- b - b_on_A resid %dot% v1 ## [1] 1.065814e-14 resid %dot% v2 ## [1] 4.973799e-14 resid %dot% v3 ## [1] 7.105427e-15 resid %dot% v4 ## [1] 0 resid %dot% v5 ## [1] 1.776357e-14 43.5 Exercises Exercise XX.XX: wniqMc Refer to the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), and \\(\\vec{d}\\) in the figure. Carry out the following projections graphically. You should show not only the result of the projection, but also the original vector being projected and the original vector being projected onto. Project \\(\\vec{a}\\) onto \\(\\vec{b}\\) Project \\(\\vec{c}\\) onto \\(\\vec{a}\\) Project \\(\\vec{b}\\) onto itself. Go back to your diagrams and add on to each diagram the residual vector from the projection. Make sure to use a different color ink or some other device to distinguish the residuals from the vectors you had already drawn. Exercise XX.XX: yeY17y Here is a direction, \\(\\vec{\\mathbf D}\\): \\[\\vec{\\mathbf D} \\equiv \\left(\\begin{array}{c}2\\\\5\\end{array}\\right)\\] Question A Find the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.59 \\vec{\\mathbf D}\\)Correct.  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question B Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(0.83, 2.07\\right)\\)Right!  \\(\\vec{\\mathbf R} = \\left(0.31, 1.28\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(1.28, 0.31\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.83, 0.31\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Question C Find the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.379 \\vec{\\mathbf D}\\)Nice!  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question D Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)^T\\)Excellent!  \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Exercise XX.XX: xSoxzL The text gives a formula for the scalar multiplier \\(\\alpha\\) such that \\(\\alpha \\vec{a} = \\modeledby{\\vec{b}}{\\vec{a}}\\): \\[\\alpha = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\ .\\] For a &lt;- rbind(5, -2, 3, 7) calculate the scalar multiplier \\(\\alpha\\) for each of these \\(\\vec{b}\\) vectors. b1 &lt;- rbind(1, 0, 0, 0) b2 &lt;- rbind(0, 1, 0, 0) b3 &lt;- rbind(0, 1, 2, 3) b4 &lt;- rbind(-4, 1, 2, 3) Exercise XX.XX: zK7fQn Construct a random 4-by-4 matrix named A whose columns are mutually orthogonal. Here’s the process: Construct a vector with four elements that has random elements. A command to do so is v1 &lt;- cbind(rnorm(4)) This will be the first column of the matrix. Construct a new vector with random elements. w &lt;- cbind(rnorm(4)) w will not be orthogonal to v1 as you can confirm by calculating the dot product between v1 and w. However, you can construct from the two vectors a new one that will be perpendicular to v1. v2 &lt;- w - (w %onto% v1) Take the mutually orthogonal vectors you already have and package them into a matrix M. Then construct a new random vector w and project it onto M. ```r M &lt;- cbind(v1, v2) w &lt;- cbind(rnorm(4)) v3 &lt;- w - (w %onto% M) ``` Continue the process of step (iii) until you have 4 mutually orthogonal vectors, and collect them into the matrix `A`. Use dot products to verify that v1, v2, v3, and v4 are mutually orthogonal. Exercise XX.XX: vUbXdq The previous exercise showed how to create mutually orthogonal columns. Generate four such mutually orthogonal vectors: v1 through v4. Create a target vector b with the same dimension as the v_ vectors. b can point in any direction whatsoever, your choice! Using %onto%, calculate the vector projection ofbonto the matrixA &lt;- cbind(v1, v2, v3, v4). The result will be a vector. Call this vectorb_model`. A previous exercise gave the formula for the scalar multiplier \\(\\alpha\\) of a vector \\(\\vec{v}\\) such that \\(\\alpha\\,\\vec{v} = \\modeledby{\\vec{b}}{\\vec{v}}\\): \\[\\alpha = \\frac{\\vec{b} \\bullet \\vec{v}}{\\vec{v} \\bullet \\vec{v}}\\ .\\] Use this formula to calculate the four scalars multipliers that result from projecting respectively b onto v1, b onto v2, and so on. Call these scalars alpha1, alpha2, and so on. Calculate the sum alpha1*v1 + alpha2*v2 + alpha3*v3 + alpha4*v4. Show that the vector found in step (d) matches the vector b_model found in step (b). This is a demonstration that, with mutually orthogonal vectors, finding scalar multipliers for a linear combination of those vectors can be done one vector at a time. Repeat the above, but this time using vectors v1, v2, v3, and v4 that are not mutually orthogonal. One way to generate such vectors is at random: v1 &lt;- cbind(rnorm(4)) and so on. For non-mutually orthogonal vectors, does the one-vector-at-a-time approach produce a match to b %onto% cbind(v1, v2, v3, v4)? Exercise XX.XX: p209w8 Here are twelve labeled vectors, A through M. There is a thirteenth vector, labeled “Null vector.” That’s a vector of length zero, so it can’t be drawn as an arrow. Note that the direction of the null vector doesn’t matter, since the vector length is zero. Each of the following statements is of the form, \"\\(\\vec{v}\\) projected onto \\(\\vec{u}\\) gives \\(\\vec{w}\\). Say whether the statement is true or false. Question A \\(\\vec{A}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{K}\\) TrueExcellent!  False︎✘ A projected onto D will be in the direction of D. K is in the direction of D, points in the right direction (downwards), and equals the vertical component of A. Question B \\(\\vec{D}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{L}\\) True︎✘ D projected onto E will be in the direction of E. L is in the direction of E but L does not have the right length. FalseCorrect.  Question C \\(\\vec{J}\\) projected onto \\(\\vec{E}\\) gives the null vector. True︎✘ J and E are not orthogonal. So the projection of one onto the other cannot be the null vector. FalseCorrect.  Question D \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question E \\(\\vec{J}\\) projected onto \\(\\vec{K}\\) gives \\(\\vec{D}\\) True︎✘ J and K are parallel, so projecting J onto K will produce the vector J. But J is much shorter than D. FalseExcellent!  Question F \\(\\vec{C}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{L}\\) True︎✘ C projected onto D will be in the direction of D. L is not in the direction of D. FalseExcellent!  Question G \\(\\vec{L}\\) projected onto \\(\\vec{B}\\) gives the null vector True︎✘ It’s only when vectors are orthogonal that the projection of one onto the other produces the null vector. L and B are not orthogonal. FalseRight!  Question H \\(\\vec{E}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{E}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question I \\(\\vec{G}\\) projected onto \\(\\vec{C}\\) gives the null vector.     True\\(\\heartsuit\\ \\)       False︎✘ Question J \\(\\vec{E}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{J}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question K \\(\\vec{A}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{K}\\)     True︎✘ A onto B will be in the direction of B. But K is orthogonal to B.       False\\(\\heartsuit\\ \\) Question L \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question M \\(\\vec{F}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{H}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question N \\(\\vec{C}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{E}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question O \\(\\vec{E}\\) projected onto \\(\\vec{G}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question P \\(\\vec{G}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{C}\\)     True︎✘        False\\(\\heartsuit\\ \\) "],["target-problem.html", "Chapter 44 The target problem 44.1 Linear equations 44.2 Visualization in a two-dimensional subspace 44.3 Properties of the solution 44.4 Application of the target problem 44.5 Exercises", " Chapter 44 The target problem “In theory there is no difference between theory and practice, while in practice there is.” In this chapter, we ask you to reconsider a mathematical theory that is universally taught in high-school and to consider augmenting it with newer computational ideas that address the same kind of problems, but which produce useful results even when the mathematical theory insists the “a solution does not exist.” The time-honored theory is that taught in high-school algebra. There’s nothing wrong with that theory except that it is incomplete. It doesn’t address the needs of present-day practice, particularly in data science, statistics, and machine learning. Algebra, in its basic sense, is about generalizing arithmetic to handle situations where some quantities are not yet known numerically and so are represented by symbols. The algebra student learns rules for symbolic expressions that allow the expressions to be re-arranged into other forms that would clearly be valid if replaced by numbers. Some examples of these rules: i. \\(ax = b\\) is equivalent to \\(x = a/b\\). ii. \\(a + x = b\\) is equivalent to \\(x=b-a\\). iii. \\(a x^2 + b x + c = 0\\) is equivalent to \\(x = \\frac{-b\\pm \\sqrt{\\strut b^2 - 4ac}}{2a}\\). iv. \\(\\ln(ax) = b\\) is equivalent to \\(x = \\frac{1}{a}\\ln(b)\\). A major challenge to the algebra student is to use such rules to re-arrange expressions into a form \\(x=\\) that enables \\(x\\) to be calculated from the numerical values of the other symbols. Unfortunately, students are given little or no insight to the historical origins of algebra techniques and why they are not necessarily appropriate for all tasks. In English, the word “algebra” is seen as early as 1551. It comes from a book written by the Persian Muhammad ibn Musa al-Khwarizmi (780-850), The Compendious Book on Calculation by Completion and Balancing. The book introduced the use of rules familiar to every algebra student. In the original Arabic, the title includes the word “al-jabr,” meaning “completion” or “rejoining.” According to some sources, the literal meaning of “al-jabr” was resetting and rejoining broken bones. That literal meaning correctly conveys the importance of the subject, but also the pain endured by many algebra students. (Incidentally, the “algorithm” comes from the name of the book’s author: al Khwarizmi. He is a major figure in the history of mathematics.) This history may not be of immediate interest to every reader, but there is a good point to it. The roots of algebra are ancient and developed in an era very different from our own. Today’s student learns algebra in order to facilitate the study and practice of physics, chemistry, statistics, engineering, and other fields. None of these fields existed when algebra was being conceived. That is, the theory was developed before the recognition of the problems and calculations that arise in modern practice. Thus, “in practice, theory and practice are different.” This chapter is about re-expressing some basic algebraic theory in order to align it better to today’s practice. 44.1 Linear equations The focus of interest will be the familiar task \\[\\ \\ \\ \\ \\ \\ \\text{given}\\ \\ a x = b\\,,\\ \\ \\text{find}\\ \\ x\\ .\\] All algebra students learn that \\(x = b/a\\), with the proviso that if \\(a = 0\\), “there is no solution.” A somewhat more advanced algebra task is to work with “simultaneous linear equations,” for example: \\[\\ \\ \\ \\text{given}\\ \\ \\ \\begin{array}{rrrcr} 3 x &amp; + &amp; 2 y &amp; = &amp; 7\\\\ -1&amp;+&amp;y&amp;=&amp;4\\end{array} \\ \\ \\ \\ \\text{find}\\ \\ x\\,\\&amp;\\,y\\ . \\] Solving simultaneous linear equations is hard. It involves more arithmetic than \\(ax = b\\) and requires the student to make good choices how to take linear combinations of the two equations to reduce the problem to two equations, one with \\(x\\) as the only unknown and one with \\(y\\). Also, the “there is no solution” proviso is not easy to state, so you can’t know at a glance whether there is indeed a solution. The simultaneous linear equation problem can be more compactly written using matrix and vector notation. \\[\\begin{array}{rrrcr}3x &amp; + &amp;2y &amp; = &amp; 7\\\\-1&amp;+&amp;y&amp;=&amp;4\\end{array} \\ \\ \\text{is the same as}\\ \\ \\left[\\begin{array}{r}3\\\\-1\\end{array}\\right] x + \\left[\\begin{array}{r}2\\\\1\\end{array}\\right] y = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right] \\] You can see the vector form as a linear combination of two vectors. Collecting these two vectors into a matrix \\(\\mathit{A}\\), and similarly writing \\(x\\, \\text{and}\\, y\\) as the scalar components of a vector \\(\\vec{x}\\) gives \\[\\left[\\begin{array}{rr}3&amp;2\\\\-1&amp;1\\end{array}\\right]\\ \\vec{x} = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right]\\] Which can be expressed as \\(\\mathit{A} \\vec{x} = \\vec{b}\\). A student, recognizing the similarity of \\(\\mathit{A}\\vec{x} = \\vec{b}\\) to \\(a x = b\\) would reasonably suggest the solution \\(\\vec{x} = \\vec{b}/ \\mathit{A}\\). Such a student might be instructed, “No, you can’t do this.” A better response would be, “Good. Now tell me what you mean by \\(\\vec{b}/\\mathit{A}\\)?” Modern practice often calls for solving \\(\\mathit{A}\\vec{x} = \\vec{b}\\) in settings where a traditional algebra teacher might say, as for \\(0 x = b\\) that “there is no solution.” To illustrate such a setting, recall the problems from Block 1 of finding the linear combination of the functions \\(f(\\mathtt{time})=1\\) and \\(g(\\mathtt{time}) = e^{-0.019 \\mathtt{time}}\\) that best matches the CoolingWater data: time temp 0 98.2 1 94.4 2 91.4 … and so on … 220 25.9 221 25.8 We seek scalars \\(C\\) and \\(D\\) such that the function \\(C f(\\mathtt{time}) + D g(\\mathtt{time})\\) gives the best possible match to temp. We can compactly write the problem of finding the best linear combination into matrix form by evaluating \\(f()\\) and \\(g()\\) at the values listed in the time column: \\[\\underbrace{\\left[\\begin{array}{rr}1&amp;1.0000\\\\1&amp;0.9812\\\\1&amp;0.9627\\\\\\vdots\\\\1&amp;0.0153\\\\1&amp;0.0150\\end{array}\\right]}_{\\!\\!\\!\\!\\!\\!\\!\\!{\\large\\mathit{A}} = \\left[\\strut f(\\mathtt{time})\\,,\\ \\ \\ g(\\mathtt{time})\\right]} \\underbrace{\\left[\\begin{array}{r}C\\\\D\\end{array}\\right]}_{\\large\\vec{x}} \\ \\text{is the best match to}\\ \\underbrace{\\left[\\begin{array}{r}\\mathtt{98.2}\\\\\\mathtt{94.4}\\\\\\mathtt{91.4}\\\\\\vdots\\\\\\mathtt{25.9}\\\\\\mathtt{25.8}\\end{array}\\right]}_{\\large\\vec{b}}\\] Regrettably, the classical algebraicists did not propose a rule for “is the best match to.” Replacing “is the best match to” with \\(=\\) is not literally correct since “there is no solution” that makes the equality literally true. We’ll use the term target problem to name the task of finding \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) is the best possible match to \\(\\vec{b}\\). This term is motivated by the idea that \\(\\vec{b}\\) is a target, and we seek to use the resources in \\(\\mathit{A}\\) to get as close as possible to the target: choose \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) falls as closely as possible to the target. To address the practical problem in the notation of algebra theory, people write \\[\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\] where \\(\\vec{r}\\) is a vector specially selected to path up \\(\\mathit{A} \\vec{x} = \\vec{b}\\) so that when the best-matching \\(C\\) and \\(D\\) are found, there will be a literal equality solution to \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\). At first glance, \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\) might seem intractable: How are we to find \\(\\vec{r}\\). The answer is that \\(\\vec{r}\\) will be the solution to the projection problem \\(\\vec{b}\\sim\\mathit{A}\\). When \\(\\vec{r}\\) is selected this way, \\(\\vec{r}\\) will be the shortest possible vector that can do the matching up. In other words, by choosing \\(\\vec{r} = \\vec{b} \\sim \\mathit{A}\\) we are implementing the following definition of “is the best match to”: “the best match is the one with the smallest length \\(\\vec{r}\\).” It’s remarkable that one can find \\(\\vec{r}\\) even without knowing \\(\\vec{x}\\). That’s why we introduced and solved the projection problem before taking on the target problem. The part of the target problem that we have still to figure out is how, given \\(\\vec{r}\\), to find \\(\\vec{x}\\). But even at this point you can see that \\(\\mathit{A}\\vec{x} = \\vec{b} - \\vec{r}\\) must have a solution, since \\(\\vec{b} - \\vec{r}\\) is exactly the model vector \\(\\hat{b}\\) which, as we saw in Chapter 43, must lie in \\(span{\\mathit{A}}\\). 44.2 Visualization in a two-dimensional subspace To help you create a mental model of the geometry of the target problem, we’ll solve it graphically for a two dimensional subspace. That is, we’ll solve \\(\\left[\\vec{u}, \\vec{v}\\right] \\vec{x} = \\vec{b}\\). For simplicity, the vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\) will have two components. This means that there is no need to project \\(\\vec{b}\\) onto the subspace; it’s already there (so long as \\(\\vec{u}\\) and \\(\\vec{v}\\) have different directions. ) You may already have encountered the step (ii) technique in your childhood reading. The problem appears in Robert Louis Stevenson’s famous novel, Treasure Island. The story is about the discovery of a treasure map indicating the location of buried treasure on the eponymous Island. There is a red X on the map labelled “bulk of treasure here,” but that is hardly sufficient to guide the dig for treasure. After all, every buried treasure needs some secret to protect it. On the back of the map is written a cryptic clue to the precise location: Tall tree, Spy-glass shoulder, bearing a point to the N. of N.N.E. Skeleton Island E.S.E. and by E. Ten feet. Skeleton Island is clearly marked on the map, as is Spy-glass Hill. The plateau marked by the red X “was dotted thickly with pine-trees of varying height. Every here and there, one of a different species rose forty or fifty feet clear above its neighbors.” But which of these was the “tall tree” mentioned in the clue? Figure 44.1: The map of Treasure Island. The heading ‘E.S.E. and by E.’ is marked with a solid black line starting at Skeleton Island. The heading ‘N. of N.N.E.’ is marked by dotted lines, one of which is positioned to point at the shoulder of Spy-glass Hill. Where the bearing from Skeleton Island meets the bearing to Spy-glass Hill will be the Tall tree. With your new-found background in vectors, you will no doubt recognize that “N. of N.N.E” is the direction of a vector as is “E.S.E. and by E.” Pirate novels seem always to use the length unit of “pace,” which we’ll use here as well. The target is the shoulder of Spy-glass Hill. Or, in vector terms, \\(\\vec{b}\\) is the vector with Skeleton Island as the tail and the should of Spy-glass Hill as the tip. The vectors are \\(\\vec{u} = \\text{N. of N.N.E.}\\) and \\(\\vec{v} = \\text{E.S.E. and by E.}\\) We need to \\[\\text{solve} \\ \\ \\underbrace{\\left[\\vec{u}, \\vec{v}\\right]}_{\\Large\\strut\\mathit{A}} \\underbrace{\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}_{\\Large\\vec{x}} = \\vec{b}\\ \\ \\text{for}\\ \\ {\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}\\ .\\] Long John Silver, obviously an accomplished mathematician, starts near Skeleton Island, moving on along the vector that keeps Skeleton Island to the compass bearing one point east of east-south-east. While on the march, he keeps a telescope trained on the shoulder of Spy-glass Hill. When that telescope points one point north of north-north-east, they are in the vicinity of a tall tree. That’s the tree matching the clue. The vectors in Treasure Island were perpendicular to one another, that is, mutually orthogonal. The more general situation is that the vectors in \\(\\mathit{A}\\) will be somewhat aligned with one another: not mutually orthogonal. 44.2 illustrates the situation: \\(\\vec{v}\\) is not perpendicular to \\(\\vec{u}\\). The task, still, is to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that will match \\(\\vec{b}\\). The diagram shows the \\(\\vec{u}\\) vector and the subspace aligned with \\(\\vec{u}\\), and similarly for \\(\\vec{v}\\) Figure 44.2: The telescope method of solving projection onto two vectors. The algorithm is based in Long John Silver’s technique. Pick either \\(\\vec{u}\\) or \\(\\vec{v}\\), it doesn’t matter which. In the diagram, we’ve picked \\(\\vec{v}\\). Align your telescope with that vector. Now march along the other vector, \\(\\vec{u}\\), carefully keeping the telescope on the bearing aligned with \\(\\vec{v}\\). From the diagram, you can see that when you’ve marched to \\(\\frac{1}{2} \\vec{u}\\), the telescope does not yet have \\(\\vec{b}\\) in view. Similarly, at \\(1 \\vec{u}\\), the target \\(\\vec{b}\\) isn’t yet visible. Marching a little further, to about \\(1.6 \\vec{u}\\) brings you to the point in the \\(\\vec{u}\\)-subspace where the target falls into view. This tells us that the coefficient on \\(\\vec{u}\\) will be 1.6. To find the coefficient on \\(\\vec{v}\\), you’ll need to march along the line of the telescope, taking steps of size \\(\\|\\vec{v}\\|\\). In the diagram, we’ve marked the march with copies of \\(\\vec{v}\\) to make the counting easier. We’ll need to march opposite the direction of \\(\\vec{v}\\), so the coefficient will be negative. Taking 2.8 steps of size \\(\\|\\vec{v}\\|\\) brings us to the target. Thus: \\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\] To handle vectors in spaces where telescopes are not available, we need an arithmetic algorithm. In R, that algorithm is packaged up as qr.solve(). We will pick this up again the next section. Example 44.1 In 3-dimensional space, visualization of the solution to the target problem is possible, at least for those who have the talent of rotating three-dimensional objects in their head. For the rest of us, a physical model can help; take three pencils labeled \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{b}\\) and bury their tails in a small ball of putty. (Chemistry molecular construction kits are a good alternative.) In case putty, pencils, or a molecular model kit are not available, use the interactive diagram in Figure @ref{fig:b-onto-u-v}. This diagram also includes \\(\\hat{b}\\) and \\(\\vec{r}\\) with the hope that this will guide you into orienting the diagram appropriately to see where the solution comes from. Figure 44.3: Showing the relative orientation of the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\). Drag the image to rotate it. \\(\\vec{u}\\) and \\(\\vec{v}\\) are fixed in length. However, their lengths will appear to change as you rotate the space. This might be called the “gun-barrel” effect; a tube looks very short when you look down it’s longitudinal axis, but looks longer when you look at it from the side. Rotate the space until both \\(\\vec{u}\\) and \\(\\vec{v}\\) reach their maximum apparent length. The viewpoint that accomplishes this is looking downward perpendicularly onto the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. In this orientation, you will be looking down the barrel of the \\(\\vec{r}\\) gun. Vector \\(\\vec{b}\\) is not in that plane, as you can confirm by rotating the plot a bit out of the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Returning to the perspective looking down perpendicularly on the place, you can see how \\(\\vec{b}\\) corresponds to \\(\\hat{b}\\), the point in the plane where the projection of \\(\\vec{b}\\) will fall. To find the scalar multiplier on \\(\\vec{v}\\), rotate the space until the vector \\(\\vec{u}\\) is pointing straight toward you. You’ll see only the arrowhead of \\(\\vec{u}\\). Vectors \\(\\vec{v}\\) and \\(\\hat{b}\\) will appear parallel to each other, but that’s because you are looking at the plane edge on. In this orientation, \\(\\hat{b}\\) will appear just a little longer than \\(\\vec{v}\\), perhaps 1.2 times longer. So 1.2 is the scalar multiplier on \\(\\vec{v}\\). To figure out the scalar multiplier on \\(\\vec{u}\\), follow the same procedure as in the previous paragraph, but looking down the barrel of \\(\\vec{v}\\). From this perspective, \\(\\vec{u}\\) appears longer than \\(\\hat{b}\\); the scalar multiplier on \\(\\vec{u}\\) will be about 0.9. In terms of \\(\\mathit{A} \\vec{x} = \\hat{b}\\), the solution is \\[\\vec{x} = \\left[\\begin{array}{r}0.9\\\\1.2\\end{array}\\right]\\ .\\] 44.3 Properties of the solution As you might expect, there is a known solution to the target problem. We’ll start by using a computer implementation of this solution to demonstrate some simple properties of the solution. As an example, we’ll use three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) in a 5-dimensional space as the “screen” to be projected onto, and another vector \\(\\vec{b}\\) as the object being projected. The matrix \\(\\mathit{A}\\) is: \\[{\\mathbf A} \\equiv \\left[\\strut \\begin{array}{ccc}|&amp;|&amp;|\\\\\\vec{u} &amp; \\vec{v} &amp; \\vec{w}\\\\|&amp;|&amp;|\\end{array}\\right]\\] For the sake of example, we’ll make up some vectors. In your own explorations, you can change them to anything you like. # the three vectors u &lt;- rbind(6, 4, 9, 3, 1) v &lt;- rbind(1, 5,-2, 0, 7) w &lt;- rbind(3,-5, 2, 8, 4) A &lt;- cbind(u, v, w) # the target b &lt;- rbind(8, 2,-5, 7, 0) The operator %onto% model vector and from that we can calculate the residual vector. s &lt;- b %onto% A r &lt;- b - s Those two simple commands constitute a complete solution to the projection problem, where see seek to model vector and the residual vector. In the target problem we want more: How to express \\(\\hat{b}\\) as a linear combination of the columns in \\(\\mathit{A}\\). At the risk of being repetitive, this means finding \\(\\color{magenta}{\\vec{x}}\\) in \\[\\mathit{A}\\ \\color{magenta}{\\large\\vec{x}} = \\vec{b}\\] where \\(\\mathit{A}\\) and \\(\\vec{b}\\) are given. The function qr.solve() finds \\(\\vec{x}\\). x &lt;- qr.solve(A, b) ## [,1] ## [1,] 0.03835171 ## [2,] 0.33478133 ## [3,] 0.48849968 How can we confirm that this really is the solution to the target problem for this set of vectors? Easy! Just multiply \\(\\mathit{A}\\) by the \\(\\vec{x}\\) that we found. The result should be the target vector \\(\\hat{b}\\): A %*% x ## [,1] ## [1,] 2.0303906 ## [2,] -0.6151849 ## [3,] 0.6526021 ## [4,] 4.0230526 ## [5,] 4.3358197 s ## [,1] ## [1,] 2.0303906 ## [2,] -0.6151849 ## [3,] 0.6526021 ## [4,] 4.0230526 ## [5,] 4.3358197 You should add qr.solve() to your computational toolbox of R functions. 44.4 Application of the target problem In Section @ref{linear-equations} we translated into vector/matrix form the problem, originally stated in Block 1, of finding the best linear combination of \\(f(\\mathtt{time}) \\equiv 1\\) and \\(g(\\mathtt{time}) \\equiv e^{-0.019 \\mathtt{time}}\\). Let’s solve that problem now. Earlier we introduced rbind() for the purpose of making column vectors, as in rbind(3,7,-1) ## [,1] ## [1,] 3 ## [2,] 7 ## [3,] -1 Now we are going to work with columns of data stored in the CoolingWater data frame. A good way to extract a column from a data frame is using the with() function. For instance, b &lt;- with(CoolingWater, temp) time &lt;- with(CoolingWater, time) A &lt;- cbind(1, exp(-0.019 * time)) head(A) ## [,1] [,2] ## [1,] 1 1.0000000 ## [2,] 1 0.9811794 ## [3,] 1 0.9627129 ## [4,] 1 0.9445941 ## [5,] 1 0.9268162 ## [6,] 1 0.9093729 Notice that cbind() automatically translated 1 into the vector of all ones. We’re all set up to solve the target problem: x &lt;- qr.solve(A, b) ## [1] 25.92024 61.26398 How good an answer is the x calculated by qr.solve()? Judge for yourself! gf_point(temp ~ time, data = CoolingWater, size=0) %&gt;% slice_plot(25.92 + 61.26*exp(-0.019*time) ~ time, color=&quot;blue&quot;) You may recall from Block 1 the explanation for the poor match between the model and the data for early times: that the water cooled quickly when poured into the cool mug, but the mug-with-water cooled much slower into the room air. Let’s augment the model by adding another vector with a much faster exponential cooling, say, \\(e^{-0.06 \\mathtt{time}}\\). newA &lt;- cbind(A, exp(-0.06*time)) qr.solve(newA, b) ## [1] 26.82297 53.27832 12.67486 gf_point(temp ~ time, data = CoolingWater, size=0) %&gt;% slice_plot(26.82 + 53.28*exp(-0.019*time) + 12.67*exp(-0.06*time) ~ time, color=&quot;green&quot;) 44.5 Exercises Exercise XX.XX: 1Jxboc In this exercise, you are going to check proposed solutions to the target problem. Each question poses one target problem. One of the answers is correct. Use R in a SANDBOX to select the correct answer. The vectors you will be working with are: \\[\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\] Question A i. What linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) will reach target \\(T\\)? \\(3\\vec{a} - 7\\vec{b}\\)︎✘ \\(\\vec{a} + 2\\vec{b}\\)︎✘ \\(-4 \\vec{a} + 7 \\vec{b}\\)Correct.  \\(- \\vec{a} + 4\\vec{b}\\)︎✘ No combination will reach \\(\\vec{T}\\) exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question B ii. What linear combination of \\(\\vec{b}\\) and \\(\\vec{c}\\) will reach target \\(T\\)? \\(\\frac{4}{3}\\vec{b} - \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{7}{3}\\vec{c}\\)︎✘ \\(-\\frac{7}{3}\\vec{b} + \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{5}{3} \\vec{b} + \\frac{4}{3} \\vec{c}\\)Nice!  No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question C iii. What linear combination of \\(\\vec{b}\\) and \\(\\vec{d}\\) will reach target \\(T\\)? \\(0\\vec{b} - \\frac{3}{2}\\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ \\(0 \\vec{b} - \\frac{1}{2} \\vec{d}\\)Nice!  \\(\\frac{3}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question D iv. Which of these linear combinations of $ ec{a}$, \\(\\vec{b}\\) and \\(\\vec{c}\\) will reach target \\(T\\)? \\(\\vec{a} -\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{a} +\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{a} -\\vec{b} + 2\\vec{c}\\)Nice!  \\(-2\\vec{a} +\\vec{b} + 2\\vec{c}\\)︎✘ No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. Adding a third vector just increases the number of possibilities. None of the above︎✘ Exercise XX.XX: DPY4Ue In this exercise, you are going to use R to find solutions to the target problem. Each question poses one target problem. You could solve these problems by eye, but we want to get you started on the computer solution so that you’ll be ready to solve harder problems that must be worked on the computer. The vectors you will be working with are: \\[\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\] To make your life easier, here are the commands for defining these vectors. One of the commands is wrong. You’ll have to correct it before moving on the the rest of the problem. NEED TO BREAK ONE OF THESE a &lt;- rbind(1, 2) b &lt;- rbind(1, 1) c &lt;- rbind(1, -1) d &lt;- rbind(-6, 2) T &lt;- rbind(3, -1) Question A What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) to reach the target $ ec{T}? \\(-4 \\vec{a} + 7 \\vec{b}\\)Correct.  \\(-2 \\vec{a} + 5 \\vec{b}\\)︎✘ \\(2 \\vec{a} -7 \\vec{b}\\)︎✘ \\(4 \\vec{a} - 5 \\vec{b}\\)︎✘ Question B What what is the correct linear combination of \\(\\vec{b}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{3}\\vec{b} + \\frac{4}{3} \\vec{c}\\)Correct.  \\(\\frac{4}{3}\\vec{b} + \\frac{7}{2} \\vec{c}\\)︎✘ \\(\\frac{5}{2}\\vec{b} + \\frac{4}{5} \\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{9}{4} \\vec{c}\\)︎✘ Question C What what is the correct linear combination of \\(\\vec{c}\\) and \\(\\vec{d}\\) to reach the target $ ec{T}? \\(\\frac{0}{3}\\vec{c} + \\frac{4}{5} \\vec{d}\\)Nice!  \\(\\frac{1}{3}\\vec{c} + \\frac{1}{3} \\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{c} + \\frac{5}{7} \\vec{d}\\)︎✘ \\(\\frac{0}{3}\\vec{c} + \\frac{1}{2} \\vec{d}\\)︎✘ Question D What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)Excellent!  \\(\\frac{5}{4}\\vec{a} + \\frac{5}{4} \\vec{c}\\)︎✘ \\(\\frac{7}{4}\\vec{a} + \\frac{9}{4} \\vec{c}\\)︎✘ \\(\\frac{3}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)︎✘ Exercise XX.XX: dIobrt Each of the diagrams below consists of two vectors and a target point (denoted with a bull’s eye). Being a point, the target has a definite location relative to the coordinate axes. “Solving” this sort of system amounts to finding a linear combination of the two vectors whose result is a vector that can connect the origin to the target. (We call this a target problem.) Copy over each diagram to your paper. (Any reasonable approximation will do.) Then, for each diagram, find the linear combination of the two vectors that will reach from the origin to the target. Show your work, meaning that you should draw the scaled vectors in a proper position that demonstrates that they do indeed connect the origin to the target. Underneath the diagram, write down the numerical value of the scalars in the linear combination. (Again, any reasonable approximation will do.) "],["stat-modeling.html", "Chapter 45 Statistical modeling and R2 45.1 R-squared 45.2 Interpreting the model 45.3 Improving the model 45.4 Machine learning 45.5 Exercises", " Chapter 45 Statistical modeling and R2 So far, we’ve been looking at linear combinations from a distinctively mathematical point of view: vectors, collections of vectors (matrices), projection, angles and orthogonality. We’ve show a few applications of the techniques for working with linear combinations, but have always expressed those techniques using mathematical terminology. In this Chapter, we will take a detour to get a sense of the perspective and terminology of another field: statistics. In the quantitative world, including fields such as biology and genetics, the social sciences, business decision making, etc. there are far more people working with linear combinations with a statistical eye than there are people working with the mathematical form of notation. Statistics is a far wider field than linear combination, so this chapter is not an attempt to replace the need to study statistics and data science. The purpose is merely to show you how a mathematical process can be used as part of a broader framework to provide useful information to decision-makers. Since a statistics and data-science courses are part of a complete quantitative education, we want to point out from the beginning what you are likely to experience a traditional introductory statistics course and why that will seem largely disconnected from what you see here. Statistics did not start out as a branch of mathematics, although people trained as mathematicians played a central role in it’s development. The story is complicated, but a simplification faithful to that history is to see statistics as an extension of biology. Statistics emerged in the last couple of decades of the 1800s. Possibly the key motivation was to understand genetics and Darwinian evolution. Today we know much about DNA sequences, RNA, amino acids, proteins, and so on. But quantitative genetics started in complete ignorance of any physical mechanism for genetics. Instead, mathematical models were the basis for the theory of genetics, beginning with with Gregor Mendel’s work on heritable traits published in 1865.1 Perhaps coincidentally—or perhaps not—Charles Darwin (1809-1882) was a half-cousin of Francis Galton (1822-1911). Traditional introductory statistics is based, more or less, on the work Galton and his contemporaries, for instance Karl Pearson (1857-1936) and William Sealy Gosset (1876-1937). The terms that you encounter in introductory statistics—correlation coefficient, regression, chi-squared test, t-test—where in place by 1910. Ronald Fisher (1890-1962), also a geneticist, extended this work based in part on the mathematical ideas of projection. Fisher invented analysis of variance (ANOVA) and maximum likelihood estimation and is perhaps the “great man” of statistics. (See Section 35.4 for an introduction to likelihood. We’ll briefly touch on ANOVA in this chapter.) To his historical discredit, Fisher was a leader in eugenics. He was also a major obstacle, to the statistical acceptance of the dangers of smoking. Most of the many techniques covered in traditional introductory statistics are, in fact, manifestations of the application of a general technique: the target problem. They are not taught this way, partly out of hide-bound tradition and partly because the target problem has been covered, if at all, in the fourth or fifth semester of a traditional calculus sequence. It often happens that a model is needed to help organize complex, multivariate data for purposes such as prediction. As a case in point, consider the data available in the Body_fat data frame, which consists of measurements of characteristics such as height, weight, chest circumference, and body fat on 252 men. Znotes::and_so_on(Body_fat) %&gt;% kableExtra::kable_minimal() bodyfat age weight height neck chest abdomen hip thigh knee ankle biceps forearm wrist 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 37.3 21.9 32.0 27.4 17.1 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 37.3 23.4 30.5 28.9 18.2 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 38.9 24.0 28.8 25.2 16.6 … and so on … 26.0 72 190.75 70.5 38.9 108.3 101.3 97.8 56.0 41.6 22.7 30.5 29.4 19.8 31.9 74 207.50 70.0 40.8 112.4 108.5 107.1 59.3 42.2 24.6 33.7 30.0 20.9 Body fat, the percentage of total body mass consisting of fat, is thought by some to be a good measure of general fitness. To what extent this theory is merely a reflection of general societal attitudes toward body shape is unknown. Whatever its actual utility, body fat is hard to measure directly; it involves submerging a person in water to measure total body volume, then calculating the persons mass density and converting this to a reading of body-mass percentage. For those who would like to see body fat used more broadly as a measure of health and fitness, this elaborate procedure stands in the way. And so they seek easier ways to estimate body fat along the lines of the Body Mass Index (BMI), which is a simple arithmetic combination of easily measured height and weight. (Note that BMI is also controversial as anything other than a rough description of body shape. In particular, the label “overweight,” officially \\(25 \\leq \\text{BMI}\\leq 30\\) has at best little connection to actual health.) How can we construct a model, based on the available data, of body fat as a function of the easy-to-measure characteristics such as height and weight? You can anticipate that this will be a matter of applying what we know about the target problem \\[\\text{Given}\\ \\mathit{A}\\ \\text{and}\\ \\vec{b}\\text{, solve } \\mathit{A} \\vec{x} = \\vec{b}\\ \\text{for}\\ \\vec{x}\\]where \\(\\vec{b}\\) is the column of body-mass measurements and \\(\\mathit{A}\\) is the matrix of all the other columns in the data frame. In statistics, the target \\(\\vec{b}\\) is called the response variable and \\(\\mathit{A}\\) is the set of explanatory variables. You can also think of the response variable as the output of the model we will build and the explanatory variables as the inputs to that model. Although application of the target problem is an essential part of constructing a statistical model, it is far from the only part. For instance, statisticians find is useful to think about “how much” of the response variable is explained by the explanatory variables. Measuring this requires a definition for “how much.” In defining “how much,” statisticians focus not on how much variation there is among the values in the response variable. The standard way to measure this is with the variance, which was introduce in Section 41.4 and can be thought of as the average of pair-wise differences among the elements in \\(\\vec{b}\\). In order to support this focus on the variance of \\(\\vec{b}\\), statisticians typically augment \\(\\vec{A}\\) with a column of ones, which they call the intercept. To move forward, we’re going to extract the response variable from the data and construct \\(\\vec{A}\\), adding in the vector of ones. We’ll show the vector/matrix commands for doing this, but you don’t have to remember them because statisticians have a more user-friendly interface to the calculations. b &lt;- with(Body_fat, cbind(bodyfat)) A &lt;- Body_fat[-1] %&gt;% as.matrix A &lt;- cbind(1, A) x &lt;- qr.solve(A, b) The -1 in the second command is a directive to get rid of column 1, which happens to the the response variable, in constructing \\(\\vec{A}\\). Having applied qr.solve(), \\(\\vec{x}\\) now contains the coefficients on the “best” linear combination of the columns in \\(\\vec{A}\\). One of the ways in which the R language is designed to support statistics, is that it keeps track of names of columns, so the elements of \\(\\vec{x}\\) are labelled with the name of the column the element applies to. x ## bodyfat ## -18.18848508 ## age 0.06207865 ## weight -0.08844468 ## height -0.06959043 ## neck -0.47060001 ## chest -0.02386415 ## abdomen 0.95477346 ## hip -0.20754112 ## thigh 0.23609984 ## knee 0.01528121 ## ankle 0.17399537 ## biceps 0.18160242 ## forearm 0.45202491 ## wrist -1.62063910 Based on the above, the model for body fat as a function of the explanatory variables is: \\[\\text{body fat} = -18.2 + 0.062\\, \\mathtt{age} - 0.0884\\, \\mathtt{weight} - 0.696\\, \\mathtt{height} + \\text{an so on}\\ .\\] Once we have the means to solve the target problem, as we do with qr.solve(), constructing such a model is straightforward. But some questions remain: - How good is the model? - Could we make a better model? To answer these questions, we’ll have to develop a measure of how good the model is. And this depends on the purpose for which we’ll be using the model. 45.1 R-squared One of the most basic indices of the quality of a model—but far from the only one!—is a statistic written R2 and pronounced “R-squared.” R2 is quite simple. Remember that a basic statistical question is how much variation there is in the response variable. That’s measured with the variance: var(b) ## bodyfat ## bodyfat 70.03582 We can also look at the variation in the model vector, \\(\\hat{b}\\). bhat &lt;- A %*% x var(bhat) ## bodyfat ## bodyfat 52.46033 R2 is simply the ratio of these two variances: var(bhat) / var(b) ## bodyfat ## bodyfat 0.74905 This result, 74.9%, is interpreted as the fraction of the variance in the response variable that is accounted for by the model. Near synonyms for “accounted for” is explained by or can be attributed to. In the same spirit, we can ask how much of the variance in the response variable is unexplained by, or unaccounted by the explanatory variables. To answer this, look at the size of the residual: var(b - bhat) / var(b) ## bodyfat ## bodyfat 0.25095 Notice that the amount of variance explained, 74.905%, plus the amount remaining unexplained, 25.095%, add up to 100%. This is no accident. It is the reason why statisticians use the variance as a measure of variability. Let’s re-do the calculations, but using the software interface designed for statistical modeling rather than the interface used for constructing matrices. In R, the core program is called lm(), which stands for “linear model.” The lm() interface combines the construction phase and the model fitting phase and is connected with other software for doing an analysis of the model. mod_big &lt;- lm(bodyfat ~ ., data = Body_fat) coef(mod_big) ## (Intercept) age weight height neck chest ## -18.18848508 0.06207865 -0.08844468 -0.06959043 -0.47060001 -0.02386415 ## abdomen hip thigh knee ankle biceps ## 0.95477346 -0.20754112 0.23609984 0.01528121 0.17399537 0.18160242 ## forearm wrist ## 0.45202491 -1.62063910 rsquared(mod_big) ## [1] 0.74905 The first argument to lm() is a tilde expression specifying the response variable (on the left of ~) and the explanatory variables. The vector of ones is included by default without needing to specify it. And there are some other shortcuts. For instance, in the tilde expression bodyfat ~ . the dot on the right-hand side of the tilde means “all the columns except the response variable.” The results from lm() are, as you can see, identical to those produced by the matrix manipulations in the previous section. 45.2 Interpreting the model One of the ways statistics differs from mathematics is that statistics is concerned with the interpretation of the model, the ways the model is or is not fit for purpose, and the ways the model can be improved. Since the purpose of the body-fat model is to estimate the actual body fat percentage from easy-to-measure variables, let’s examine how much knowing the output of the model tells us about the actual body fat. We’ve already seen that R2 is 75%, but there are other ways to look at things. Figure 45.1 shows the actual body fat in each of the 252 men whose data are in Body_fat as a function of the model output. Figure 45.1: Comparing the output of the model to the actual body-fat measurements made in the 252 men represented in the Body_fat data frame. Using the graph, consider what to make of a model output of 20. Looking at the dozen or so men whose measurements led to an output near 20, their actual body fat spans a range of values, from about 10% to 30%. The prediction error for any given man is the difference between the actual value of body fat and the model output. The gray band on the graph contains about 95% of all the men. The vertical width of the band—about -8% to +8%—gives a prediction error that encompasses 95% of the men. This is interpreted to mean that the model output reflects the actual body mass, plus-or-minus 8%. (There’s no guarantee that the prediction error will be \\(\\leq 8\\%\\), but in the large majority of cases (roughly 95%) the model output will have a prediction error that’s no larger than \\(\\pm 8\\%\\).) Another difference between statistics and mathematics is that good statistical work always requires some understanding of the system being studied, not just the manipulation of columns of numbers. Look back at the coefficients. Some make sense and some are questionable. For instance, the positive coefficient on abdomen makes intuitive sense since everyday experience is that body fat often appears there. But the negative coefficient on neck, what’s that about? 45.3 Improving the model The model that we built of body fat, which we called big_mod, might just as well have been called “kitchen sink.” It includes all possible explanatory variables. But it doesn’t have terms higher than first order. That is, neck enters into the model just as a linear function. But in Chapter 25 we introduced low-order polynomial terms, such as the quadratic neck^2 or the interaction neck*abdomen. Maybe we would do better by introducing such terms. Since there are 13 explanatory variables, this would add 13 quadratic terms and many interaction terms, one for each pair of variables. That’s 78 vectors for the interaction plus 13 linear terms plus another 13 quadratic terms. Let’s try adding these interaction terms a few at a time to see what happens. The modeling syntax for interaction terms uses the multiplication symbol *. We’ll compare the new models to big_mod which had an R2 of 74.9%. mod2 &lt;- lm(bodyfat ~ . * neck, data = Body_fat) rsquared(mod2) ## [1] 0.7730899 Including the interactions with neck has increased R2 to 77.3%. Let’s keep going: mod3 &lt;- lm(bodyfat ~ . * neck + . * weight + . * abdomen, data=Body_fat) rsquared(mod3) ## [1] 0.7932603 Again, R2 has gone up! Unfortunately, adding more terms is not a sure-fire way to improve a model. In model 3, we added 12 vectors from *.neck, 11 from .*weight, 10 from .*abdomen. (The reason for the decreasing counts is that .*neck already includes weight*neck, so the neck*weight that’s included in . * weight is redundant.) Adding altogether 33 new vectors on top of the 13 in the original model, has increased the R2 from 74.9% to 79.3%. That’s a gain of 4.4 percentage points from 33 new vectors. Is that gain worth it? To answer that question, we need to consider what is the cost induced by adding the 33 new vectors. The computational cost is not the issue, since even much bigger models are easily constructed on ordinary laptops. The issue has to do with the goal of predicting bodyfat. We don’t need to predict body fat for the men in the data; we already know their body fat. Instead, the point is to make predictions for men not in the data. These two kinds of prediction are called in-sample prediction (men in the data set) and out-of-sample prediction (men not in the data set). The problem is that in-sample predictions tend to have a larger R2 than out-of-sample predictions. And it’s the out-of-sample prediction that will matter in applications: predictions for people whose body fat is unknown and therefore will be estimated from the model. A major goal in statistics and machine learning is to estimate the R2 for out-of-sample prediction from just the data in the sample itself. Here’s one way to get at the out-of-sample prediction error: divide the data at random into two halves, fitting the model on the first half and estimating the out-of-sample prediction error using the other half, the half that is out-of-sample for fitting the model. This strategy, and many refinements to it, are called cross-validation and is a major technique in machine learning. An older statistical technique gets at the problem of in- and out-of-sample by using models constructed from random stand-ins for explanatory vectors. For instance, we can examine whether the 33 interaction terms we added to the model are contributing to prediction by replacing them with 33 random vectors and examining the gain in R2 from that. Here’s one way to do this, using rand(33) to generate the random variables: mod3R &lt;- lm(bodyfat ~ . + rand(33), data=Body_fat) rsquared(mod3) ## [1] 0.7932603 The random variables did every bit as well as the interaction terms! So there’s no particular reason to think that the interaction terms are contributing to the prediction. The example we just gave with . + rand(33) isn’t completely adequate to the statistician’s needs. Those 33 random vectors were just a particular random choice, a role of the vector dice. We need to establish whether the result we got, R2=79.3% was just good luck. The next code block shows one way to do this: we generate many trials of rand(33), calculating R2 for each trial. Then compare the “genuine vectors” result to the distribution of results from the random trials. You do not need to learn how to construct such code, but perhaps you will be able to gain some insight from it. We’re doing 100 random trials. trials &lt;- do(100) * {lm(bodyfat ~ . + rand(33), data=Body_fat) %&gt;% rsquared()} table(rsquared(mod3) &gt; trials) ## ## FALSE TRUE ## 14 86 The “genuine vectors” gave a result bigger than 90 out of 100 trials. This sort of “most of the time” doesn’t cut muster. A convention is to insist that the genuine vectors win at least 95% of the time. The 90-out-of-100 result would typically be reported as a p-value less than 10%. Another sort of standard statistical report carries out this same sort of comparison to random vectors, but building up the model one term at a time. This is called analysis of variance. A standard report looks like this: anova(mod_big) ## Analysis of Variance Table ## ## Response: bodyfat ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 1493.3 1493.3 80.5644 &lt; 2.2e-16 *** ## weight 1 6674.3 6674.3 360.0837 &lt; 2.2e-16 *** ## height 1 1043.0 1043.0 56.2712 1.250e-12 *** ## neck 1 152.4 152.4 8.2227 0.004508 ** ## chest 1 641.1 641.1 34.5856 1.373e-08 *** ## abdomen 1 2757.4 2757.4 148.7645 &lt; 2.2e-16 *** ## hip 1 22.5 22.5 1.2157 0.271327 ## thigh 1 110.6 110.6 5.9665 0.015309 * ## knee 1 0.0 0.0 0.0013 0.971409 ## ankle 1 0.1 0.1 0.0043 0.948064 ## biceps 1 45.2 45.2 2.4369 0.119842 ## forearm 1 57.5 57.5 3.1013 0.079514 . ## wrist 1 170.1 170.1 9.1781 0.002720 ** ## Residuals 238 4411.4 18.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first row of the report examines how well the model with just age does compared to random vectors. As you can see, the p-value (column Pr(&gt;F)) is much smaller than the standard cut-off of 0.05. The next row in the report gives the improvement in the model when weight is included along with age in the model. And so on down the line. The report highlights some stinkers among the explanatory variables: hip, knee, ankle, and biceps. Maybe this matches your intuition, for instance that ankle circumference is not the best way to look at body fat. This sort of analysis, which has many nuances not covered here, falls under the name statistical inference. Another, more recent term is statistical learning, often called machine learning. The general idea of statistical or machine learning is to search through many combinations of possible explanatory variables to construct a “best” model. This search often involves models that are not simply linear combinations. Some of these model architectures have catchy names: “deep learning,” “neural nets,” “support vector machines,” “lasso,” and so on. Very often, these machine learning models are built by gradient descent, and are elaborations of the basic \\(\\mathit{A}\\vec{x} = \\vec{b}\\) model architecture. But they often have capabilities beyond \\(\\mathit{A}\\vec{x} = \\vec{b}\\). For instance, deep learning and lasso are designed to handle situations where the number of explanatory variables—the number of columns in the data frame—is far larger than the number of rows. An example: learning to identify whether there is an animal in a photograph with 20,000 pixels. And returning to genetics: measuring 100,000 genetic expression products on a sample of 10 people with a disease and 10 healthy controls to determine which, if any, genes are related to the disease. 45.4 Machine learning If you pay attention to trends, you will know about advances in artificial intelligence and the many claims—some hype, some not—about how it will change everything from animal husbandry to warfare. Services such as Google Translate are based on artificial intelligence, as are many surveillance technologies. (Whether the surveillance is for good or ill is a serious matter.) Skills in artificial intelligence are currently a ticket to lucrative employment. Like so many things, “artificial intelligence” is a branding term. In fact, what all the excitement is about is not mostly artificial intelligence at all. The advances, by and large, have come over 50 years of development in a field called “statistical learning” or “machine learning,” depending on whether the perspective is from statistics or computer science. A major part of the mathematical foundation of statistical (or “machine”) learning is linear algebra. Many workers in “artificial intelligence” are struggling to catch up because they never took linear algebra in college or, if they did, they took a proof-oriented course that didn’t cover the elements of linear algebra that are directly applicable. We’re trying to do better in this course. So if you’re diligent, and continue your studies to take actual statistical/machine learning courses, you’ll find yourself at the top of the heap. Even xkcd, the beloved techno-comic, gets in on the act, as this cartoon reveals: Look carefully below the paddle and you’ll see the Greek letter “lambda”, \\(\\lambda\\). You’ll meet the linear algebra concept signified by \\(\\lambda\\)—eigenvalues and eigenvectors—in Block 6. 45.5 Exercises Exercise XX.XX: 97zwR6 A. What’s the largest possible value for R2? As you know, R2 is a ratio of variances. A good way to think of variance is as the square length of a vector. So think about R2 as if it were calculated from the square length of \\(\\vec{b}\\) and \\(\\hat{b}\\). (Hint: What does the projection problem tell you about the lengths of \\(\\vec{b}\\) and \\(\\hat{b}\\).) B. What’s the smallest possible value for R2? (Hint: What’s the smallest possible length for \\(\\hat{b}\\)?) Exercises confirming that qr.solve() produces results that are as they should be: residual orthogonal to every vector in A, projected + residual = \\(\\vec{b}\\). Exercises confirming that adding more columns to A produces a smaller residual. Demonstration that even random vectors can be combined to exactly equal \\(\\vec{b}\\), so long as we have enough of them. Amazingly, this work attracted little attention until after 1900, when Mendel’s laws were rediscovered by the botanists de Vries, Correns, and von Tschermak.↩︎ "],["functions-as-vectors-1.html", "Chapter 46 Functions as vectors 46.1 Dot product for functions 46.2 Sinusoids as vectors 46.3 Exercises", " Chapter 46 Functions as vectors Starting with Chapter 41, we have been working with the dot product, an operation that combines two vectors to produce a scalar. \\[\\vec{b}\\bullet\\vec{a} \\equiv \\left[\\begin{array}{c}b_1\\\\b_2\\\\\\vdots\\\\b_n\\end{array}\\right] \\bullet \\left[\\begin{array}{c}a_1\\\\a_2\\\\\\vdots\\\\a_n\\end{array}\\right] \\equiv b_1 a_1 + b_2 a_2 + \\cdots b_n a_n\\] The dot product enables us to use arithmetic to calculate geometric properties of vectors, even in high dimensional spaces that are out of reach of a ruler or protractor. For instance length: \\(\\|\\vec{a}\\| = \\sqrt{\\strut\\vec{a}\\bullet\\vec{a}}\\) included angle: \\[\\cos(\\theta_{ab}) = \\left[\\vec{a}\\bullet\\vec{b}\\right] / \\left[\\sqrt{\\strut \\strut\\vec{a}\\bullet\\vec{a}}\\sqrt{\\strut \\strut\\vec{b}\\bullet\\vec{b}}\\right]\\] projection onto \\(\\vec{a}\\): \\[\\text{model vector:}\\ \\ \\hat{b} = \\left[\\vec{b} \\bullet \\vec{a}\\right] \\,\\vec{a} / {\\len{a}^2} = \\left[\\vec{b} \\bullet \\vec{a} {\\LARGE/} \\vec{a} \\bullet \\vec{a}\\right]\\ \\vec{a}.\\] We used such operations to solve the target problem: finding the best approximation of a vector \\(\\vec{b}\\) as a linear combination of a set of vectors in a matrix \\(\\mathit{A}\\). As early as Block 1, we constructed functions as a linear combination of other functions, for example: \\[g(t) \\equiv A + B \\sin\\left(\\frac{2 \\pi}{P} t\\right)\\] where \\(A\\) is the scalar multiplier for the function \\(\\text{one}(t) \\equiv 1\\) and \\(B\\) the scalar multiplier for the sinusoid of period \\(P\\). We’re going to revisit the idea of linear combinations of functions using our new tools of length, included angle, and projection. To do this, we need to have a definition of the dot product suitable for application to functions. 46.1 Dot product for functions Given two functions, \\(f(t)\\) and \\(g(t)\\) defined over some domain \\(D\\), we’ll compute the dot product of the functions as a sum of the product of the two functions, that is: \\[f(t) \\bullet g(t) \\equiv \\int_{D} f(t)\\,g(t)\\,dt\\ .\\] ::: {.example data-latex=\"\"} Suppose that our two functions are \\(\\text{one}(t) \\equiv 1\\) and \\(\\text{identity}(t) \\equiv t\\) on the domain \\(0 \\leq t \\leq 1\\). Find the length of each function and the included angle between them. Length: \\(\\|\\text{one}(t)\\| = \\left[\\int_0^1 1 \\cdot 1\\,dt\\right]^{1/2} = \\left[\\ \\strut t\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = 1\\) Length: \\(\\|\\text{identity}(t)\\| = \\left[\\int_0^1 t \\cdot t\\,dt\\right]^{1/2} = \\left[\\ \\strut \\frac{1}{2}t^2\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = \\frac{1}{\\sqrt{2}}\\) Included angle: \\[\\cos(\\theta) = \\frac{\\text{one}(t) \\bullet \\text{identity}(t)}{\\|\\strut\\text{one}(t)\\| \\, \\|\\text{identity}(t)\\|} = \\sqrt{2}\\ \\int_0^1 t\\, dt = \\sqrt{\\strut 2} \\left.{\\Large\\strut}\\frac{1}{2} t^2\\right|_0^1 = \\sqrt{\\frac{1}{2}}\\] Since \\(\\cos(\\theta) = \\sqrt{1/2}\\), the angle \\(\\theta\\) is 45 degrees. ::: Example 46.1 Project \\(f(t) \\equiv t^2\\) onto \\(g(t) = \\text{one}(t)\\) over the domain \\(-1 \\leq t \\leq 1\\). The projection of \\(f(t)\\) onto \\(g(t)\\) will be \\[\\widehat{f(t)} = \\frac{f(t) \\bullet g(t)}{g(t) \\bullet g(t)}\\ g(t)\\] \\(f(t) \\bullet g(t) \\equiv \\int_{-1}^{1} t^2 dt = \\frac{1}{3} \\left.{\\Large \\strut}t^3\\right|_{-1}^{1} = \\frac{2}{3}\\) \\(g(t) \\bullet g(t) \\equiv \\int_{-1}^1 \\ dt = 2\\) Thus, \\[\\widehat{f(t)} = \\frac{1}{3} \\text{one(t)} = \\frac{1}{3}\\ .\\] The left panel of Figure 46.1 shows the functions \\(f(t) \\equiv t^2\\) and \\(\\color{magenta}{\\widehat{f(t)} \\equiv 1/3}\\) on the domain. The center panel shows the residual function, that is \\(f(t) - \\widehat{f(t)}\\). The right panel gives the square of the length of the residual function, which is \\(\\int_{-1}^1 \\left[f(t) - \\widehat{f(t)}\\right]^{1/2}\\, dt\\) as indicated by the area shaded in \\(\\color{blue}{\\text{blue}}\\). Figure 46.1: Projecting \\(f(t) \\equiv t^2\\) onto \\(g(t) \\equiv \\text{one}(t)\\). The table links to audio files recorded by a human speaker voicing various vowels. Play the sounds to convince yourself that they really are the vowels listed. (It may help to use the controls to slow down the playback.) Vowel Player “o” as in “stone” “e” as in “eel” As you may know, the physical stimuli involved in sound are rapid oscillations in air pressure. Our standard model for oscillations is the sinusoid function, which is parameterized by its period and it’s amplitude. The period of a sound oscillation is short: between 0.3 and 10 milliseconds. The amplitude is small. To get a sense for how small, consider the change in air pressure when you take an elevator up 10 stories in a building. The pressure amplitude of sound at a conversational level of loudness corresponds to taking that elevator upward by 1 to 10 mm. The shapes of the “e” (as in “eel”) and “o” (as in “stone”) sound waves—in short, the waveforms—are drawn in Figure 46.2. Figure 46.2: The waveforms of two vowel sounds. Only about five hundredths of a second is shown. The function resembles none of our small set of pattern-book functions. It is more complicated, more detailed, more irregular than any of the basic modeling functions featured in this book. For many tasks it’s helpful to have a modeling approach that’s well suited to such detailed and irregular functions. For example, we might want to identify the speaker from a recording, or to play the recording slower or faster without changing the essence of the sound, or to tweak the function to have additional properties such as being exactly on tune while maintaining its individuality as a sound. A remarkable aspect of the waveforms in Figure 46.2 is their periodicity. The 0.05 sec graphics domain shown includes roughly seven repetitions of a basic waveform. That is, each cycle lasts about \\(\\frac{0.05 \\text{s}}{7} \\approx 7 \\text{ms}\\). what distinguishes the “e” waveform from the “o” waveform is the shape of the waveform that’s being repeated. The individual cycle of the “o” has three peaks of diminishing amplitude. The “e” cycle has two main peaks, high then low. It also has a very fast wiggle superimposed on the two peaks. An important strategy for modeling such complicated oscillations is to decompose (synonym: analyze) them into a linear combination of simpler parts. 46.2 Sinusoids as vectors The sinusoid is our fundamental model of periodic phenomena. To get started with using sinusoids as vectors, we’ll start with a simple setting: a single sinusoid of a specified frequency. Figure 46.3 shows three sinusoids all with the same frequency, but shifted somewhat in time: Figure 46.3: Three sinusoids with a frequency of \\(\\omega=3\\) cycles per second. Since we have a dot product for functions, we can treat each of the three sinusoids as a vector. For instance, consider the length of waveforms A and B and the included angle between them. ## vector lengths lengthA &lt;- Integrate(waveA(t) * waveA(t) ~ t, domain(t=0:1)) %&gt;% sqrt() lengthA ## [1] 0.7071068 lengthB &lt;- Integrate(waveB(t) * waveB(t) ~ t, domain(t=0:1)) %&gt;% sqrt() lengthB ## [1] 0.7071068 lengthC &lt;- Integrate(waveC(t) * waveC(t) ~ t, domain(t=0:1)) %&gt;% sqrt() lengthC ## [1] 0.7071068 ## dot products dotAB &lt;- Integrate(waveA(t) * waveB(t) ~ t, domain(t=0:1)) dotAB ## [1] -3.984443e-18 dotAC &lt;- Integrate(waveA(t) * waveC(t) ~ t, domain(t=0:1)) dotAC ## [1] -0.1545085 dotBC &lt;- Integrate(waveB(t) * waveC(t) ~ t, domain(t=0:1)) dotBC ## [1] -0.4755283 The cosine of the included angle \\(\\theta\\) between functions A and B is calculated using the the dot product formula: \\[\\cos(\\theta) = \\frac{A\\bullet B}{\\|A\\|\\, \\|B\\|}\\] or, computationally dotAB / (lengthA * lengthB) ## [1] -7.968886e-18 Since \\(\\cos(\\theta) = 0\\), wave A and B are orthogonal. Admittedly, there is no right angle to be perceived from the graph, but the mathematics of angles gives this result. The graphical presentation of orthogonality between waveforms A and B is easier to appreciate if we plot out the dot product itself: the integral of waveform A times waveform B. Figure 46.4 shows this integral using colors, blue for positive and orange for negative. The integral is zero, since the positive (blue) areas exactly equal the negative (orange) areas. Figure 46.4: The dot product between waveforms A and B, graphically. In contrast, waveform A is not orthogonal to waveform C, and similarly for waveform B. Figure 46.5 shows this graphically: the positive and negative areas in the two integrals do not cancel out to zero. Figure 46.5: The dot products between waveforms A and C (top panel) and between B and C (bottom panel). We can project waveform C onto the 2-dimensional subspace spanned by A and B. Since waveforms A and B are orthogonal, This can be done simply by projecting C onto each of A and B one at a time. Here’s a calculation of the scalar multipliers for A and for B and the model vector (that is, the component of C in the A-B subspace): A_coef &lt;- dotAC / lengthA^2 B_coef &lt;- dotBC / lengthB^2 mod_vec &lt;- makeFun(A_coef*waveA(t) + B_coef*waveB(t) ~ t) # length of mod_vec Integrate(mod_vec(t)*mod_vec(t) ~ t, domain(t=0:1)) %&gt;% sqrt() ## [1] 0.7071068 You can see that the length of the model vector is exactly the same as the length of the vector being projected. This means that waveform C lies exactly in the subspace spanned by waveforms A and B. A time-shifted sinusoid of frequency \\(\\omega\\) can always be written as a linear combination of \\(\\sin(2\\pi\\omega t)\\) and \\(\\cos(2\\pi\\omega t)\\). The coefficients of the linear combination tell us both the amplitude of the time-shifted sinusoid and the time shift. Example 46.2 Consider the function \\(g(t) \\equiv 17.3 \\sin(2*pi*5*(t-0.02)\\) on the domain \\(0 \\leq t \\leq 1\\) seconds. The amplitude is 17.3. The time shift is 0.02 seconds. Let’s confirm this using the coefficients on the linear combination of sine and cosine of the same frequency. g &lt;- makeFun(17.3 * sin(2*pi*5*(t-0.02)) ~ t) sin5 &lt;- makeFun(sin(2*pi*5*t) ~ t) cos5 &lt;- makeFun(cos(2*pi*5*t) ~ t) A_coef &lt;- Integrate(g(t) * sin5(t) ~ t, domain(t=0:1)) / Integrate(sin5(t) * sin5(t) ~ t, domain(t=0:1)) A_coef ## [1] 13.99599 B_coef &lt;- Integrate(g(t)*cos5(t) ~ t, domain(t=0:1)) / Integrate(cos5(t) * cos5(t) ~ t, domain(t=0:1)) B_coef ## [1] -10.16868 The amplitude of \\(g(t)\\) is the Pythagorean sum of the two coefficients: sqrt(A_coef^2 + B_coef^2) ## [1] 17.3 The time delay involves the ratio of the two coefficients: atan2(B_coef, A_coef) / (2*pi*5) ## [1] -0.02 For our purposes here, we’ll need only the Pythagorean sum and will ignore the time delay. Figure 46.6 (top) shows the waveform of a note played on a cello. The note lasts about 1 second. The bottom panel zooms in on the waveform, showing 82 ms (that is, 0.082 s). Figure 46.6: Waveform recorded from a cello. The whole note starts with a sharp “attack,” followed by a long period called a “sustain,” and ending with a “decay.” Within the sustain and decay, the waveform is remarkably repetitive, seen best in the bottom panel of the figure. If you count carefully in the bottom panel, you’ll see that the waveform completes 9 cycles in the 0.082 s graphical domain. This means that the period is 0.082 / 9 = 0.0091 s. The frequency \\(\\omega\\) is the reciprocal of this: 1/0.0091 = 109.76 Hz. That is, the cello is vibrating about 110 times per second. In modeling the cello waveform as a linear combination of sinusoids, the frequencies we use ought to respect the period of the cello vibration. Figure 46.7 shows the original waveform as well as the projection of the waveform onto a sinusoid with a frequency of 109.76 Hz. The figure also shows the residual from the projection, which is simply the original waveform minus the projected version. Figure 46.7: Top: The cello waveform and its projection onto a sinusoid with frequency \\(\\omega = 109.76\\) Hz. Bottom: The residual from the projection. The sinusoid with \\(\\omega = 109.76\\) is not the only one that will repeat every 0.0091 s. So will a sinusoid with frequency \\(2\\omega = 219.52\\), one with frequency \\(3\\omega = 329.28\\) and so on. These multiples of \\(\\omega\\) are called the harmonics of that frequency. In Figure ?? (top) the cello waveform is projected onto \\(\\omega\\) and its first harmonic \\(2\\omega\\). In the middle panel, the projection is made onto \\(\\omega\\) and its first three harmonics. In the bottom panel, the projection is onto \\(\\omega\\) and its first eight harmonics. As the number of harmonics increases, the approximation gets better and better. Until now, all the plots of the cello waveform have been made in what’s called the time domain. That is, the horizontal axis of the plots has been time, as seems natural for a function of time. The decomposition into sinusoids offers another way of describing the cello waveform: the frequency domain. In the frequency domain, we report the amplitude and phase of the projection onto each frequency, plotting that versus frequency. Figure 46.8 shows the waveform in the frequency domain. Figure 46.8: The frequency domain description of the cello waveform. From the amplitude graph in Figure 46.8, you can see that only a handful of frequencies account for almost all of the signal. Thus, the frequency domain representation is in many ways much more simple and compact than the time domain representation. The frequency domain description is an important tool in many fields. As you’ll see in Block 6, models of many kinds of systems, from the vibrations of buildings during an earthquake, aircraft wings in response to turbulence, and the bounce of a car moving over a rutted road have a very simple form when stated in the frequency domain. Each sinusoid in the input (earthquake shaking, air turbulence, rutted road) gets translated into the same frequency sinusoid in the output (building movement, wing bending, car bound): just the amplitude and phase of the sinusoid is altered. The construction of the frequency domain description from the waveform is called a Fourier Transform, one of the most important techiques in science. An important tool in chemistry is molecular vibrational spectroscopy in which a sample of the material is illuminated by an infrared beam of light. The frequency of infrared light ranges from about \\(300 \\times 10^7\\) Hz to \\(400 \\times 10^{10}\\) Hz, about 30 million to 40 billion times faster than the cello frequency. Infrared light is well suited to trigger vibrations in the various bonds of a molecule. By measuring the light absorbed at each frequency, a frequency domain picture can be drawn of the molecules in the sample. This picture can be compared to a library of known molecules to identify the makeup of the sample. The analogous procedure for stringed musical instruments such as the cello or violin would be to rap on the instrument and record the hum of the vibrations induced. The Fourier transform of these vibrations effectively paint a picture of the tonal qualities of the instrument. 46.3 Exercises Exercise 46.03: uKOO7K Consider these functions/vectors on the domain \\(0 \\leq t \\leq 1\\): \\(s_1(t) \\equiv \\sin(2\\pi t)\\) \\(s_2(t) \\equiv \\sin(2 \\pi 2 t)\\) (that is, \\(\\omega = 2\\)) \\(s_3(t) \\equiv \\sin(2 \\pi 3 t)\\) (that is, \\(\\omega = 3\\)) \\(c_0(t) \\equiv \\cos(\\pi t)\\) \\(c_1(t) \\equiv \\cos(2 \\pi t)\\) \\(c_2(t) \\equiv \\cos(2 \\pi 2 t)\\) Plot out each of the functions on the domain. How many complete cycles does each function complete as \\(t\\) goes from 0 to 1? What is the length of each function? All of the functions are mutually orthogonal except one. Which is the odd one out? (Hint: If the dot product is zero, the vectors are orthgonal.) Exercise 46.05: sNEKcy Here is a square-wave function: sq_wave &lt;- makeFun(ifelse(abs(t) &lt;= 0.5, 1, 0) ~ t) slice_plot(sq_wave(t) ~ t, domain(t=-1:1)) Find the projection of the square wave onto each of these functions (using the domain \\(-1 \\leq t \\leq 1\\)): \\(c_0 \\equiv 1\\) \\(c_1 \\equiv \\cos(1 \\pi t)\\) \\(c_2 \\equiv \\cos(2 \\pi t)\\) \\(c_3 \\equiv \\cos(3 \\pi t)\\) \\(c_4 \\equiv \\cos(4 \\pi t)\\) \\(c_5 \\equiv \\cos(5 \\pi t)\\) \\(c_6 \\equiv \\cos(6 \\pi t)\\) \\(c_7 \\equiv \\cos(7 \\pi t)\\) Hint: To find the scalar multiplier of projecting \\(g(t)\\) onto \\(f(t)\\), use \\[\\int_{-1}^1 g(t)\\, f(t)\\, dt {\\LARGE /} \\int_{-1}^1 f(t)\\, f(t)\\,dt\\] or, in R/mosaic A &lt;- Integrate(g(t)*f(t) ~ t, domain(t=-1:1)) / Integrate(f(t)*f(t) ~ t, domain(t=-1:1)) Then the projection of \\(g()\\) onto \\(f()\\) is \\(A\\, f(t)\\). Write down the scalar multiplier on each of the 8 functions above. If you calculated things correctly, this is the linear combination of the 8 functions that best matches the square wave. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
