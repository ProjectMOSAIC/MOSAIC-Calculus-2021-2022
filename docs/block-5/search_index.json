[["vectors.html", "Chapter 41 Vectors 41.1 Length &amp; direction 41.2 The nth dimension 41.3 Geometry &amp; arithmetic 41.4 Vector lengths 41.5 Angles 41.6 Orthogonality 41.7 Exercises 41.8 Sub-spaces 41.9 Exercises", " Chapter 41 Vectors Until now, our presentation of calculus has featured functions, sometimes expressed as formulas involving combinations of the basic modeling functions, sometimes generated directly from data by splines. Now we turn to a new framework for expressing functions, the inputs on which they operate, and the kind of outputs they generate. This framework is central to technical work in a huge range of fields. The usual name given to it by mathematicians is linear algebra, although only the word “linear” conveys useful information about the subject. The physicists developing the first workable quantum theory called it matrix mechanics. The framework is fundamental to scientific computation and is often the approach of choice even to non-linear problems. Application of the framework to problems of information access was the spark the ignited the modern era of search engines. Although the words “algebra” and “quantum” may suggest that conceptual difficulties are in store, in fact human intuition is well suited to establishing a useful understanding. We will use two formats to introduce linear algebra: (1) geometric and visual; (2) via simple arithmetic, numbers, and algorithms. 41.1 Length &amp; direction A vector is a mathematical idea that is deeply rooted in everyday physical experience. Geometrially, a vector is simply an object consisting only of length and direction. A pencil is a good physical metaphor for a vector, but a pencil has other, non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil always has position: the place it’s at. Figure 41.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are exactly the same vector. Pencils have position, but vectors don’t. The green pencil shares the same direction, but it has a different length, so it is a different vector from the blue/yellow vector. You can move in a given direction either forward or in reverse. To eliminate this ambiguity, it’s helpful to imagine vectors having a tip and a tail. For the pencil illustrations, the writing end it the tip and the eraser is the tail. Figure 41.2: Two different vectors. They have the same length and are parallel, but they point in opposite directions. Vectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a table top: a two-dimensional space. Naturally, the pencil-vectors are also embedded in our everyday three-dimensional space. The table-top can be thought of as a representation of a two-dimensional subspace of three-dimensional space. Often, we will use vectors to represent a change in position, that is, a step or displacement in the sense of “step to the left” or “step forward.” An individual vector is a step specific length in a particular direction. Much of the useful mathematics of vectors can be understood as constructing instructions for reaching a target: “take three and a half steps along the green vector, then turn and take two steps backwards along the yellow vector.” Vectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are properly represented not as simple numerical quantities but as vectors with magnitude (that is, length) and direction. The statement, “The plane’s velocity is 450 miles per hour to the north-north-west” is perfectly intelligible to most people, describing as it magnitude and direction. Note that the vector velocity can be understood without having to know where the plane is located; vectors have only the two qualities of magnitude and direction. Position is irrelevant to describing velocity, or, for that matter, force or acceleration. The gradients that we studied with partial differentiation (Chapter 24) are vectors. A gradient’s direction points directly uphill from a given point; it’s magnitude tells how steep the hill is at that point. Vectors are a practical tool in many situations such as relative motion. Consider the problem of finding an aircraft heading and speed to intercept another plane that’s also moving. The US Navy training movie from the 1950s shows how such calculations used to be done with paper and pencil. Nowadays such relative motion calculations are computerized. You may well wonder how the computer is able to represent vectors, since pencils aren’t part of computer hardware. The answer is disappointingly simple: the properties of direction and magnitude can also be represented by a set of numbers. Two numbers will do for a vector embedded in two-dimensional space, three for a vector embedded in three-dimensions. Representing a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 41.3, the vector (that is, the green pencil) has been placed in a coordinate system. Usually you would expect there to be labels for each of the coordinate lines, but this labeling is not necessarily to show a vector (even if it is needed to specify a position). The two coordinates to be assigned to the vector are the difference between the tip and the tail. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\). Figure 41.3: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper. By convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure 41.3, which we’ll call \\(\\vec{green}\\), is written numerically as: \\[\\vec{green} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] In more advanced linear algebra, the distinction between a column vector (like \\(\\vec{green}\\)) and a row vector (like \\(\\left[20 \\ 16\\right]\\)) is important. For our purposes in this block, we will only have need of column vectors. Column vectors can be constructed with the rbind() function, as in rbind(1,3,-4) ## [,1] ## [1,] 1 ## [2,] 3 ## [3,] -4 Note that the elements are separated by commas in the ordinary way for R functions that take more than one input. Later in this block, we will be using data frames to define vectors. We’ll introduce the R syntax for that when we need it. Note to R experts: The R language has a data structure called a “vector,” which is a set of elements without the information needed to consider it a row or column vector. As such, the native R “vector” is not suited for linear-algebra computations in R. Many people construct mathematical vectors using the matrix() function. Such a matrix() command to produce a column vector would look like matrix(c(1, 3, -4), ncol=1). This is a professional practice, but we regard this as too verbose for our purposes in this book, so we will use rbind() instead. Note that combining rbind() with c() or other preconstructed R “vectors” will not produce a mathematical row vector rbind(c(1,2,3)) ## [,1] [,2] [,3] ## [1,] 1 2 3 rbind(1:5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 Eventually, we will be constructing matrices. We’ll do this by concatenating column vectors, e.g. cbind( rbind(1,2,3), rbind(10,11, 12) ) ## [,1] [,2] ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 It’s also possible to use cbind() and rbind() in the reverse order, so that the construction statement looks like the matrix itself. rbind( cbind(1, 10), cbind(2, 11), cbind(3, 12) ) ## [,1] [,2] ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 In physics and engineering vectors are used to describe positions, velocities, acceleration, forces, momentum, and so on. In mathematical notation, you will often see vector-valued functions. For instance, a velocity as it changes in time can be written \\(\\vec{v}(t)\\), and it’s common to perform calculus operations such a differentiation, writing it as \\(\\partial_t \\vec{v}(t)\\). The vector-valued function of time \\(\\vec{v}(t)\\) can also be written in terms of scalar-valued components assembled into a vector. For instance, a subscript is often used to identify which component is which, so that \\[\\vec{v}(t) = \\left[\\begin{array}{c}v_x(t)\\\\v_y(t)\\\\v_z(t)\\end{array}\\right]\\] where the \\(x\\), \\(y\\), and \\(z\\) refer to the axes of the coordinate system. Since the physics and engineering vectors are typically 2- or 3-dimensional, when working numerically there’s not much lost by keeping track of the components with a set of scalar quantities rather than as a vector. You saw this already in Chapter 33 when we represented the instantaneous vector position of a robot arm as a pair of scalar-valued functions \\(x(t)\\) and \\(y(t)\\). In linear algebra, vectors often have many more than 3 components. In this book, we will work with vectors with hundreds of components. Services like Google search rely on vector calculations with millions of components. When programming such systems, representing the vectors as individual scalar components is unwieldy. The programming must rely on handling the whole vector as a single entity. 41.2 The nth dimension Living as we do in a palpably three-dimensional space, and being part of a species whose senses and brains developed in three dimensions, it’s hard and maybe even impossible to get a grasp on what higher-dimensional spaces would be like. A lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character, Square, receives a visitor, Sphere, from the three-dimensional world in which Flatland is embedded. Only with difficulty can Square assemble a conception of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace. Even if the spatial extent of higher dimensions is not accessible, the one-dimensional vector inhabitants of any such space can be readily perceived and constructed as a list of numbers. With this device, allow us to introduce vectors from 4, 5, and 6 dimensions, and even \\(n\\) dimensional space. \\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\] Sensible people may consider it mathematical ostentation to promote an everyday column of numbers into a vector in high-dimensional space. The utility of doing so is to help us think about the arithmetic we are about to do on vectors in terms of familiar geometrical concepts: lengths, angles, alignment, and so on. Perhaps unexpectedly, it also guides us to think about data—which consists of columns of numbers in a data frame—using our powerful geometrical intuition. There’s nothing science-fiction-like about so-called “high-dimensional” spaces; they are not usually intended to correspond to a physical space. Vectors with many components are often used in advanced physics to represent the state of a particle. For instance, the state vector could contain both the position and velocity and might be written \\((x, y, z, v_x, v_y, v_z)\\), but you can easily see this as the concatenation of a position vector and a velocity vector, each of which is 3-dimensional. In statistics, engineering, and statistical mechanics, the term “degrees of freedom” is used as an alternative to “dimension.” Another example: computer-controlled machine tools are often described as having 5 degrees of freedom (or more). There is a cutting head whose \\(x, y, z\\) position can be set as well as the head’s orientation (tilt) as an azimuth and inclination. If ever you start to freak out about the idea of a 10-dimensional space, just close your eyes and remember that this is only shorthand for the set of arrays with 10 elements. 41.3 Geometry &amp; arithmetic There are three common mathematical tasks involving vectors that can be understood with simple geometry. Given a set of vectors drawn on paper, you can carry out these tasks by pencil assisted by a simple ruler and a protractor. Measure the length of a vector. Measure the angle between two vectors. Create a new vector by scaling a vector. Scaling makes the new vector longer or shorter or point in the opposite orientation, but the direction remains identical to the original. The geometrical perspective is helpful for many purposes, but often we need to work with vectors using computers. For this, we use the numerical representation of vectors. This section introduces the arithmetic of vectors. With this arithmetic in hand, we can carry out the three tasks (and more!) on vectors that consist of a column of numbers. Especially noteworthy is that the arithmetic enables to to apply simple geometrical concepts to vectors in three or more dimensions. To scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. A good mental image for scaling is based on thinking about the vector as a step or displacement in the direction of \\(\\vec{w}\\). Scaling means to go on a simple walk, taking one step after the other in the same direction as the \\(\\vec{w}\\). We write a scaled vector by placing a number in front of the name of the vector. \\(3 \\vec{w}\\) is a short walk of three steps; \\(117 \\vec{w}\\) is a considerably longer walk; \\(-5 \\vec{w}\\) means to take five steps backwards. You can also take fraction steps: \\(0.5 \\vec{w}\\) is half a step, \\(19.3 \\vec{w}\\) means to take 19 steps followed by a 30% step. Scaling a vector by \\(-1\\) means flipping the vector tip-for-tail; this doesn’t change the length, just the orientation. Arithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. Suppose that we are working with a vector \\(\\vec{v}\\) that has \\(n\\) components. (We’ll also define another vector \\(\\vec{w}\\) to use in examples.) \\[\\vec{v} \\equiv \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\ \\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]\\] To scale a vector by 3 is accomplished by multiplying each component by 3 \\[3\\, \\vec{v} = 3\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right] = \\left[\\begin{array}{r}18\\\\6\\\\-12\\\\\\vdots\\\\3\\\\24\\end{array}\\right]\\] This is perfectly ordinary multiplication applied component by component. Scaling involves a number (the “scalar”) and a single vector. There are other sorts of multiplication however, that involve two or more vectors. The dot product is one approach to multiplication of one vector with another. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\[\\vec{v} \\bullet \\vec{w}\\]. The arithmetic of the dot product involves two steps: Multiply the two vectors component-wise. For instance: \\[\\vec{v} \\vec{w} = \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right] = \\left[\\begin{array}{r}-18\\\\2\\\\20\\\\\\vdots\\\\2\\\\40 \\end{array}\\right]\\] Sum the elements in the component-wise product. For the component-wise product of \\(\\vec{v}\\) and \\(\\vec{w}\\), this will be \\(-18 + 2 + 20 + \\cdots +2 + 40\\). The resulting sum, which is an ordinary quantity, that is, a scalar, is the output of the dot product. That is, the dot product takes two vectors as inputs and produces a scalar as an output. R/mosaic provides a beginner-friendly function for computing a dot product. To mimic the use of the dot, as in \\(\\vec{v} \\bullet \\vec{w}\\), the function will be invoked using infix notation. You have a huge amount of experience with infix notation, even if you never heard the term. Some examples: 3 + 2 7 / 4 6 - 2 9 * 3 2 ^ 4 In principle, you can invoke the +, -, *, /, and ^ operations using functional notation. Nobody does this because the commands are so ugly: `+`(3, 2) ## [1] 5 `/`(7, 4) ## [1] 1.75 `-`(6, 2) ## [1] 4 `*`(9, 3) ## [1] 27 `^`(2, 4) ## [1] 16 The R language makes it possible to define new infix operators, but there is a catch. The new operators must always have a name that begins and ends with the % symbol, for example %in% or %*% or %dot%. You’ll be using %*% and %dot% a lot in this block and the next. Here’s an example of using %dot% to calculate the dot product of two vectors: a &lt;- rbind(1, 2, 3, 5, 8, 13) b &lt;- rbind(1, 4, 2, 3, 2, -1) a %dot% b ## [1] 33 The vectors being combined with %dot% must both have the same number of elements. Otherwise, an error message will result: rbind(2, 1) %dot% rbind(3, 4, 5) ## Error in rbind(2, 1) %dot% rbind(3, 4, 5): Vector &lt;u&gt; must have the same number of elements as vector &lt;b&gt;. To the student encountering the dot product for the first time, a natural response is to wonder what such a two-step operation might be good for. As we walk through this block and the next one, you’ll see the dot product playing a central role. You will be seeing a lot of the dot product, so it’s important to have it firmly in mind that a dot product is not ordinary multiplication. 41.4 Vector lengths The arithmetic used to calculate the length of a vector is based on the Pythagorean theorem. For a vector \\(\\vec{u} = \\left[\\begin{array}{c}4\\\\3\\end{array}\\right]\\) the vector is the hypotenuse of a right triangle with legs of length 4 and 3 respectively. Therefore, \\[\\len{\\vec{u}} = \\sqrt{4^2 + 3^2} = 5\\ .\\] For vectors with more than two components, follow the same pattern: sum the squares of the components then take the square root. The length of a vector \\(\\vec{u}\\) can also be computed using the dot product: \\[\\len{\\vec{u}} = \\sqrt{\\strut\\vec{u} \\bullet \\vec{u}}\\ .\\] Although length has an obvious physical interpretation, in many areas of science including statistics and quantum physics, the square length is a more fundamental quantity. The square length of \\(\\vec{u}\\) is simply \\(\\len{\\vec{u}}^2 = \\vec{u}\\bullet \\vec{u}\\). Example 41.1 Consider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) \\] The length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\). The length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\). In statistics, applications if linear algebra often involve a simple constant vector, which we’ll write \\(\\vec{1}\\). It is simply a column vector of 1s, \\[\\vec{1} \\equiv \\left[\\begin{array}{c}1\\\\1\\\\1\\\\\\vdots\\\\1\\\\1\\\\ \\end{array}\\right]\\ .\\] Common statistical calculations can be expressed compactly in vector notation. For example, if \\(\\vec{x}\\) is an \\(n\\)-dimensional vector, then the mean of the components of \\(\\vec{x}\\), which is often written \\(\\bar{x}\\), is \\[\\bar{x} \\equiv \\frac{1}{n}\\ \\vec{x} \\bullet \\vec{1}\\ .\\] The symbol \\(\\bar{}\\) is pronounced “bar”, and \\(\\bar{x}\\) is pronounced “x-bar.”. Another commonly used statistic is the variance of the components of a vector \\(\\vec{x}\\). This is only slightly more complicated than the mean: \\[\\text{var}(x) \\equiv \\frac{1}{n-1}\\ (\\vec{x} - \\bar{x}) \\bullet (\\vec{x} - \\bar{x})\\ .\\] The quantity \\(\\vec{x} - \\bar{x}\\) is an example of scalar subtraction, which is done on a component-wise basis. For instance, with \\[\\vec{x} = \\left[\\begin{array}{r}1\\\\2\\\\3\\\\4\\\\\\end{array}\\right]\\] then \\(\\bar{x} = 2.5\\). This being the case, \\[\\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right]\\ ,\\] with the variance of \\(\\vec{x}\\) being \\[\\frac{1}{4-1} \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] \\bullet \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] = \\frac{5}{3}\\ .\\] 41.5 Angles Any two vectors of the same dimension have a distinct angle between them. This is easily seen for two-dimensional vectors. Draw two vectors on a sheet of paper. Since vectors have only two properties, length and direction, in your mind’s eye you can pick up one of the vectors and relocate its “tail” to meet the tail of the other vector. The letters L and V illustrate the connection between the two vectors as do the characters ^, &gt;, and &lt;. The angle for L is roughly 90 degrees, the other characters are made of vectors with acute angles (that is, less than 90 degrees). The two vectors and /, when brought together as / subtend an obtuse angle. Measure the angle between two vectors the short way round: between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260 degree angle. In 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the vectors so they are tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector. It’s also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) that are in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components: \\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\end{array}\\right]\\ ,\\] There is a straightforward arithmetic formula for the cosine of the angle \\(\\theta\\) between \\(\\vec{v}\\) and \\(\\vec{w}\\): \\[\\cos(\\theta) = \\frac{v_1\\, w_1 \\ + \\ v_2\\, w_2 \\ + \\ \\cdots\\ + \\ v_n\\, w_n}{\\sqrt{\\strut v_1^2 + v_2^2 + \\cdots + v_n^2}\\ \\sqrt{\\strut w_1^2 + w_2^2 + \\cdots + w_n^2}}\\] You might recognize the two quantities in the denominator of the ratio as the lengths \\(\\|\\vec{v}\\|\\) and \\(\\|\\vec{w}\\|\\) respectively. There’s also a special notation and name for the quantity in the numerator. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\(\\vec{v}\\cdot\\vec{w}\\) and the sum of pairwise products of the vectors components: \\[\\text{dot product:}\\ \\ \\ \\ \\ \\vec{v}\\cdot\\vec{w} \\equiv v_1\\, w_1 + v_2\\, w_2 + \\cdots + v_n\\, w_n\\ .\\] Using the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\] If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), there is a function that will do the conversion show in in Figure 41.4. Figure 41.4: The \\(\\arccos()\\) function converts \\(\\cos(\\theta)\\) to \\(\\theta\\). What does the angle \\(\\theta\\) between two vectors tell us? In geometrical terms, the angle tells us how strongly aligned the vectors are. An angle of 0 tells us the vectors point in exactly the same direction, and angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that by appropriate scalar multiplication, the two vectors could be made exactly equal to one another and, consequently, that the scaled vectors would be one and the same. Angles such as 5 or 175 degrees indicate that the two vectors are mostly aligned, but imperfectly. When the angle is 90 degrees of course—a right angle—the two vectors are perpendicular. The vector alignment has a particularly important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics there is an important quantity called the correlation coefficient, denoted \\(r\\). To say that two variables are correlated means that the variables are connected to one another in some way. For instance, among children, height and age are correlated. Since height tends to increase along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\). A negative correlation means that as one variable increases the other tends to decrease. Temperature and elevation are negatively correlated, as are the pressure and volume of a gas at a given temperature. The most negative possible correlation is \\(r=-1\\). A zero correlation indicates that there is no simple relationship between the two variables. In terms of vectors, that is, the columns in the data frame, the correlation coefficient \\(r\\) is exactly the same quantity as the cosine of the angle between the vectors. At the time the correlation coefficient was invented in the 1880s, it was not widely appreciated that \\(r\\) is simply the cosine of an angle. Perhaps the several generations of statistics students who have studied correlation would have had a better grasp on the subject if it had been called alignment and measured in degrees. 41.6 Orthogonality Two vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech we call a 90 degree angle a “right angle.” The word “orthogonal” is really just a literal translation of “right angle.” The syllable “gon” indicates an angle, as in the five-angled pentagon or six angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet). Two vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero. Example 41.2 Find a vector that’s orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\). The arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\). We can confirm the orthogonality by calculating the dot product: \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\). In R, this can be written u &lt;- rbind( 1, 2) v &lt;- rbind(-2, 1) u %dot% v ## [1] 0 Example 41.3 Find a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\). We have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components. For instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\ \\text{__}\\\\ \\text{__}\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}\\text{__}\\\\0\\\\\\text{__}\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\). 41.7 Exercises Exercise 41.XX: IpAGZw Consider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) \\] Use R commands to create the vectors \\(\\vec{u}\\) and \\(\\vec{w}\\) and find their lengths using the `%dot% operator. u &lt;- rbind( ____ ) # length of u sqrt( ____ ) v &lt;- rbind( ______ ) # length of v sqrt( ____ ) ```&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/snake-choose-ring.Rmd&quot; href=&quot;#Of7QlW&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/snake-choose-ring.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;Of7QlW&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Write (or draw) your answers on a sheet of paper. You can do the any calculations you need in the sandbox. Make sure to write column vectors in a correct column format. Use a [SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/) for numerical calculations. a. What is $5.42 \\left(\\begin{array}{r}7.3\\\\8.9\\\\-2.4\\end{array}\\right)$? b. What is $-2.67 \\left(\\begin{array}{r}-19.34\\\\0.23\\\\14.82\\end{array}\\right)$? c. Draw a vector of your own choice. Next to it, draw the vector which is 2.5 times your vector. d. Take your original vector from (c) and draw the vector which is -3 times it. e. Write down the R code to create a vector named `w` that is $$\\left(\\begin{array}{c}2\\\\5\\\\1\\\\5\\end{array}\\right)$$ &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/octopus-sell-mattress.Rmd&quot; href=&quot;#dwALW7&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/octopus-sell-mattress.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;dwALW7&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Consider the two vectors $$\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) $$ **Question A** What is the length of the vector $\\vec{u}$? i. &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ There are two elements in $\\vec{u}$, but that&#39;s not what the &quot;length&quot; of a vector means.&lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;Nice! &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;25&lt;span class=&#39;mcanswer&#39;&gt;︎✘ This is the length-squared. Take the square root of it to find the length.&lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** What is the length of the vector $\\vec{w}$? i. &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;Nice! Right: $\\sqrt{1^2 + 1^2 + 1^2 + 1^2}$&lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ There are four elements in $\\vec{w}$$, and the square-length of $\\vec{v}$ happens to be 4 ($1^2 + 1^2 + 1^2 + 1^2$), but neither of these is the &quot;length&quot; of $\\vec{w}$.&lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** What is $w^T \\circ u$? i. &lt;span class=&#39;Zchoice&#39;&gt;7&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;It doesn&#39;t exist.&lt;span class=&#39;mcanswer&#39;&gt;Nice! Two vectors have to have the same dimension for a dot product to make sense.&lt;/span&gt;&lt;/span&gt;&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/rhinosaurus-break-knob.Rmd&quot; href=&quot;#xuENab&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/rhinosaurus-break-knob.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;xuENab&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Suppose $$\\vec{\\mathbf u} \\equiv \\left(\\begin{array}{c}4\\\\-3\\end{array}\\right) \\ \\ \\mbox{and}\\ \\ \\vec{\\mathbf v} \\equiv \\left(\\begin{array}{c}2\\\\2\\end{array}\\right)\\ .$$ The following questions ask you to compute the **length** of the vector given. (If none of the available choices are exactly right, choose the closest.) **Question A** $3 \\vec{u}$     &lt;span class=&#39;Zchoice&#39;&gt;3&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;15&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** 42) $-2 \\vec{u}$     &lt;span class=&#39;Zchoice&#39;&gt;3&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;15&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** $-\\vec{v}$     &lt;span class=&#39;Zchoice&#39;&gt;3&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $Right. It&#39;s $\\sqrt{8} = \\sqrt{2^2 + 2^2}$ to be precise.&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;15&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question D** $\\vec{u}+ \\vec{v}$     &lt;span class=&#39;Zchoice&#39;&gt;3&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;15&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question E** $2 \\vec{u} - 5 \\vec{v}$     &lt;span class=&#39;Zchoice&#39;&gt;3&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;15&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/duck-do-pantry.Rmd&quot; href=&quot;#MBTGQt&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/duck-do-pantry.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;MBTGQt&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Collision course? Consider the diagram showing two straight-line tracks, a dot on each track, and a vector. &lt;img src=&quot;www/collision.png&quot; width=&quot;90%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Let&#39;s imagine that dot 1 is an aircraft and that the black vector attached to it is the aircraft&#39;s velocity. We&#39;ll call this $\\vec{v}_1$, Similarly for dot 2, where the velocity vector will be called $\\vec{v}_2$. There&#39;s a third vector drawn in red: the difference in position of the two aircraft at the exact moment depicted in the drawing. The question we want to address is whether the aircraft are on a collision course. Obviously, the two courses cross. So we know that the two aircraft will cross the same point. For a collision, the aircraft have to cross that point at the same time. Copy over the drawing to your own piece of paper. You don&#39;t need to get the vectors and positions exactly right; any reasonable approximation will do. Now you are going to do *visual* vector addition and subtraction to answer the collision question. 1) The *relative* velocity of the two planes is the difference between their velocities. Subtract $\\vec{v}_2$ from $\\vec{v}_1$ and draw the resulting vector. Pay attention to both the *length* and *direction* of the relative velocity. 2) The displacement between the two planes is the red vector: the position of dot 2 subtracted from dot 1. Compare the *directions* of the relative velocity vector and the displacement vector. **If they are aligned, then the planes are on a collision course.** 3) In the picture as drawn, the relative velocity vector and the displacement vector are not aligned. Figure out how much you would need to change the length of $\\vec{v}_2$ so that the relative velocity *does* align with the displacement. (Keep the direction the same.) Draw this new vector and label it clearly &quot;vector for intercept.&quot; 4) In (3) you changed the *length* of $\\vec{v}_2$ keeping the direction the same. Now you are going to keep $\\vec{v}_2$ at the original length, but change its direction so that the new relative velocity is aligned with the displacement vector. Items (3) and (4) are two different ways of designing an intercept of plane 1 by plane 2. Bonus) You can figure out how long it takes for each plane to reach the intersection point by finding out how many multiples of the velocity vector will cover the line segment between the plane&#39;s position and the intersection point. For example, in the original drawing $4 \\vec{v}_1$ will bring the plane to the intersection point, so it takes 4 &quot;time units&quot; for the plane to reach the point. (What is the time unit? If velocity is in miles/hour, then the time unit is hours. If the velocity is in feet/second, then the time unit is seconds.) Your task: Figure out where aircraft 2 will be in 4 time units. This will tell you the separation between aircraft 2 and aircraft 1 when 1 reaches the intersection point. Draw and clearly label this vector. &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/squirrel-hear-glasses.Rmd&quot; href=&quot;#Q2ars0&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/squirrel-hear-glasses.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;Q2ars0&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Copy over and label these vectors onto your paper. Any good approximation will do. &lt;img src=&quot;www/4vectors.png&quot; width=&quot;80%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Then, on paper, draw (and label with the Roman numeral) the following vector additions and subtractions. (You should &quot;show your work&quot; by putting the vectors being added in the diagram along with the result of the addition.) i. $\\vec{a} + \\vec{b}$ ii. $\\vec{c} + \\vec{d}$ iii. $\\vec{d} + \\vec{b} + \\vec{a}$ iv. $\\vec{b} - \\vec{a}$&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/dog-let-coat.Rmd&quot; href=&quot;#OoPatc&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/dog-let-coat.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;OoPatc&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Draw two vectors with different directions. Estimate by eye the angle between them. Give your angle in two different units: degrees and radians. (The conversion is degrees = $\\frac{180}{\\pi}$ radians. Or, roughly, degrees = 57.3 radians.)&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/beech-iron-clock.Rmd&quot; href=&quot;#Gw42pX&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/beech-iron-clock.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;Gw42pX&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; *Note: The radio-button multiple choice questions don&#39;t allow us to display a column vector as such. Instead, we use the notation involving a superscript T. For instance, $(2, -3)^T$ is a column vector which we would ordinarily write $\\left(\\begin{array}{c}2\\\\-3\\end{array}\\right)$ A vector written like $(2, -3)$, without the $^T$, is a row vector.* Suppose $$\\vec{\\mathbf{u}} \\equiv \\left(\\begin{array}{c}2\\\\-3\\end{array}\\right) \\ \\ \\hbox{and}\\ \\ \\vec{\\mathbf{v}} \\equiv \\left(\\begin{array}{c}4\\\\1\\end{array}\\right)$$ Compute the following linear combinations (arithmetically) ... **Question A** $- \\vec{\\mathbf{u}} -8 \\vec{\\mathbf{v}}$ i. &lt;span class=&#39;Zchoice&#39;&gt;$(-34, -5)^T$&lt;span class=&#39;mcanswer&#39;&gt;Right! &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$(-34, -5)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ The linear combination of two column vectors will be a column vector.&lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$(-34, 15)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ This is a row vector, not a column vector.&lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$(34, 15)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Watch your arithmetic.&lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;Invalid combination&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** $2.1 \\vec{\\mathbf{u}} -1.3 \\vec{\\mathbf{v}}$ i. &lt;span class=&#39;Zchoice&#39;&gt;$(-1, -7.6)^T$&lt;span class=&#39;mcanswer&#39;&gt;Correct. &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$(0.8)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ The linear combination of two column vectors will be a column vector.&lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$(-1, -7.6)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ This is a row vector, not a column vector.&lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$(2.1, -1.3)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ These are the scalar multipliers in the linear combination, not the value of the linear combination.&lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;Invalid combination&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** $-0.5 \\vec{\\mathbf{u}} + 3.2 \\vec{\\mathbf{v}}$ i. &lt;span class=&#39;Zchoice&#39;&gt;$(11.8, 4.7)^T$&lt;span class=&#39;mcanswer&#39;&gt;Good. &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$(2.7)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ The linear combination of two column vectors will be a column vector.&lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$(13.8, 1.7)$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ This is a row vector&lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$(34, 16)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Watch your arithmetic.&lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;Invalid combination&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question D** $7 \\vec{\\mathbf{u}} + 5 \\vec{\\mathbf{v}}$ i. &lt;span class=&#39;Zchoice&#39;&gt;$(34, -16)^T$&lt;span class=&#39;mcanswer&#39;&gt;Right! &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$(32, -14)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Check your arithmetic!&lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$(34, -14)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Check your arithmetic!&lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$(32, -16)^T$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Check your arithmetic!&lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;Invalid combination&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/snake-ride-glasses.Rmd&quot; href=&quot;#1Jxboc&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/snake-ride-glasses.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;1Jxboc&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; In this exercise, you are going to *check* proposed solutions to the target problem. Each question poses one target problem. One of the answers is correct. Use R in a [SANDBOX](https://maa-statprep.shinyapps.io/CalcZ-Sandbox/) to select the correct answer. The vectors you will be working with are: $$\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ $$ **Question A** **i.** What linear combination of $\\vec{a}$ and $\\vec{b}$ will reach target $T$? i. &lt;span class=&#39;Zchoice&#39;&gt;$3\\vec{a} - 7\\vec{b}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$\\vec{a} + 2\\vec{b}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$-4 \\vec{a} + 7 \\vec{b}$&lt;span class=&#39;mcanswer&#39;&gt;Nice! &lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$- \\vec{a} + 4\\vec{b}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;No combination will reach $\\vec{T}$ exactly.&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane.&lt;/span&gt;&lt;/span&gt; vi. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** **ii.** What linear combination of $\\vec{b}$ and $\\vec{c}$ will reach target $T$? i. &lt;span class=&#39;Zchoice&#39;&gt;$\\frac{4}{3}\\vec{b} - \\frac{5}{3}\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$\\frac{2}{3}\\vec{b} + \\frac{7}{3}\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$-\\frac{7}{3}\\vec{b} + \\frac{5}{3}\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$\\frac{5}{3} \\vec{b} + \\frac{4}{3} \\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;Right! &lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;No combination will reach $\\vec{T} exactly.&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane.&lt;/span&gt;&lt;/span&gt; vi. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** **iii.** What linear combination of $\\vec{b}$ and $\\vec{d}$ will reach target $T$? i. &lt;span class=&#39;Zchoice&#39;&gt;$0\\vec{b} - \\frac{3}{2}\\vec{d}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$\\frac{1}{2}\\vec{b} + \\frac{1}{2}\\vec{d}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$0 \\vec{b} - \\frac{1}{2} \\vec{d}$&lt;span class=&#39;mcanswer&#39;&gt;Right! &lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$\\frac{3}{2}\\vec{b} + \\frac{1}{2}\\vec{d}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;No combination will reach $\\vec{T} exactly.&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane.&lt;/span&gt;&lt;/span&gt; vi. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question D** **iv.** Which of these linear combinations of $ ec{a}$, $\\vec{b}$ and $\\vec{c}$ will reach target $T$? i. &lt;span class=&#39;Zchoice&#39;&gt;$\\vec{a} -\\vec{b} - 2\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; ii. &lt;span class=&#39;Zchoice&#39;&gt;$2\\vec{a} +\\vec{b} - 2\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; iii. &lt;span class=&#39;Zchoice&#39;&gt;$2\\vec{a} -\\vec{b} + 2\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;Good. &lt;/span&gt;&lt;/span&gt; iv. &lt;span class=&#39;Zchoice&#39;&gt;$-2\\vec{a} +\\vec{b} + 2\\vec{c}$&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; v. &lt;span class=&#39;Zchoice&#39;&gt;No combination will reach $\\vec{T} exactly.&lt;span class=&#39;mcanswer&#39;&gt;︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. Adding a third vector just increases the number of possibilities.&lt;/span&gt;&lt;/span&gt; vi. &lt;span class=&#39;Zchoice&#39;&gt;None of the above&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/falcon-begin-door.Rmd&quot; href=&quot;#N98zli&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/falcon-begin-door.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;N98zli&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; &lt;img src=&quot;www/4vectors.png&quot; width=&quot;80%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Referring to the vectors $\\vec{a}$, $\\vec{b}$, $\\vec{c}$, and $\\vec{d}$ in the figure, construct these linear combinations. Your diagram should show both the scaled vectors being added and the output of the linear combination. i. $2 \\vec{a} + 1 \\vec{b}$ ii. $1.5 \\vec{c} - 2 \\vec{d}$ iii. $- \\vec{b} + 2\\vec{c} + 3\\vec{d}$&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/crocodile-talk-mattress.Rmd&quot; href=&quot;#PCfYDo&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/crocodile-talk-mattress.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;PCfYDo&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; In physics and engineering, there is a very important operation on two vectors $(a, b, c)^T$ and $(e, f, g)^T$ called the &quot;cross product.&quot; (The operation is only defined for vectors in 3-dimensional space.) The output of the cross product is another 3-dimensional vector which can be calculated arithmetically as: $$\\left(\\begin{array}{c}a\\\\b\\\\c\\end{array}\\right) \\times \\left(\\begin{array}{c}e\\\\f\\\\g\\end{array}\\right) \\equiv \\left(\\begin{array}{c}b g - c f\\\\c e - a g\\\\a f - b e\\end{array}\\right)$$ **DD16 Exercise 6** on paper. Make up coordinates for two three-dimensional vectors $\\vec{u}$ and $\\vec{v}$ that point in different directions. Then carry out the arithmetic to find the vector $\\vec{w}$ that is the cross product $\\vec{u} \\times \\vec{v}$. i. Find the angle between $\\vec{w}$ and $\\vec{u}$. ii. Find the angle between $\\vec{w}$ and $\\vec{v}$. Other than this brief description, we will not use cross products at all in this course. But keep them in mind for your upcoming physics and engineering courses. &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/cat-do-pants.Rmd&quot; href=&quot;#4QzeWP&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/cat-do-pants.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;4QzeWP&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; a. Place a dot on a piece of paper. Hold a pencil perpendicular to the paper with the eraser on the dot. Now, (using a pen!) draw a vector on the paper that is perpendicular to the pencil. Now draw 5 more vectors, each different from the other, that are all perpendicular to the pencil. b. Draw a vector $\\vec{c}$ on the paper. Now draw another vector on the paper that is perpendicular to the first. Label it $\\vec{a}$. Find another vector, to be labeled $\\vec{b}$ that lies on the paper, is orthogonal to $\\vec{c}$, and has a different direction from $\\vec{a}$. It will turn out that $\\vec{a} = k \\vec{b}$. Write down the numerical value of the scalar $k$ for your $\\vec{a}$ and $\\vec{b}$. c. Consider the vector $\\vec{w} \\equiv (2, 5)^T$. Using only arithmetic, find some other vector $\\vec{z}$ that is perpendicular to $\\vec{w}$. (Hint: To be perpendicular, $\\vec{z}^T \\cdot \\vec{w}$ must be zero. So make up two numbers such $a$ and $b$ such that $2 a + 5 b = 0$.) Write $\\vec{z}$ on your paper. d. Repeat (c) and create another vector $\\vec{z}_2$ that is different from $\\vec{z}$. Write $\\vec{z}_2$ on your paper. In addition, find the scalar $k$ such that $\\vec{z}_2 = k \\vec{z}$. *In the following exercises, you are asked to find numerical vectors that are perpendicular (&quot;orthogonal&quot;) to the stated vector. This is equivalent to finding a new vector whose dot product with the stated vector is zero. (Except ... the new vector can&#39;t be all zeros! The all-zero vector is exceptional and has no direction.) One effective strategy is to write down a vector consisting of a 1 in any position you like and zeros elsewhere. Find the dot product, calling it $d$. Then select one of the zeros to turn it non-zero. What value should it have? Just enough so that when included in the dot product it contributes $-d$.* e. Consider the vector $\\vec{w} \\equiv (1, 2, -3, 2)^T$. i. Find a vector $\\vec{a}$ that is perpendicular to $\\vec{w}$. ii. Find another vector $\\vec{b}$ that is perpendicular both to $\\vec{w}$ and to $\\vec{a}$. (Hint: It can be done.) iii. Create yet another vector $\\vec{c}$ that is a linear combination of $\\vec{a}$ and $\\vec{b}$. (You can choose whatever combination you like, but both coefficients should be non-zero.) What is the angle between $\\vec{c}$ and $\\vec{w}$? f. You have been sent on a mission to 5-dimensional space. Your task is to find a vector that is perpendicular to $\\vec{w} \\equiv (2,1,3,3,-4)^T$. Do so. g. BONUS. Find as many vectors as you can that are perpendicular to $\\vec{w} \\equiv (2,1,3,3,-4)^T$ and to each other. The arithmetic will not necessarily be easy. When you give up (with honor!), figure out how many mutually perpendicular vectors there can be in 5-dimensional space. How about 10-dimensional space? &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/girl-send-scarf.Rmd&quot; href=&quot;#5zmmBu&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/girl-send-scarf.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;5zmmBu&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Find the lengths of these vectors. Each square on the &quot;graph paper&quot; is 1 unit. Assume vectors begin and end exactly on the graph-paper intersections. &lt;img src=&quot;www/basic-vectors.png&quot; width=&quot;90%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; **Question A** Length of vector $\\vec{A}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** Length of vector $\\vec{B}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** Length of vector $\\vec{C}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question D** Length of vector $\\vec{D}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question E** Length of vector $\\vec{E}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question F** Length of vector $\\vec{A} + 2 \\vec{B}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question G** Length of vector $\\vec{C} - \\vec{B}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question H** Length of vector $\\vec{C} + \\vec{E}$.     &lt;span class=&#39;Zchoice&#39;&gt;2&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;2.82&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;4.12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;5.66&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6.32&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.18&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;11.66&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;12.22&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/bear-fight-closet.Rmd&quot; href=&quot;#7KJQQd&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/bear-fight-closet.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;7KJQQd&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; &lt;img src=&quot;www/basic-vectors.png&quot; width=&quot;90%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; **Question A** What is $\\vec{C}^T\\cdot\\vec{D}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question B** What is $\\vec{E}^T\\cdot\\vec{B}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question C** What is $\\vec{A}^T\\cdot\\vec{E}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question D** What is $\\vec{D}^T\\cdot\\vec{E}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question E** What is $\\left(\\vec{A}+\\vec{B}\\right)^T\\cdot\\vec{C}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question F** What is $\\left(\\vec{C}+\\vec{D}\\right)^T\\cdot\\vec{E}$?     &lt;span class=&#39;Zchoice&#39;&gt;-8&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;0&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;6&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;10&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;14&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;16&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;24&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;30&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;34&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;40&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;42&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;&lt;/details&gt; &lt;details&gt; &lt;summary&gt;**Exercise 41.XX**: &lt;span&gt;&lt;a name=&quot;File: Exercises/titmouse-stand-pantry.Rmd&quot; href=&quot;#k5u7hG&quot;&gt;&lt;img src=&quot;www/icons8-signpost.png&quot; title=&quot;Location: Exercises/titmouse-stand-pantry.Rmd&quot; width=&quot;12px&quot;/&gt;&lt;/a&gt;&lt;span style=&quot;color: red; font-size: 9pt;&quot;&gt;k5u7hG&lt;/red&gt;&lt;/span&gt;&lt;/summary&gt; Here are 12 vectors, labeled &quot;a&quot; through &quot;m.&quot; (Letter &quot;i&quot; has been left out.) There are several quick questions, each of which makes a claim about whether the sum of two vectors equals a third. Answer *true* or *false* to the claim. There are no tricks about exactitude, so if the claim is close to being true, answer true. &lt;img src=&quot;www/vectors-for-sums.png&quot; width=&quot;90%&quot; style=&quot;display: block; margin: auto;&quot; /&gt; **Question A** 1) $\\vec{a} + \\vec{b} = \\vec{L}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; **Question B** 2) $\\vec{b} + \\vec{J} = \\vec{a}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; **Question C** 3) $\\vec{b} + \\vec{m} = \\vec{J}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; **Question D** 4) $\\vec{c} + \\vec{f} = \\vec{d}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; **Question E** 5) $\\vec{k} + \\vec{L} = \\vec{e}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; **Question F** 6) $\\vec{e} + \\vec{b} = \\vec{m}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt; **Question G** 7) $\\vec{m} + \\vec{g} = \\vec{b}$     &lt;span class=&#39;Zchoice&#39;&gt;True&lt;span class=&#39;mcanswer&#39;&gt;$\\heartsuit\\ $&lt;/span&gt;&lt;/span&gt;       &lt;span class=&#39;Zchoice&#39;&gt;False&lt;span class=&#39;mcanswer&#39;&gt;︎✘ &lt;/span&gt;&lt;/span&gt; &lt;/details&gt; &lt;!--chapter:end:B5-vectors.Rmd--&gt; # Linear combinations of vectors {#linear-combs-vectors} ::: {.underconstruction} Chapter not yet released &lt;!-- $\\vec{v}$ \\vec $\\mathit{M}$ \\mathit $\\color{magenta}{\\alpha}$ $\\color{red}{\\mathit{M}}$ $\\color{green}{\\mathit{M}}$ $\\color{blue}{\\mathit{M}}$ $\\color{cyan}{\\mathit{M}}$ $\\color{magenta}{\\mathit{M}}$ $\\color{yellow}{\\mathit{M}}$ $\\color{black}{\\mathit{M}}$ $\\color{gray}{\\mathit{M}}$ $\\color{white}{\\mathit{M}}$ $\\color{darkgray}{\\mathit{M}}$ $\\color{lightgray}{\\mathit{M}}$ $\\color{brown}{\\mathit{M}}$ $\\color{lime}{\\mathit{M}}$ $\\color{olive}{\\mathit{M}}$ $\\color{orange}{\\mathit{M}}$ $\\color{pink}{\\mathit{M}}$ $\\color{purple}{\\mathit{M}}$ $\\color{teal}{\\mathit{M}}$ $\\color{violet}{\\mathit{M}}$ --&gt; ::: &lt;div style=&quot;float:right;&quot;&gt;[![](www/icons/edit.png)](https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/Block-5/B5-linear-combinations.Rmd)&lt;/div&gt; In this chapter, we introduce ***linear combinations of vectors***. As you recall, a linear combination is a sum of basic elements each of which has been ***scaled***. For instance, in Block 1 we looked at linear combinations of functions such as $$g(t) = A + B e^{kt}$$ which involves the basic functions $\\text{one}(t)$ and $e^{kt}$ scaled respectively by $A$ and $B$. Linear combinations of vectors involve scaling and addition, which are simple seen either as numerical operations or a geometric ones. A useful concept will be the set of all vectors that can be constructed as linear combinations of given vectors. This set of all possibilities, called the ***subspace spanned*** by the given vectors is key to understanding how to find the &quot;best&quot; scalars for a given purpose. ## Scaling vectors To scale a vector means to change its length without altering its direction. Scaling by a negative number flips the vector tip-for-tail. Figure \\@ref(fig:scaling) shows two vectors $\\vec{v}$ and $\\vec{w}$ together with several scaled versions of each. &lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt; &lt;img src=&quot;MOSAIC-Calculus_files/figure-html/scaling-1.png&quot; alt=&quot;Vectors $\\vec{v}$ and $\\vec{w}$ and some scaled versions of them.&quot; width=&quot;90%&quot; /&gt; &lt;p class=&quot;caption&quot;&gt;(\\#fig:scaling)Vectors $\\vec{v}$ and $\\vec{w}$ and some scaled versions of them.&lt;/p&gt; &lt;/div&gt; The vectors we create by scalar multiplication can be placed anywhere we like. Arithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g. $$\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\ -\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ $$ Every vector is associated with a subspace that is ***one-dimensional***; you can only reach the points on a line by stepping in the direction of a vector. ## Adding vectors To add two vectors, choose either one of the vectors as a start, then move the tail of the second vector to the tip of the first, as in Figure \\@ref(fig:add-yellow-green). &lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt; &lt;img src=&quot;www/pencils/addition.png&quot; alt=&quot;Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector.&quot; width=&quot;50%&quot; /&gt; &lt;p class=&quot;caption&quot;&gt;(\\#fig:add-yellow-green)Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector.&lt;/p&gt; &lt;/div&gt; Adding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to whatever place is convenient. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result---the blue pencil in the picture above---has the length and direction from the tail of the first pencil (yellow) to the tip of the second (green). But so long as we maintain this length and direction, we can put the result (blue) anywhere we want. Arithmetically, vector addition is simply a matter of working with each component individually. For instance, consider adding two vectors $\\vec{v}$ and $\\vec{w}$: $$\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}$$ Unlike our pencil exemplars of vectors, which must of physical necessity always be in the three-dimensional space we inhabit, mathematical vectors can be embedded in any-dimensional space. Addition is applicable to vectors embedded in the same space. Arithmetically, this means that the two vectors to be added must have the same number of components. Arithmetic ***subtraction*** of one vector from another is a simple component-wise operation. For example: $$\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .$$ From a geometrical point of view, many people like to think of $\\vec{v} - \\vec{w}$ in terms of placing the two vectors **tail to tail** as in Figure \\@ref(fig:subtract-blue-from-yellow). Read out the result as the vector running from the tip of $\\vec{v}$ to the tip of $\\vec{w}$. In Figure \\@ref(fig:subtract-blue-from-yellow), the yellow vector is $\\vec{v}$, the blue vector is $\\vec{w}$. The result of the subtraction is the green vector. &lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt; &lt;img src=&quot;www/pencils/subtraction.png&quot; alt=&quot;Subtracting blue from yellow gives green.&quot; width=&quot;50%&quot; /&gt; &lt;p class=&quot;caption&quot;&gt;(\\#fig:subtract-blue-from-yellow)Subtracting blue from yellow gives green.&lt;/p&gt; &lt;/div&gt; ::: {.takenote data-latex=&quot;&quot;} Vector addition and subtraction work just like arithmetic with scalars, but vector ***multiplication*** and ***division*** do not follow the same pattern. As a matter of arithmetic, it&#39;s easy to carry out the ***component-wise multiplication*** or ***component-wise division***, for instance $$\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large \\times} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w}\\ \\ \\ = \\underbrace{\\left[\\begin{array}{r}3\\\\-4\\\\-4\\\\-18.12\\end{array}\\right]}_\\text{componentwise product}\\ \\ \\text{is NOT vector multiplication}\\ .$$ In vector mathematics, &quot;multiplication&quot; is a much richer concept than componentwise arithmetic. In fact, there are four different kinds of &quot;multiplication&quot; and it&#39;s important to keep track of which one you mean in any context. In this book, we will use two kinds of &quot;multiplication&quot;: * ***matrix multiplication*** which, as you&#39;ll see later in the chapter, is about linear combinations of vectors. * ***inner product***, which we prefer to call ***dot product*** and encountered in the previous chapter as a way of calculating vector lengths and the angle between vectors, is a special case of matrix multiplication. Another two forms of &quot;multiplication,&quot; which we won&#39;t use in this book are: * ***cross product*** which is very important in physics and engineering. * ***outer product*** which has important applications in data science. ::: ## Linear combinations In the previous chapter, we suggested that you think of a vector as a &quot;step&quot; or displacement in a given direction and of a given magnitude as in, &quot;1 foot to the northeast.&quot; This interpretation highlights the mathematical structure of vectors: just a direction and a length, nothing else. The &quot;step&quot;-interpretation is also faithful to an important reason why vectors are useful. We use steps to get from one place to another. Similarly, a central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one &quot;place&quot; to another. We&#39;ve used quotation marks around &quot;place&quot; because we are not necessarily referring to a physical destination. We&#39;ll get to what else we might mean by &quot;place&quot; later in this chapter. As a fanciful example of getting to a &quot;place,&quot; consider a treasure hunt. You are given these instructions to get there: &gt; i. On June 1, go to the flagpole before sunrise. ii. At 6:32, walk 213 paces away from the sun. iii. At 12:19, walk 126 paces toward the sun. The sun position varies over the day, so the direction to the sun on June 1 at 6:32 will be different than at 12:19. Figure \\@ref(fig:sun-direction) show an aerial views annotated with the direction of the sun at 6:32 and 12:19 on June 1. &lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt; &lt;img src=&quot;www/sun-directions.png&quot; alt=&quot;Maps showing the directions of sunrise and sunset on June 1 at latitude/longitude (38.0091,-104.8871). The Sun&#39;s direction at 6:32 is shown in the left map, the direction at 12:19 in the right map. Source: [suncalc.org](https://www.suncalc.org/)&quot; width=&quot;80%&quot; /&gt; &lt;p class=&quot;caption&quot;&gt;(\\#fig:sun-direction)Maps showing the directions of sunrise and sunset on June 1 at latitude/longitude (38.0091,-104.8871). The Sun&#39;s direction at 6:32 is shown in the left map, the direction at 12:19 in the right map. Source: [suncalc.org](https://www.suncalc.org/)&lt;/p&gt; &lt;/div&gt; The treasure-hunt directions are in the form of a ***linear combination*** of vectors. For each of the two vectors described in the treasure instructions, the length of the vector is 1 pace. (Admittedly, not a scientific unit of length.) The direction of each vector is toward the sun. Scaling vector (ii) by -213 and vector (iii) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure. Since this is a calculus book, and calculus is about functions, our interest in linear combinations of vectors in this book relates to the construction (and de-construction) of functions. Recall from Block 1 (Section 4.2) the idea of representing a function as a ***table*** of inputs and the corresponding outputs. Here is such a table with some of our pattern-book functions. &lt;table class=&quot; lightable-minimal&quot; style=&#39;font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;&#39;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align:right;&quot;&gt; t &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; one(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; identity(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; exp(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; sin(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; pnorm(t) &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1.000000 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0000000 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.5000000 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1.105171 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0998334 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.5398278 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.2 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.2 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1.221403 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.1986693 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.5792597 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.3 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.3 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1.349859 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.2955202 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.6179114 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.4 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.4 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1.491825 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.3894183 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.6554217 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&quot;6&quot; align=&quot;center&quot;&gt;... and so on ...&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.6 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.6 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 99.48432 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -0.9936910 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.9999979 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.7 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.7 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 109.94717 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -0.9999233 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.9999987 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.8 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.8 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 121.51042 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -0.9961646 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.9999992 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.9 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.9 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 134.28978 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -0.9824526 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.9999995 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 5.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 5.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 148.41316 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -0.9589243 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.9999997 &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; In this representation, each of the pattern-book functions is a column of numbers, that is, a ***vector***. Functions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function $g(t) \\equiv 3 - 2 t$ is $3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)$ &lt;table class=&quot; lightable-minimal&quot; style=&#39;font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;&#39;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align:right;&quot;&gt; t &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; one(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; identity(t) &lt;/th&gt; &lt;th style=&quot;text-align:right;&quot;&gt; g(t) &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 3.0 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 2.8 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.2 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.2 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 2.6 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.3 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.3 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 2.4 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.4 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 0.4 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 2.2 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;... and so on ...&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.6 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.6 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -6.2 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.7 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.7 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -6.4 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.8 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.8 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -6.6 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.9 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 4.9 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -6.8 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 5.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 1 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; 5.0 &lt;/td&gt; &lt;td style=&quot;text-align:right;&quot;&gt; -7.0 &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; The table above is a collection of four vectors: $\\stackrel{\\uparrow}{\\text{t}}$, $\\stackrel{\\uparrow}{\\text{one(t)}$, $\\stackrel{\\uparrow}{\\text{identity(t)}}$, and $\\stackrel{\\uparrow}{\\text{g(t)}}$. Each of those vectors has 51 components. In math-speak, we can say that the vectors are &quot;embedded in a 51-dimensional space.&quot; ## Matrices and linear combinations A collection of vectors, such as the one displayed in the previous table, is called a ***matrix***. Each of the vectors in a matrix must have the same number of components. As mathematical notation, we will use **bold-faced**, capital letters to stand for matrices, for example $\\mathit{M}$. The symbol $\\Uparrow$ is a reminder that a matrix can contain multiple vectors, just as the symbol $\\uparrow$ in $\\vec{v}$ reminds us that the name &quot;$v$&quot; refers to a vector. In the conventions for data, we give a name to each column of a data frame so that we can refer to it individually. In the conventions used in vector mathematics, names are not used to refer to the individual vectors. As a case in point, let&#39;s look at a matrix $\\mathit{M}$ containing the two vectors which we&#39;ve previously called $\\stackrel{\\uparrow}{\\text{one(t)}}$ and $\\stackrel{\\uparrow}{\\text{identity(t)}}$: $$\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 &amp; 0\\\\ 1 &amp; 0.1\\\\ 1 &amp; 0.2\\\\ 1 &amp; 0.3\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; 4.9\\\\ 1 &amp; 5.0\\\\ \\end{array}\\right]\\ .$$ The linear combination which we might previous have called $3\\cdot \\stackrel{\\uparrow}{\\text{one(t)}} - 2\\cdot \\stackrel{\\uparrow}{\\text{identity(t)}}$ can be thought of as $$\\left[\\overbrace{\\begin{array}{r} 1\\\\ 1 \\\\ 1 \\\\ 1 \\\\ \\vdots &amp;\\\\ 1 \\\\ 1 \\end{array}}^{3 \\times} \\stackrel{\\begin{array}{r} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{array}}{\\Large + \\ } \\overbrace{\\begin{array}{r} 0\\\\ 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ \\vdots\\\\ 4.9 \\\\ 5.0 \\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r} \\\\ \\\\ 3\\\\ 2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\ \\end{array}\\right]\\ ,$$ but this is not conventional notation. Instead, we would write this more concisely as $$\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 &amp; 0\\\\ 1 &amp; 0.1\\\\ 1 &amp; 0.2\\\\ 1 &amp; 0.3\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; 4.9\\\\ 1 &amp; 5.0\\\\ \\end{array}\\right]} \\cdot \\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}$$ In symbolic form, the linear combination of the columns of $\\mathit{M}$ using respectively the scalars in $\\vec{w}$ is simply $\\mathit{M} \\cdot \\vec{w}$. This is called ***matrix multiplication***. Naturally, the operation only makes sense if there are as many components to $\\vec{w}$ as there are columns in $\\mathit{M}$. ::: {.takenote data-latex=&quot;&quot;} &quot;Matrix multiplication&quot; might better have been called &quot;$\\mathit{M} linearly combined by $\\vec{w}$.&quot; But &quot;matrix multiplication&quot; is the phrase you will hear in other courses. ::: ::: {.rmosaic data-latex=&quot;&quot;} In R, you can make vectors with the `rbind()` command, short for &quot;bind rows,&quot; as in ```r rbind(2, 5, -3) ## [,1] ## [1,] 2 ## [2,] 5 ## [3,] -3 with the components of the vector presented as successive arguments to the function. One way to make a matrix is with the cbind() command, short for “bind columns”. The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands. u &lt;- rbind(2, 5, -3) v &lt;- rbind(1, -4, 0) A &lt;- cbind(u, v) A ## [,1] [,2] ## [1,] 2 1 ## [2,] 5 -4 ## [3,] -3 0 To compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that’s easy to read: x &lt;- rbind(3, 1) A %*% x ## [,1] ## [1,] 7 ## [2,] 11 ## [3,] -9 It’s a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with: A * x ## Error in A * x: non-conformable arrays The phrase “non-conformable arrays” is R-speak for saying “The arguments to * don’t have compatible shapes for componentwise multiplication.” ::: In chapters to come, we will sometimes make several different linear combinations of the vectors in a matrix. Of course the result of each individual linear combination will be a vector, so the “several different linear combinations” can be thought of as a collection of vectors, that is, a matrix. For example, consider linear combinations of the two vectors in a matrix \\[\\mathit{A} = \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\]: The combinations we have in mind are: \\[ \\mathit{A}\\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]= \\left[\\begin{array}{r}7\\\\11\\\\-9\\end{array}\\right] \\ ,\\ \\ \\mathit{A} \\cdot \\left[\\begin{array}{r}-0\\\\2\\end{array}\\right]= \\left[\\begin{array}{r}2\\\\-8\\\\0\\end{array}\\right] \\ ,\\ \\ \\ \\mathit{A}\\cdot \\left[\\begin{array}{r}-1\\\\0\\end{array}\\right] = \\left[\\begin{array}{r}-2\\\\-5\\\\3\\end{array}\\right] \\] A more concise way to write this collects the vectors with the values for the scalars into a matrix, which we’ll call \\[\\mathit{X} \\equiv \\left[\\begin{array}{rr}3 &amp; 0 &amp; -1\\\\1 &amp; 2 &amp; 0\\end{array}\\right]\\ .\\] \\[\\mathit{A} \\cdot \\mathit{X} = \\left[\\begin{array}{r}7 &amp;2 &amp;-2\\\\11 &amp; -8 &amp; -5\\\\-9 &amp; 0 &amp; 3\\end{array}\\right]\\] ::: {.rmosaic data-latex=\"\"} In R, to create the set of linear combinations, we create the matrices \\(\\mathit{A}\\) and \\(\\mathit{X}\\) and combine them with matrix multiplication. A &lt;- cbind( rbind( 2, 5, -3), rbind( 1, -4, 0) ) X &lt;- cbind( rbind( 3, 1), rbind( 0, 2), rbind(-1, 0) ) A %*% X ## [,1] [,2] [,3] ## [1,] 7 2 -2 ## [2,] 11 -8 -5 ## [3,] -9 0 3 41.8 Sub-spaces Recall that a vector with \\(n\\) components can be said to be embedded in an \\(n\\)-dimensional space. You might like to think of the embedding space as a kind of club with restricted membership. A vector with 2 elements is entitled to join the 2-dimensional club, but a vector with more or fewer than 2 elements cannot be admitted to the club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on. The clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded. Now imagine that the clubhouse can be arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\vec{v}\\) and \\(\\vec{w}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace. A subspace has it’s own rules for admission. New vectors can be in the subspace only if they can be constructed as a linear combination of the sponsoring members. The subspace itself consists of all vectors that are a linear combination of the sponsors. As an example, consider the clubhouse that is open to any and all vectors with two components. The diagram in Figure ?? Now suppose that some of the other vectors decide to sponsor subspaces. The subspace sponsored by \\(\\vec{u}\\) and \\(\\vec{w}\\) contains all the vectors that can be constructed as linear combinations of \\(\\vec{u}\\) and \\(\\vec{w}\\). This subspace is, in fact, the entirety of the 2-dimensional clubhouse. On the other hand, the subspace sponsored by \\(\\vec{w}\\) and \\(\\vec{x}\\) is not the entire clubhouse because \\(\\vec{w}\\) and \\(\\vec{x}\\) are aligned with one another. The \\(\\vec{w}\\) and \\(\\vec{x}\\) subspace can contain only vectors that are aligned in the same way. More mathematically, given one or more vectors with the same number of components, the embedding space is the set of all possible vectors with that number of components. A subspace of the embedding space is defined by a collection of one or more vectors. If the vectors are \\(\\vec{u}\\) and \\(\\vec{w}\\), we speak of the subspace spanned by \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by a matrix is the subspace spanned by the vectors that compose the matrix. A helpful intuition of subspaces can be developed by displaying a few subspaces for vectors with 3 components. As an aid, Figure 41.5 uses 3-D drawing software that allows you to look at the inhabitants of the space from different perspectives. Figure 41.5: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\) embedded in 3-dimensional space. The subspace spanned by each vector, individually, is shown as a line. Notice in Figure 41.5 that the two different subspaces intersect at the origin. There is one, and only one vector in any given dimension that is part of any and all subspaces, regardless of which vectors happen to be the sponsors. This is the vector of all zeros. Why? Because \\(\\vec{\\text{0}}\\) can be constructed by multiplying any of the sponsors by the scalar 0. Things get interesting when we consider not just the subspace spanned by the vectors individually, but the subspace spanned by them jointly. Recalling that the vector \\(\\vec{0}\\) must lie in the subspace (since it is in all subspaces!) any vector in the subspace can be aligned with a plane going through the origin. The interactive Figure ?? shows the subspace as a gray, planar surface. By rotating the figure, you can see how the plane is oriented. (Strictly speaking, we’re showing only the part of the planar subspace near the origin. If we drew the whole subspace, which extends infinitely far from the origin, it would be hard to see how the plane is oriented.) Move the axes so that you are looking down the “barrel” of one of the vectors and you’ll be looking at the subspace edge on, so it will appear as a straight line. Figure 41.6: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown as a gray surface. For a more concrete representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. You can point the whole rigid assembly in any direction you like. The angle between them will remain the same. Place a card on top of the pencils, slipping it between your pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determine the orientation of the surface. This simple fact will be extremely important later on. You could replace the pencils with line segments drawn on the card underneath each pencil. Now you have the angle readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors. Notice that you can also lay a card along a single vector. What’s different here is that you can roll the card around the pencil; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not determine uniquely the orientation of the planar surface in which the two vectors can reside. But with two fixed vectors, there is only one such surface. 41.9 Exercises Exercise XX.XX: 9bAVr2 Locating WW I aircraft The photograph shows part of an aircraft detection system from World War I. The concrete block is an “acoustic mirror.” Its purpose is to collect and reflect sounds from an aircraft, concentrating them at a point where they can be picked up by a microphone. Moving the microphone to a point where the concentrated sound is strongest allows the aircraft’s bearing to be identified, helping observers acquire the aircraft visually. Source With two or more such acoustic mirrors, the location of the aircraft can be identified. Question A Give the position of the aircraft as a multiple of \\(\\vec{a}\\) from sound mirror A and as a multiple of \\(\\vec{b}\\) from sound mirror B. (Choose the closest answer) \\(3 \\vec{a}\\) and \\(4.5 \\vec{b}\\)Correct.  \\(4 \\vec{a}\\) and \\(6 \\vec{b}\\)︎✘ \\(3\\vec{a}\\) and \\(2 \\vec{b}\\)︎✘ \\(4\\vec{a}\\) and \\(4.5\\vec{b}\\)︎✘ Exercise XX.XX: djpDYI ::: {.underconstruction} Navigators are used to specifying a direction as a compass heading: a single number between 0 and 360. For specifying direction on a plane, this is YOU WERE HERE. ASK THEM TO FIND LINEAR COMBINATIONS OF COMPASS DIRECTIONS and translate into a heading. (or vice versa.) ::: Exercise XX.XX: l5UR71 Define the vector \\[\\vec{v} \\equiv \\left[\\begin{array}{r}4\\\\1\\\\3\\\\-2\\end{array}\\right]\\ .\\] Your task is to construct different vectors that are orthogonal to \\(\\vec{v}\\). You can use the trick presented in Section 41.6 of creating templates with zero in all but two of the positions, e.g. \\[\\left[\\begin{array}{r}0\\\\0\\\\\\text{__}\\\\\\text{__}\\end{array}\\right] \\ \\ \\text{or}\\ \\ \\ \\left[\\begin{array}{r}0\\\\\\text{__}\\\\0\\\\\\text{__}\\end{array}\\right] \\ \\ \\text{and}\\ \\ \\ \\left[\\begin{array}{r}\\text{__}\\\\\\text{__}\\\\0\\\\0\\end{array}\\right]\\] To construct a vector orthogonal to \\(\\vec{v}\\), fill in the blanks by taking the corresponding elements of \\(\\vec{v}\\), swapping them, and negating one of them. For example, taking the first template will produce \\[\\left[\\begin{array}{c}0\\\\0\\\\\\underline{\\ 2\\ }\\\\\\underline{\\ 3\\ }\\end{array}\\right]\\ .\\] A. Fill in the blanks of the other two tempates to create vectors orthogonal to \\(\\vec{v}\\). B. Construct 3 new vectors by taking different linear combinations of the vectors you created in (A). Are any of these new vectors orthogonal to \\(\\vec{v}\\)? Show your work. C. Prove algebraically that any linear combination of a set of vectors orthogonal to \\(\\vec{x}\\) will itself be orthogonal to \\(\\vec{x}\\). Exercise XX.XX: K8hC6q ::: {.underconstruction} This will be an exercise about translating compass directions into linear combinations. Some initial ramblings … Pirates and other mariners use direction terms like “one point north of north-north-east.” Their maps are annotated with compass roses that translate the words into a direction. Mathematicians can replace a compass rose with just two vectors, say, \\(\\overset{\\longrightarrow}{\\text{North}}\\) and \\(\\overset{\\longrightarrow}{\\text{East}}\\). Other directions can be given as a linear combination. For instance, the compass rose’s “north-north-west” is the linear combination \\(0.9239\\,\\overset{\\longrightarrow}{\\text{North}} -0.3827\\,\\overset{\\longrightarrow}{\\text{East}}\\). ::: Exercise XX.XX: dIobrt Each of the diagrams below consists of two vectors and a target point (denoted with a bull’s eye). Being a point, the target has a definite location relative to the coordinate axes. “Solving” this sort of system amounts to finding a linear combination of the two vectors whose result is a vector that can connect the origin to the target. (We call this a target problem.) Copy over each diagram to your paper. (Any reasonable approximation will do.) Then, for each diagram, find the linear combination of the two vectors that will reach from the origin to the target. Show your work, meaning that you should draw the scaled vectors in a proper position that demonstrates that they do indeed connect the origin to the target. Underneath the diagram, write down the numerical value of the scalars in the linear combination. (Again, any reasonable approximation will do.) Exercise XX.XX: DPY4Ue In this exercise, you are going to use R to find solutions to the target problem. Each question poses one target problem. You could solve these problems by eye, but we want to get you started on the computer solution so that you’ll be ready to solve harder problems that must be worked on the computer. The vectors you will be working with are: \\[\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\] To make your life easier, here are the commands for defining these vectors. One of the commands is wrong. You’ll have to correct it before moving on the the rest of the problem. NEED TO BREAK ONE OF THESE a &lt;- rbind(1, 2) b &lt;- rbind(1, 1) c &lt;- rbind(1, -1) d &lt;- rbind(-6, 2) T &lt;- rbind(3, -1) Question A What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) to reach the target $ ec{T}? \\(-4 \\vec{a} + 7 \\vec{b}\\)Right!  \\(-2 \\vec{a} + 5 \\vec{b}\\)︎✘ \\(2 \\vec{a} -7 \\vec{b}\\)︎✘ \\(4 \\vec{a} - 5 \\vec{b}\\)︎✘ Question B What what is the correct linear combination of \\(\\vec{b}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{3}\\vec{b} + \\frac{4}{3} \\vec{c}\\)Excellent!  \\(\\frac{4}{3}\\vec{b} + \\frac{7}{2} \\vec{c}\\)︎✘ \\(\\frac{5}{2}\\vec{b} + \\frac{4}{5} \\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{9}{4} \\vec{c}\\)︎✘ Question C What what is the correct linear combination of \\(\\vec{c}\\) and \\(\\vec{d}\\) to reach the target $ ec{T}? \\(\\frac{0}{3}\\vec{c} + \\frac{4}{5} \\vec{d}\\)Good.  \\(\\frac{1}{3}\\vec{c} + \\frac{1}{3} \\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{c} + \\frac{5}{7} \\vec{d}\\)︎✘ \\(\\frac{0}{3}\\vec{c} + \\frac{1}{2} \\vec{d}\\)︎✘ Question D What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)Right!  \\(\\frac{5}{4}\\vec{a} + \\frac{5}{4} \\vec{c}\\)︎✘ \\(\\frac{7}{4}\\vec{a} + \\frac{9}{4} \\vec{c}\\)︎✘ \\(\\frac{3}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)︎✘ "],["projection-residual.html", "Chapter 42 Projection &amp; residual 42.1 Projection terminology 42.2 Projection onto a single vector 42.3 Projection onto a set of vectors 42.4 Exercises", " Chapter 42 Projection &amp; residual Many problems in physics and engineering involve the task of decomposing a vector \\(\\vec{b}\\) into two perpendicular component vectors \\(\\hat{b}\\) and \\(\\vec{r}\\), such that \\(\\hat{b} + \\vec{r} = \\vec{b}\\) and \\(\\hat{b} \\cdot \\vec{r} = 0\\). There is an infinite number of ways to accomplish such a decomposition, one for each way or orienting \\(\\hat{b}\\) relative to \\(\\vec{b}\\). Figure 42.1 shows a few examples. This is the first time that we are encountering a symbol like \\(\\hat{b}\\), pronounced “b-hat.” You will see it especially in statistics and machine learning. Figure 42.1: A few ways of decomposing \\(\\vec{b}\\) into perpendicular components \\(\\hat{b}\\) and \\(\\vec{r}\\) Example 42.1 Gravitational force, as you know, always points downward. The effective acceleration due to gravity of a mass depends, however, on how that mass is situated with respect to other elements of the structure. The figure below shows several diagrams that might well be found on the pages of a physics textbook. In each diagram, there is a mass and a constraining structure: a ramp, a pendulum, an inclined plane. The force of gravity on the mass always points directly downward. In each diagram, \\(\\hat{b}\\) is the effective gravitational force on the mass, pointing down the ramp, or perpendicular to the pendulum strut, or aligned with the gradient vector of the inclined plane. The \\(\\vec{r}\\) in each diagram gives the component of gravitational force that will be counter-acted by the structure: the pull downward into the ramp, the pull along the pendulum strut, or the pull into the inclined plane. The task of decomposition is important also outside of physics and engineering. Our particular interest will be in finding how best to take a linear combination of the columns of a matrix \\(\\mathit{A}\\) in order to make the best approximation to a given vector \\(\\vec{b}\\). This problem solves all sorts of problems: finding a linear combination of functions to match a relationship laid out in data, constructing statistical models such as those found in machine learning, effortlessly solving sets of simultaneous linear equations with any number of equations and any number of unknowns. 42.1 Projection terminology The problem of decomposition can be considered to be a special case of projection. The word “projection” may bring to mind the casting of shadows on a screen in the same manner as an old-fashioned slide projector or movie projector. The light source is arranged to generate parallel rays which arrive perpendicularly to the screen. A movie screen is two-dimensional, a subspace defined by two vectors. Imagining those two vectors to be collected into matrix \\(\\mathit{A}\\), the idea is to decompose \\(\\vec{b}\\) into a component that lies in the subspace defined by \\(\\mathit{A}\\) and another component that is perpendicular to the screen. That perpendicular component is what we have been calling \\(\\vec{r}\\) while the vector \\(\\hat{b}\\) is the projection of \\(\\vec{b}\\) onto the screen. To make it easier to keep track of the various roles played by \\(\\vec{b}\\), \\(\\hat{b}\\), \\(\\vec{r}\\) and \\(\\mathit{A}\\), we’ll give these vectors English-language names. The motivation for these names will become apparent in later chapters, but for now, here they are. You will want to memorize them. \\(\\vec{b}\\) the target vector \\(\\hat{b}\\) the model vector \\(\\vec{r}\\) the residual vector \\(\\mathit{A}\\) the model space (or “model subspace”) Projection is the process of finding the model vector that is as close as possible to the target vector \\(\\vec{b}\\). Another way to see this is as finding the model vector that makes the residual vector as short as possible. Example 42.2 Figure 42.2 shows a a solved projection problem in 3-dimensional space. The figure can be rotated or set spinning, which makes it much easier to interpret the diagram as a three dimensional object. In addition to \\(\\vec{b}\\) and the vectors \\(\\vec{u}\\) and \\(\\vec{b}\\) that constitute the matrix \\(\\mathit{A}\\), the diagram includes a translucent plane marking \\(span{\\mathit{A}\\). The goal of projection is, from these givens, to find the model vector (shown in light green). Once the model vector \\(\\vec{x}\\) is known, the residual vector is easy to calculate \\[\\vec{r} \\equiv \\vec{b} - \\hat{b}\\ .\\] Another approach to the problem is to find the residual vector \\(r\\) first, then use that to find the model vector as \\[\\hat{b} \\equiv \\vec{b} - \\vec{r}\\ .\\] Figure 42.2: A three-dimensional diagram showing the target vector $ ec{b}$ and the vectors $ ec{u}$ and $ ec{v}$. The subspace spanned by $ ec{u}$ and $ ec{v}$ is indicated with a translucent plane. The model vector (green) is the result produced in solving the projection problem. Interpreting such three dimensional diagrams can be difficult. But there are tricks involving watching the diagram as it is rotated. For instance, how do we know that the translucent plane in Figure 42.2 contains \\(\\vec{u}\\) and \\(\\vec{v}\\)? As the diagram rotates, from time to time you will be looking edge on at the plane, so that the plane appears as a line on the screen. At such times, you can see that vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) disappear. There is no component to \\(\\vec{u}\\) and \\(\\vec{v}\\) that sticks out from the plane. 42.2 Projection onto a single vector As we said, projection involves a vector \\(\\vec{b}\\) and a matrix \\(\\mathit{A}\\) that defines the model space. We’ll start with the simplest case, where \\(\\mathit{A}\\) has only one column. That column is, of course, a vector. We’ll call that vector \\(\\vec{a}\\), so the projection problem is to project \\(\\vec{b}\\) onto the subspace spanned by \\(\\vec{a}\\). Geometrically, the situation of projecting the target vector \\(\\vec{b}\\) onto the model space \\(\\vec{a}\\) is diagrammed in Figure @ref{fig:b-onto-a}. Figure 42.3: The geometry of projecting \\(\\vec{b}\\) onto \\(\\vec{a}\\) to produce the model vector \\(\\hat{b}\\). The angle between \\(\\vec{a}\\) and \\(\\vec{b}\\) is labelled \\(\\theta\\). You already know how to calculate \\(\\theta\\) from \\(\\vec{b}\\) and \\(\\vec{a}\\) by using the dot product: \\[\\cos(\\theta) = \\frac{\\vec{b} \\bullet \\vec{a}}{\\len{b}\\, \\len{a}}\\ .\\] Knowing \\(\\theta\\) and \\(\\len{b}\\), you can calculate the length of the model vector \\(\\hat{b}\\): \\[\\len{s} = \\len{b} \\cos(\\theta) = \\vec{b} \\bullet \\vec{a} / \\len{a}\\ .\\] Scaling \\(\\vec{a}\\) by \\(\\len{a}\\) would produce a vector oriented in the model subspace, but it would have the wrong length: length \\(\\len{a} \\len{s}\\). So we need to divide \\(\\vec{a}\\) by \\(\\len{a}\\) to get a unit length vector oriented along \\(\\vec{a}\\): \\[\\text{model vector:}\\ \\ \\hat{b} = \\left[\\vec{b} \\bullet \\vec{a}\\right] \\,\\vec{a} / {\\len{a}^2} = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\ \\vec{a}.\\] . In R/mosaic, you can calculate the projection of \\(\\vec{b}\\) onto \\(\\vec{a}\\) using %onto%. For instance b &lt;- rbind(-1, 2) a &lt;- rbind(-2.5, -0.8) s &lt;- b %onto% a s ## [,1] ## [1,] -0.3265602 ## [2,] -0.1044993 Having found \\(\\hat{b}\\), the residual vector \\(\\vec{r}\\) can be calculated as \\(\\vec{b}- \\hat{b}\\). r &lt;- b - s r ## [,1] ## [1,] -0.6734398 ## [2,] 2.1044993 The two properties that a projection satisfies are: The residual vector is perpendicular to each and every vector in \\(\\mathit{A}\\). Since in this example, \\(\\mathit{A}\\) contains only the one vector \\(\\vec{a}\\), we need only look at \\(\\vec{r} \\cdot \\vec{a}\\) and confirm that it’s zero. r %dot% a ## [1] -2.220446e-16 The residual vector plus the model vector exactly equal the target vector. Since we computed r &lt;- b - s, we know this must be true, but still … (r+s) - b ## [,1] ## [1,] 0 ## [2,] 0 If the difference between two vectors is zero for every coordinate, the two vectors must be identical. 42.3 Projection onto a set of vectors As we have just seen, projecting a target \\(\\vec{b}\\) onto a single vector is a matter of arithmetic. Now we will expand the technique to project the target vector \\(\\vec{b}\\) onto multiple vectors collected into a matrix \\(\\mathit{b}\\). Whereas in the chapter we used trigonometry to find the component of \\(\\vec{b}\\) aligned with the single vector \\(\\vec{a}\\), now we have to deal with multiple vectors at the same time. The result will be the component of \\(\\vec{b}\\) aligned with the subspace sponsored by \\(\\mathit{A}\\). There is one situation where the projection is easy: when the vectors in \\(\\mathit{A}\\) are mutually orthogonal. In this situation, carry out several one vector at a time projections: \\[\\vec{p_1} = modeledby{\\vec{b}}{\\vec{v_1}}\\\\ \\vec{p_2} = modeledby{\\vec{b}}{\\vec{v_2}}\\\\ \\vec{p_3} = modeledby{\\vec{b}}{\\vec{v_3}}\\\\ \\text{and so on}\\] The projection of \\(\\vec{b}\\) onto \\(\\mathit{A}\\) will be the sum \\(\\vec{p_1} + \\vec{p2} + \\vec{p3}\\). Example 42.3 To illustrate the method of projection when the vectors in \\(\\mathit{A}\\) are mutually orthogonal, we can construct such a matrix. b &lt;- rbind( 1, 1, 1, 1) v1 &lt;- rbind( 1, 2, 0, 0) v2 &lt;- rbind(-2, 1, 3, 1) v3 &lt;- rbind( 0, 0, -1, 3) A &lt;- cbind(v1, v2, v3) You can verify using a dot product that v1, v2, and v3 are mutually orthogonal. Now construct the one-at-a-time projections: p1 &lt;- b %onto% v1 p2 &lt;- b %onto% v2 p3 &lt;- b %onto% v3 To find the projection of \\(\\vec{b}\\) onto the subspace spanned by \\(\\mathit{A}\\), add up the one-at-a-time projections: b_on_A &lt;- p1 + p2 + p3 Now we’ll confirm that b_on_A really is the projection of b onto A. The strategy is to construct the residual from the projection. resid &lt;- b - b_on_A All that’s needed is to confirm that the residual is perpendicular to each and every vector in A: resid %dot% v1 ## [1] 7.771561e-16 resid %dot% v2 ## [1] -2.220446e-16 resid %dot% v3 ## [1] 6.661338e-16 Now that we have a satisfactory method for projecting \\(\\vec{b}\\) onto a matrix \\(\\mathit{A}\\) consisting of mutually orthogonal vectors, we need to develop a method for the projection when the vectors in \\(\\mathit{A}\\) are not mutually orthogonal. The big picture here is that we will construct a new matrix \\(\\mathit{Q}\\) that spans the same space as \\(\\mathit{A}\\) but whose vectors are mutually orthogonal. We’ll construct \\(\\mathit{Q}\\) out of linear combinations of the vectors in \\(\\mathit{A}\\), so we can be sure that \\(span(\\mathit{Q}) = span(\\mathit{A})\\). We introduce the process with an example, involving a vectors in a 4-dimensional space. \\(\\mathit{A}\\) will be a matrix with two columns, \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\). Here’s the setup for the example vectors and model matrix: b &lt;- rbind(1,1,1,1) v1 &lt;- rbind(2,3,4,5) v2 &lt;- rbind(-4,2,4,1) A &lt;- cbind(v1, v2) We set out to find the residual \\(\\vec{r}\\) first. Then calculating the model vector will be easy: \\(\\hat{b} =\\vec{b} - \\vec{r}\\). The first step in the process is to calculate the residual from projecting \\(\\vec{b}\\) onto the first vector in \\(\\mathit{A}\\). This will be a matter of projecting \\(\\vec{b}\\) onto \\(\\vec{v}\\) and finding the residual, that is, b - (b %onto% v1). We’re going to be using this sort of residual-finding via projection a lot, so for convenience, R/mosaic packages up residual finding into an operator named %perp%. Here’s the residual from the first projection: r1 &lt;- b %perp% v1 The basic idea of the algorithm is to take the residual r1 from the first projection and use that as the input to the next projection. For vectors with more than two columns, take the residual r2 from the second projection and use that as the input to the third projection, and so on. But there’s a catch as you can see by carrying out this algorithm. r1 &lt;- b %perp% v1 r2 &lt;- r1 %perp% v2 The hoped-for result is that \\(\\vec{r2}\\) will be perpendicular to every vector in \\(span(\\mathit{A})\\). We can use a dot project to check this out. r2 %dot% v1 ## [1] 0.988989 If r2 were perpendicular to v1 the dot product would be zero. It’s not. What went wrong? The first projection produces r1 which will be some linear combination of b and v1. The second projection produces r2 which will be some linear combination of r1 and v2. Certainly r1 is perpendicular to v1, but v2 is not. So a linear combination of r1 and v2 will not be perpendicular to v1. Suppose, however, that we replaced v2 with another vector w2 that is perpendicular to v1 and lies in the v1-v2 subspace. This new vector will be the residual from projecting v2 onto v1. Since w2 is a linear combination of v1 and v2, it must be in the v1-v2 subspace. Now r2 will be a linear combination of r1 and w2, both of which are perpendicular to v1. So r2 will be perpendicular to both v1 and w2, that is, r2 will be perpendicular to the v1-w2 subspace which is the same as the v1-v2 subspace. The correct procedure looks like this: r1 &lt;- b %perp% v1 w2 &lt;- v2 %perp% v1 # both r1 and w2 are perpendicular to v1 r2 &lt;- r1 %perp% w2 Again, we can use the dot product of r2 versus v1 and v2 to confirm that r2 is perpendicular to both. r2 %dot% v1 ## [1] 8.881784e-16 r2 %dot% v2 ## [1] 2.220446e-16 This process can be extended to any number of vectors in \\(\\mathit{A}\\). Here’s the algorithm for constructing \\(\\mathit{Q}\\): Take the first vector from \\(\\mathit{A}\\) and call it \\(\\vec{q_1}\\). Take the second vector from \\(\\mathit{A}\\) and find the residual from projecting it onto \\(\\vec{q_1}\\). This residual will be \\(\\vec{q_2}\\). At this point, the matrix \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\) consists of mutually orthogonal vectors. Take the third vector from \\(\\mathit{A}\\) and project it onto \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\). We can do this because we already have an algorithm for projecting a vector onto a matrix with mutually orthogonal columns. Call the residual from this projection \\(\\mathit{q_3}\\). It will be orthogonal to the vectors in \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\), so all three of the q vectors we’ve created are mutually orthogonal. Continue onward, taking the next vector in \\(\\mathit{A}\\), projecting it onto the q-vectors already assembled, and finding the residual from that projection. Repeat step (iv) until all the vectors in \\(\\mathit{A}\\) have been handled. Example 42.4 Project a \\(\\vec{b}\\) that lives in 10-dimensional space onto the subspace sponsored by five vectors that are not mutually orthgonal: b &lt;- rbind(3,2,7,3,-6,4,1,-1, 8, 2) # or any set of 10 numbers v1 &lt;- rbind(4, 7, 1, 0, 3, 0, 6, 1, 1, 2) v2 &lt;- rbind(8, 8, 4, -3, 3, -2, -4, 9, 6, 0) v3 &lt;- rbind(12, 0, 4, -2, -6, -4, -1, 4, 6, -7) v4 &lt;- rbind(0, 3, 9, 6, -4, -5, 4, 0, 5, -4) v5 &lt;- rbind(-2, 5, -4, 8, -9, 3, -5, 0, 11, -4) A &lt;- cbind(v1, v2, v3, v4, v5) You can confirm using dot products that the v-vectors are not mutually orthogonal. Now to construct the vectors in \\(\\mathit{Q}\\). q1 &lt;- v1 q2 &lt;- v2 %perp% q1 q3 &lt;- v3 %perp% cbind(q1, q2) q4 &lt;- v4 %perp% cbind(q1, q2, q3) q5 &lt;- v5 %perp% cbind(q1, q2, q3, q4) Q &lt;- cbind(q1, q2, q3, q4, q5) Since Q consists of mutually orthogonal vectors, the projection of b onto Q can be done one vector at a time. p1 &lt;- b %onto% q1 p2 &lt;- b %onto% q2 p3 &lt;- b %onto% q3 p4 &lt;- b %onto% q4 p5 &lt;- b %onto% q5 # put together the components b_on_A &lt;- p1 + p2 + p3 + p4 + p5 # check the answer: resid should be perpendicular to A resid &lt;- b - b_on_A resid %dot% v1 ## [1] 1.065814e-14 resid %dot% v2 ## [1] 4.973799e-14 resid %dot% v3 ## [1] 7.105427e-15 resid %dot% v4 ## [1] 0 resid %dot% v5 ## [1] 1.776357e-14 42.4 Exercises Exercise XX.XX: wniqMc Refer to the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), and \\(\\vec{d}\\) in the figure. Carry out the following projections graphically. You should show not only the result of the projection, but also the original vector being projected and the original vector being projected onto. Project \\(\\vec{a}\\) onto \\(\\vec{b}\\) Project \\(\\vec{c}\\) onto \\(\\vec{a}\\) Project \\(\\vec{b}\\) onto itself. Go back to your diagrams and add on to each diagram the residual vector from the projection. Make sure to use a different color ink or some other device to distinguish the residuals from the vectors you had already drawn. Exercise XX.XX: yeY17y Here is a direction, \\(\\vec{\\mathbf D}\\): \\[\\vec{\\mathbf D} \\equiv \\left(\\begin{array}{c}2\\\\5\\end{array}\\right)\\] Question A Find the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.59 \\vec{\\mathbf D}\\)Good.  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question B Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(0.83, 2.07\\right)\\)Nice!  \\(\\vec{\\mathbf R} = \\left(0.31, 1.28\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(1.28, 0.31\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.83, 0.31\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Question C Find the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.379 \\vec{\\mathbf D}\\)Correct.  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question D Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)^T\\)Excellent!  \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Exercise XX.XX: xSoxzL The text gives a formula for the scalar multiplier \\(\\alpha\\) such that \\(\\alpha \\vec{a} = \\modeledby{\\vec{b}}{\\vec{a}}\\): \\[\\alpha = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\ .\\] For a &lt;- rbind(5, -2, 3, 7) calculate the scalar multiplier \\(\\alpha\\) for each of these \\(\\vec{b}\\) vectors. b1 &lt;- rbind(1, 0, 0, 0) b2 &lt;- rbind(0, 1, 0, 0) b3 &lt;- rbind(0, 1, 2, 3) b4 &lt;- rbind(-4, 1, 2, 3) Exercise XX.XX: zK7fQn Construct a random 4-by-4 matrix named A whose columns are mutually orthogonal. Here’s the process: Construct a vector with four elements that has random elements. A command to do so is v1 &lt;- cbind(rnorm(4)) This will be the first column of the matrix. Construct a new vector with random elements. w &lt;- cbind(rnorm(4)) w will not be orthogonal to v1 as you can confirm by calculating the dot product between v1 and w. However, you can construct from the two vectors a new one that will be perpendicular to v1. v2 &lt;- w - (w %onto% v1) Take the mutually orthogonal vectors you already have and package them into a matrix M. Then construct a new random vector w and project it onto M. ```r M &lt;- cbind(v1, v2) w &lt;- cbind(rnorm(4)) v3 &lt;- w - (w %onto% M) ``` Continue the process of step (iii) until you have 4 mutually orthogonal vectors, and collect them into the matrix `A`. Use dot products to verify that v1, v2, v3, and v4 are mutually orthogonal. Exercise XX.XX: vUbXdq The previous exercise showed how to create mutually orthogonal columns. Generate four such mutually orthogonal vectors: v1 through v4. Create a target vector b with the same dimension as the v_ vectors. b can point in any direction whatsoever, your choice! Using %onto%, calculate the vector projection ofbonto the matrixA &lt;- cbind(v1, v2, v3, v4). The result will be a vector. Call this vectorb_model`. A previous exercise gave the formula for the scalar multiplier \\(\\alpha\\) of a vector \\(\\vec{v}\\) such that \\(\\alpha\\,\\vec{v} = \\modeledby{\\vec{b}}{\\vec{v}}\\): \\[\\alpha = \\frac{\\vec{b} \\bullet \\vec{v}}{\\vec{v} \\bullet \\vec{v}}\\ .\\] Use this formula to calculate the four scalars multipliers that result from projecting respectively b onto v1, b onto v2, and so on. Call these scalars alpha1, alpha2, and so on. Calculate the sum alpha1*v1 + alpha2*v2 + alpha3*v3 + alpha4*v4. Show that the vector found in step (d) matches the vector b_model found in step (b). This is a demonstration that, with mutually orthogonal vectors, finding scalar multipliers for a linear combination of those vectors can be done one vector at a time. Repeat the above, but this time using vectors v1, v2, v3, and v4 that are not mutually orthogonal. One way to generate such vectors is at random: v1 &lt;- cbind(rnorm(4)) and so on. For non-mutually orthogonal vectors, does the one-vector-at-a-time approach produce a match to b %onto% cbind(v1, v2, v3, v4)? Exercises confirming that qr.solve() produces results that are as they should be: residual orthogonal to every vector in A, projected + residual = \\(\\vec{b}\\). Exercises confirming that adding more columns to A produces a smaller residual. Demonstration that even random vectors can be combined to exactly equal \\(\\vec{b}\\), so long as we have enough of them. Exercise XX.XX: p209w8 Here are twelve labeled vectors, A through M. There is a thirteenth vector, labeled “Null vector.” That’s a vector of length zero, so it can’t be drawn as an arrow. Note that the direction of the null vector doesn’t matter, since the vector length is zero. Each of the following statements is of the form, \"\\(\\vec{v}\\) projected onto \\(\\vec{u}\\) gives \\(\\vec{w}\\). Say whether the statement is true or false. Question A \\(\\vec{A}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{K}\\) TrueExcellent!  False︎✘ A projected onto D will be in the direction of D. K is in the direction of D, points in the right direction (downwards), and equals the vertical component of A. Question B \\(\\vec{D}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{L}\\) True︎✘ D projected onto E will be in the direction of E. L is in the direction of E but L does not have the right length. FalseCorrect.  Question C \\(\\vec{J}\\) projected onto \\(\\vec{E}\\) gives the null vector. True︎✘ J and E are not orthogonal. So the projection of one onto the other cannot be the null vector. FalseCorrect.  Question D \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question E \\(\\vec{J}\\) projected onto \\(\\vec{K}\\) gives \\(\\vec{D}\\) True︎✘ J and K are parallel, so projecting J onto K will produce the vector J. But J is much shorter than D. FalseCorrect.  Question F \\(\\vec{C}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{L}\\) True︎✘ C projected onto D will be in the direction of D. L is not in the direction of D. FalseExcellent!  Question G \\(\\vec{L}\\) projected onto \\(\\vec{B}\\) gives the null vector True︎✘ It’s only when vectors are orthogonal that the projection of one onto the other produces the null vector. L and B are not orthogonal. FalseExcellent!  Question H \\(\\vec{E}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{E}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question I \\(\\vec{G}\\) projected onto \\(\\vec{C}\\) gives the null vector.     True\\(\\heartsuit\\ \\)       False︎✘ Question J \\(\\vec{E}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{J}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question K \\(\\vec{A}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{K}\\)     True︎✘ A onto B will be in the direction of B. But K is orthogonal to B.       False\\(\\heartsuit\\ \\) Question L \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question M \\(\\vec{F}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{H}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question N \\(\\vec{C}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{E}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question O \\(\\vec{E}\\) projected onto \\(\\vec{G}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question P \\(\\vec{G}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{C}\\)     True︎✘        False\\(\\heartsuit\\ \\) Exercise XX.XX: dvWdE7 Vectors \\(\\vec{A}, \\vec{B}, \\cdots, \\vec{H}\\) have been defined as A, B, …, H so that you can use them in the sandbox. (To see one of these vectors, give a command in the sandbox consisting of just the vector name.) A &lt;- rbind(2.1, -3.4) B &lt;- rbind(0.3, 4.2) C &lt;- rbind(-2, 3.8) D &lt;- rbind(1.1, -3, 2.2, -1) E &lt;- rbind(6, 4, -2, -1) F &lt;- rbind(0, 3, 2, 1) G &lt;- rbind(1, -2, 1, 1) H &lt;- rbind(-2, 0, 3, 4) Here is the sequence of commands to project vector C onto A and B. M &lt;- cbind(A, B) # Problem setup: Matrix packaging up vectors b &lt;- C # Problem setup: Target vector x &lt;- qr.solve(M, b) # Solve the target problem M %*% x = b for x bhat &lt;- M %*% x # Result: projected vector resid &lt;- b - bhat # Result: residual vector t(A) %*% A # square length of a vector Question A A) Solve \\((\\vec{A}, \\vec{C}) \\cdot \\vec{x} = \\vec{B}\\). What is the length of \\(\\vec{x}\\)?     Exactly 0︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43︎✘        About 12\\(\\heartsuit\\ \\)       Can’t be solved︎✘ Question B B) Solve \\((\\vec{A}) \\cdot \\vec{x} = \\vec{B}\\). What’s the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6︎✘        About 2.5\\(\\heartsuit\\ \\)       About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Question C D) Solve \\((\\vec{D}, \\vec{E}) \\cdot \\vec{x} = \\vec{F}\\). What is the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43\\(\\heartsuit\\ \\)       About 12︎✘        Can’t be solved︎✘ Question D E) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}) \\cdot \\vec{x} = \\vec{G}\\). What is the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6\\(\\heartsuit\\ \\)       About 2.5︎✘        About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Question E F) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}, \\vec{G}) \\cdot \\vec{x} = \\vec{H}\\). What is the length of the residual?     Exactly 0\\(\\heartsuit\\ \\)       About 0.3︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Our goal is to scale the expkt vector so that the scaled numbers will be as close as possible to our destination, namely, temp. Comparing the two columns of numbers, you might anticipate that the scalar will be about 100. We’ll see how to calculate it exactly in the next chapter. The result turns out to be 99.23. The resulting model will be \\[T(t) = 99.23\\, e^{-0.02 t}\\ .\\] How are we to judge whether this is a good model or not? Common sense suggests plotting out the model function along with the data, as in Figure ??. gf_point(temp ~ time, data = CW) %&gt;% slice_plot(99.23*exp(-0.02*time) ~ time, color=&quot;magenta&quot;) Judge for yourself whether this is a good model. The obvious deficiency is that the model falls, as decaying exponentials will do, toward a temperature of 0, whereas the water is cooling to a room temperature of about 25 degrees. Let’s return to the model seen in terms of vectors. The advantage of doing this is to develop a general procedure we can use for interpreting models of all sorts, rather than just the particular situation of the cooling-water data. What are the geometric facts? We know that the temp vector has length 251.3 deg C. Similarly we can calculate the length of the expkt vector: 2.46 deg C. It might seem that the “direction” of the vector is meaningless, because it’s a direction in an abstract, hard-to-envision 15-dimensional space. (There are 15 components to each of temp and expkt.) Even so, we can calculate the angle between the two vectors, using the formula \\(\\cos(\\theta) = \\frac{\\vec{v}\\cdot \\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\). Doing the arithmetic gives \\[\\cos{\\theta} = \\frac{599.8}{251.3 \\times 2.46} = 0.9708\\ \\ \\implies \\ \\ \\ \\theta = 13.88^\\circ\\] Figure 42.4: The vectors temp and expkt have an angle of 13.88 deg between them. Here, expkt has been drawn 10x it’s actual size. With these geometrical facts, we can draw a picture. Figure 42.4 shows temp in black and expkt in magenta. (We’ve drawn it 10 times as long as it really is so that you can see it well.) For the expkt vector to be a good model of temp, we need to scale it so that the result, which must be on the dotted line in the picture, is as close as possible to the tip of temp. You can count off yourself how many expkt steps will bring you close to temp. (Remember to multiply your result by 10, since in the picture we drew expkt ten times longer than its arithmetic length.) One reasonable way to quantify how good a model of temp can be made by a properly scaled version of vector expkt is the angle between them: 13.88 degrees. Likewise, we can scale the vector intercept to make it match temp as well as possible. The angle between intercept and temp works out to be 75.7 degrees; the vectors are not very well aligned. Scaling by 58.2 will bring intercept as close as it is ever going to get to temp, which is not very close at all. The idea of a linear combination is to scale and add multiple vectors. As a very rough start, let’s look at the combination 58.2 intercept + 99.23 expkt, the combination of the two individual models we constructed by vector analogy. CAUTION: The model will be poor. That’s not because the vector analogy is poor but because we still have to work out, as we will in the next two chapters, how properly to work with vectors. # This is broken CW &lt;- CoolingWater CW &lt;- CW %&gt;% mutate(model = 99.23*expkt + 58.2*intercept) # Broken Znotes::and_so_on(CW) %&gt;% kableExtra::kable_minimal() The resulting model is … well, terrible! Figure ?? shows the linear combination gf_point(temp ~ time, data = CW) %&gt;% gf_point(model ~ time, data = CW, color=&quot;magenta&quot;) %&gt;% slice_plot(58.2+ 99.23*exp(-0.02*time) ~ time, color=&quot;magenta&quot;) EXERCISE: Repeat the calculations for the entire CoolingData data frame. Adding vectors. The result is a vector Scaling and adding vectors: a linear combination of vectors. As motivation for this “find \\(\\vec{x}\\)” problem, we refer you to Figure ?? which showed the temperature-vs-time data from the CoolingWater data frame. That figure shows several possible linear combinations of the vectors \\(u(t) \\equiv e^{-0.02 t}\\) (we called this expkt) and \\(v(t) \\equiv 1\\) (we called this intercept). Suppose we seek to find the particular linear combination of \\(u(t)\\) and \\(v(t)\\) that comes as close as possible to the black dots in the figure. That is, we know \\(\\mathit{A}\\): the two columns expkt and intercept from the data frame, and we know \\(\\vec{b}\\): the column temp from the data frame. This sort of problem is extremely common and important throughout quantitative fields of all sorts, from astronomy to zoology, and is one of the foundation techniques in statistics and data science. We’ll present the approach graphically, algorithmically, and computationally. "],["target-problem.html", "Chapter 43 The target problem 43.1 Linear equations 43.2 Visualization in a two-dimensional subspace 43.3 Properties of the solution 43.4 Application of the target problem 43.5 Exercises", " Chapter 43 The target problem Chapter not yet released “In theory there is no difference between theory and practice, while in practice there is.” In this chapter, we ask you to reconsider a mathematical theory that is universally taught in high-school and to consider augmenting it with newer computational ideas that address the same kind of problems, but which produce useful results even when the mathematical theory insists the “a solution does not exist.” The time-honored theory is that taught in high-school algebra. There’s nothing wrong with that theory except that it is incomplete. It doesn’t address the needs of present-day practice, particularly in data science, statistics, and machine learning. Algebra, in its basic sense, is about generalizing arithmetic to handle situations where some quantities are not yet known numerically and so are represented by symbols. The algebra student learns rules for symbolic expressions that allow the expressions to be re-arranged into other forms that would clearly be valid if replaced by numbers. Some examples of these rules: i. \\(ax = b\\) is equivalent to \\(x = a/b\\). ii. \\(a + x = b\\) is equivalent to \\(x=b-a\\). iii. \\(a x^2 + b x + c = 0\\) is equivalent to \\(x = \\frac{-b\\pm \\sqrt{\\strut b^2 - 4ac}}{2a}\\). iv. \\(\\ln(ax) = b\\) is equivalent to \\(x = \\frac{1}{a}\\ln(b)\\). A major challenge to the algebra student is to use such rules to re-arrange expressions into a form \\(x=\\) that enables \\(x\\) to be calculated from the numerical values of the other symbols. Unfortunately, students are given little or no insight to the historical origins of algebra techniques and why they are not necessarily appropriate for all tasks. In English, the word “algebra” is seen as early as 1551. It comes from a book written by the Persian Muhammad ibn Musa al-Khwarizmi (780-850), The Compendious Book on Calculation by Completion and Balancing. The book introduced the use of rules familiar to every algebra student. In the original Arabic, the title includes the word “al-jabr,” meaning “completion” or “rejoining.” According to some sources, the literal meaning of “al-jabr” was resetting and rejoining broken bones. That literal meaning correctly conveys the importance of the subject, but also the pain endured by many algebra students. (Incidentally, the “algorithm” comes from the name of the book’s author: al Khwarizmi. He is a major figure in the history of mathematics.) This history may not be of immediate interest to every reader, but there is a good point to it. The roots of algebra are ancient and developed in an era very different from our own. Today’s student learns algebra in order to facilitate the study and practice of physics, chemistry, statistics, engineering, and other fields. None of these fields existed when algebra was being conceived. That is, the theory was developed before the recognition of the problems and calculations that arise in modern practice. Thus, “in practice, theory and practice are different.” This chapter is about re-expressing some basic algebraic theory in order to align it better to today’s practice. 43.1 Linear equations The focus of interest will be the familiar task \\[\\ \\ \\ \\ \\ \\ \\text{given}\\ \\ a x = b\\,,\\ \\ \\text{find}\\ \\ x\\ .\\] All algebra students learn that \\(x = b/a\\), with the proviso that if \\(a = 0\\), “there is no solution.” A somewhat more advanced algebra task is to work with “simultaneous linear equations,” for example: \\[\\ \\ \\ \\text{given}\\ \\ \\ \\begin{array}{rrrcr} 3 x &amp; + &amp; 2 y &amp; = &amp; 7\\\\ -1&amp;+&amp;y&amp;=&amp;4\\end{array} \\ \\ \\ \\ \\text{find}\\ \\ x\\,\\&amp;\\,y\\ . \\] Solving simultaneous linear equations is hard. It involves more arithmetic than \\(ax = b\\) and requires the student to make good choices how to take linear combinations of the two equations to reduce the problem to two equations, one with \\(x\\) as the only unknown and one with \\(y\\). Also, the “there is no solution” proviso is not easy to state, so you can’t know at a glance whether there is indeed a solution. The simultaneous linear equation problem can be more compactly written using matrix and vector notation. \\[\\begin{array}{rrrcr}3x &amp; + &amp;2y &amp; = &amp; 7\\\\-1&amp;+&amp;y&amp;=&amp;4\\end{array} \\ \\ \\text{is the same as}\\ \\ \\left[\\begin{array}{r}3\\\\-1\\end{array}\\right] x + \\left[\\begin{array}{r}2\\\\1\\end{array}\\right] y = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right] \\] You can see the vector form as a linear combination of two vectors. Collecting these two vectors into a matrix \\(\\mathit{A}\\), and similarly writing \\(x\\, \\text{and}\\, y\\) as the scalar components of a vector \\(\\vec{x}\\) gives \\[\\left[\\begin{array}{rr}3&amp;2\\\\-1&amp;1\\end{array}\\right]\\ \\vec{x} = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right]\\] Which can be expressed as \\(\\mathit{A} \\vec{x} = \\vec{b}\\). A student, recognizing the similarity of \\(\\mathit{A}\\vec{x} = \\vec{b}\\) to \\(a x = b\\) would reasonably suggest the solution \\(\\vec{x} = \\vec{b}/ \\mathit{A}\\). Such a student might be instructed, “No, you can’t do this.” A better response would be, “Good. Now tell me what you mean by \\(\\vec{b}/\\mathit{A}\\)?” Modern practice often calls for solving \\(\\mathit{A}\\vec{x} = \\vec{b}\\) in settings where a traditional algebra teacher might say, as for \\(0 x = b\\) that “there is no solution.” To illustrate such a setting, recall the problems from Block 1 of finding the linear combination of the functions \\(f(\\mathtt{time})=1\\) and \\(g(\\mathtt{time}) = e^{-0.019 \\mathtt{time}}\\) that best matches the CoolingWater data: time temp 0 98.2 1 94.4 2 91.4 … and so on … 220 25.9 221 25.8 We seek scalars \\(C\\) and \\(D\\) such that the function \\(C f(\\mathtt{time}) + D g(\\mathtt{time})\\) gives the best possible match to temp. We can compactly write the problem of finding the best linear combination into matrix form by evaluating \\(f()\\) and \\(g()\\) at the values listed in the time column: \\[\\underbrace{\\left[\\begin{array}{rr}1&amp;1.0000\\\\1&amp;0.9812\\\\1&amp;0.9627\\\\\\vdots\\\\1&amp;0.0153\\\\1&amp;0.0150\\end{array}\\right]}_{\\!\\!\\!\\!\\!\\!\\!\\!{\\large\\mathit{A}} = \\left[\\strut f(\\mathtt{time})\\,,\\ \\ \\ g(\\mathtt{time})\\right]} \\underbrace{\\left[\\begin{array}{r}C\\\\D\\end{array}\\right]}_{\\large\\vec{x}} \\ \\text{is the best match to}\\ \\underbrace{\\left[\\begin{array}{r}\\mathtt{98.2}\\\\\\mathtt{94.4}\\\\\\mathtt{91.4}\\\\\\vdots\\\\\\mathtt{25.9}\\\\\\mathtt{25.8}\\end{array}\\right]}_{\\large\\vec{b}}\\] Regrettably, the classical algebraicists did not propose a rule for “is the best match to.” Replacing “is the best match to” with \\(=\\) is not literally correct since “there is no solution” that makes the equality literally true. We’ll use the term target problem to name the task of finding \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) is the best possible match to \\(\\vec{b}\\). This term is motivated by the idea that \\(\\vec{b}\\) is a target, and we seek to use the resources in \\(\\mathit{A}\\) to get as close as possible to the target: choose \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) falls as closely as possible to the target. To address the practical problem in the notation of algebra theory, people write \\[\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\] where \\(\\vec{r}\\) is a vector specially selected to path up \\(\\mathit{A} \\vec{x} = \\vec{b}\\) so that when the best-matching \\(C\\) and \\(D\\) are found, there will be a literal equality solution to \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\). At first glance, \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\) might seem intractable: How are we to find \\(\\vec{r}\\). The answer is that \\(\\vec{r}\\) will be the solution to the projection problem \\(\\vec{b}\\sim\\mathit{A}\\). When \\(\\vec{r}\\) is selected this way, \\(\\vec{r}\\) will be the shortest possible vector that can do the matching up. In other words, by choosing \\(\\vec{r} = \\vec{b} \\sim \\mathit{A}\\) we are implementing the following definition of “is the best match to”: “the best match is the one with the smallest length \\(\\vec{r}\\).” It’s remarkable that one can find \\(\\vec{r}\\) even without knowing \\(\\vec{x}\\). That’s why we introduced and solved the projection problem before taking on the target problem. The part of the target problem that we have still to figure out is how, given \\(\\vec{r}\\), to find \\(\\vec{x}\\). But even at this point you can see that \\(\\mathit{A}\\vec{x} = \\vec{b} - \\vec{r}\\) must have a solution, since \\(\\vec{b} - \\vec{r}\\) is exactly the model vector \\(\\hat{b}\\) which, as we saw in Chapter 42, must lie in \\(span{\\mathit{A}}\\). 43.2 Visualization in a two-dimensional subspace To help you create a mental model of the geometry of the target problem, we’ll solve it graphically for a two dimensional subspace. That is, we’ll solve \\(\\left[\\vec{u}, \\vec{v}\\right] \\vec{x} = \\vec{b}\\). For simplicity, the vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\) will have two components. This means that there is no need to project \\(\\vec{b}\\) onto the subspace; it’s already there (so long as \\(\\vec{u}\\) and \\(\\vec{v}\\) have different directions. ) You may already have encountered the step (ii) technique in your childhood reading. The problem appears in Robert Louis Stevenson’s famous novel, Treasure Island. The story is about the discovery of a treasure map indicating the location of buried treasure on the eponymous Island. There is a red X on the map labelled “bulk of treasure here,” but that is hardly sufficient to guide the dig for treasure. After all, every buried treasure needs some secret to protect it. On the back of the map is written a cryptic clue to the precise location: Tall tree, Spy-glass shoulder, bearing a point to the N. of N.N.E. Skeleton Island E.S.E. and by E. Ten feet. Skeleton Island is clearly marked on the map, as is Spy-glass Hill. The plateau marked by the red X “was dotted thickly with pine-trees of varying height. Every here and there, one of a different species rose forty or fifty feet clear above its neighbors.” But which of these was the “tall tree” mentioned in the clue? Figure 43.1: The map of Treasure Island. The heading ‘E.S.E. and by E.’ is marked with a solid black line starting at Skeleton Island. The heading ‘N. of N.N.E.’ is marked by dotted lines, one of which is positioned to point at the shoulder of Spy-glass Hill. Where the bearing from Skeleton Island meets the bearing to Spy-glass Hill will be the Tall tree. With your new-found background in vectors, you will no doubt recognize that “N. of N.N.E” is the direction of a vector as is “E.S.E. and by E.” Pirate novels seem always to use the length unit of “pace,” which we’ll use here as well. The target is the shoulder of Spy-glass Hill. Or, in vector terms, \\(\\vec{b}\\) is the vector with Skeleton Island as the tail and the should of Spy-glass Hill as the tip. The vectors are \\(\\vec{u} = \\text{N. of N.N.E.}\\) and \\(\\vec{v} = \\text{E.S.E. and by E.}\\) We need to \\[\\text{solve} \\ \\ \\underbrace{\\left[\\vec{u}, \\vec{v}\\right]}_{\\Large\\strut\\mathit{A}} \\underbrace{\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}_{\\Large\\vec{x}} = \\vec{b}\\ \\ \\text{for}\\ \\ {\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}\\ .\\] Long John Silver, obviously an accomplished mathematician, starts near Skeleton Island, moving on along the vector that keeps Skeleton Island to the compass bearing one point east of east-south-east. While on the march, he keeps a telescope trained on the shoulder of Spy-glass Hill. When that telescope points one point north of north-north-east, they are in the vicinity of a tall tree. That’s the tree matching the clue. The vectors in Treasure Island were perpendicular to one another, that is, mutually orthogonal. The more general situation is that the vectors in \\(\\mathit{A}\\) will be somewhat aligned with one another: not mutually orthogonal. 43.2 illustrates the situation: \\(\\vec{v}\\) is not perpendicular to \\(\\vec{u}\\). The task, still, is to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that will match \\(\\vec{b}\\). The diagram shows the \\(\\vec{u}\\) vector and the subspace aligned with \\(\\vec{u}\\), and similarly for \\(\\vec{v}\\) Figure 43.2: The telescope method of solving projection onto two vectors. The algorithm is based in Long John Silver’s technique. Pick either \\(\\vec{u}\\) or \\(\\vec{v}\\), it doesn’t matter which. In the diagram, we’ve picked \\(\\vec{v}\\). Align your telescope with that vector. Now march along the other vector, \\(\\vec{u}\\), carefully keeping the telescope on the bearing aligned with \\(\\vec{v}\\). From the diagram, you can see that when you’ve marched to \\(\\frac{1}{2} \\vec{u}\\), the telescope does not yet have \\(\\vec{b}\\) in view. Similarly, at \\(1 \\vec{u}\\), the target \\(\\vec{b}\\) isn’t yet visible. Marching a little further, to about \\(1.6 \\vec{u}\\) brings you to the point in the \\(\\vec{u}\\)-subspace where the target falls into view. This tells us that the coefficient on \\(\\vec{u}\\) will be 1.6. To find the coefficient on \\(\\vec{v}\\), you’ll need to march along the line of the telescope, taking steps of size \\(\\|\\vec{v}\\|\\). In the diagram, we’ve marked the march with copies of \\(\\vec{v}\\) to make the counting easier. We’ll need to march opposite the direction of \\(\\vec{v}\\), so the coefficient will be negative. Taking 2.8 steps of size \\(\\|\\vec{v}\\|\\) brings us to the target. Thus: \\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\] To handle vectors in spaces where telescopes are not available, we need an arithmetic algorithm. In R, that algorithm is packaged up as qr.solve(). We will pick this up again the next section. Example 43.1 In 3-dimensional space, visualization of the solution to the target problem is possible, at least for those who have the talent of rotating three-dimensional objects in their head. For the rest of us, a physical model can help; take three pencils labeled \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{b}\\) and bury their tails in a small ball of putty. (Chemistry molecular construction kits are a good alternative.) In case putty, pencils, or a molecular model kit are not available, use the interactive diagram in Figure @ref{fig:b-onto-u-v}. This diagram also includes \\(\\hat{b}\\) and \\(\\vec{r}\\) with the hope that this will guide you into orienting the diagram appropriately to see where the solution comes from. Figure 43.3: Showing the relative orientation of the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\). Drag the image to rotate it. \\(\\vec{u}\\) and \\(\\vec{v}\\) are fixed in length. However, their lengths will appear to change as you rotate the space. This might be called the “gun-barrel” effect; a tube looks very short when you look down it’s longitudinal axis, but looks longer when you look at it from the side. Rotate the space until both \\(\\vec{u}\\) and \\(\\vec{v}\\) reach their maximum apparent length. The viewpoint that accomplishes this is looking downward perpendicularly onto the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. In this orientation, you will be looking down the barrel of the \\(\\vec{r}\\) gun. Vector \\(\\vec{b}\\) is not in that plane, as you can confirm by rotating the plot a bit out of the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Returning to the perspective looking down perpendicularly on the place, you can see how \\(\\vec{b}\\) corresponds to \\(\\hat{b}\\), the point in the plane where the projection of \\(\\vec{b}\\) will fall. To find the scalar multiplier on \\(\\vec{v}\\), rotate the space until the vector \\(\\vec{u}\\) is pointing straight toward you. You’ll see only the arrowhead of \\(\\vec{u}\\). Vectors \\(\\vec{v}\\) and \\(\\hat{b}\\) will appear parallel to each other, but that’s because you are looking at the plane edge on. In this orientation, \\(\\hat{b}\\) will appear just a little longer than \\(\\vec{v}\\), perhaps 1.2 times longer. So 1.2 is the scalar multiplier on \\(\\vec{v}\\). To figure out the scalar multiplier on \\(\\vec{u}\\), follow the same procedure as in the previous paragraph, but looking down the barrel of \\(\\vec{v}\\). From this perspective, \\(\\vec{u}\\) appears longer than \\(\\hat{b}\\); the scalar multiplier on \\(\\vec{u}\\) will be about 0.9. In terms of \\(\\mathit{A} \\vec{x} = \\hat{b}\\), the solution is \\[\\vec{x} = \\left[\\begin{array}{r}0.9\\\\1.2\\end{array}\\right]\\ .\\] 43.3 Properties of the solution As you might expect, there is a known solution to the target problem. We’ll start by using a computer implementation of this solution to demonstrate some simple properties of the solution. As an example, we’ll use three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) in a 5-dimensional space as the “screen” to be projected onto, and another vector \\(\\vec{b}\\) as the object being projected. The matrix \\(\\mathit{A}\\) is: \\[{\\mathbf A} \\equiv \\left[\\strut \\begin{array}{ccc}|&amp;|&amp;|\\\\\\vec{u} &amp; \\vec{v} &amp; \\vec{w}\\\\|&amp;|&amp;|\\end{array}\\right]\\] For the sake of example, we’ll make up some vectors. In your own explorations, you can change them to anything you like. # the three vectors u &lt;- rbind(6, 4, 9, 3, 1) v &lt;- rbind(1, 5,-2, 0, 7) w &lt;- rbind(3,-5, 2, 8, 4) A &lt;- cbind(u, v, w) # the target b &lt;- rbind(8, 2,-5, 7, 0) The operator %onto% model vector and from that we can calculate the residual vector. s &lt;- b %onto% A r &lt;- b - s Those two simple commands constitute a complete solution to the projection problem, where see seek to model vector and the residual vector. In the target problem we want more: How to express \\(\\hat{b}\\) as a linear combination of the columns in \\(\\mathit{A}\\). At the risk of being repetitive, this means finding \\(\\color{magenta}{\\vec{x}}\\) in \\[\\mathit{A}\\ \\color{magenta}{\\large\\vec{x}} = \\vec{b}\\] where \\(\\mathit{A}\\) and \\(\\vec{b}\\) are given. The function qr.solve() finds \\(\\vec{x}\\). x &lt;- qr.solve(A, b) ## [,1] ## [1,] 0.03835171 ## [2,] 0.33478133 ## [3,] 0.48849968 How can we confirm that this really is the solution to the target problem for this set of vectors? Easy! Just multiply \\(\\mathit{A}\\) by the \\(\\vec{x}\\) that we found. The result should be the target vector \\(\\hat{b}\\): A %*% x ## [,1] ## [1,] 2.0303906 ## [2,] -0.6151849 ## [3,] 0.6526021 ## [4,] 4.0230526 ## [5,] 4.3358197 s ## [,1] ## [1,] 2.0303906 ## [2,] -0.6151849 ## [3,] 0.6526021 ## [4,] 4.0230526 ## [5,] 4.3358197 You should add qr.solve() to your computational toolbox of R functions. 43.4 Application of the target problem In Section @ref{linear-equations} we translated into vector/matrix form the problem, originally stated in Block 1, of finding the best linear combination of \\(f(\\mathtt{time}) \\equiv 1\\) and \\(g(\\mathtt{time}) \\equiv e^{-0.019 \\mathtt{time}}\\). Let’s solve that problem now. ::: {.rmosaic data-latex=\"\"} Earlier we introduced rbind() for the purpose of making column vectors, as in rbind(3,7,-1) ## [,1] ## [1,] 3 ## [2,] 7 ## [3,] -1 Now we are going to work with columns of data stored in the CoolingWater data frame. A good way to extract a column from a data frame is using the with() function. For instance, b &lt;- with(CoolingWater, temp) time &lt;- with(CoolingWater, time) A &lt;- cbind(1, exp(-0.019 * time)) head(A) ## [,1] [,2] ## [1,] 1 1.0000000 ## [2,] 1 0.9811794 ## [3,] 1 0.9627129 ## [4,] 1 0.9445941 ## [5,] 1 0.9268162 ## [6,] 1 0.9093729 Notice that cbind() automatically translated 1 into the vector of all ones. We’re all set up to solve the target problem: x &lt;- qr.solve(A, b) ## [1] 25.92024 61.26398 How good an answer is the x calculated by qr.solve()? Judge for yourself! gf_point(temp ~ time, data = CoolingWater, size=0) %&gt;% slice_plot(25.92 + 61.26*exp(-0.019*time) ~ time, color=&quot;blue&quot;) You may recall from Block 1 the explanation for the poor match between the model and the data for early times: that the water cooled quickly when poured into the cool mug, but the mug-with-water cooled much slower into the room air. Let’s augment the model by adding another vector with a much faster exponential cooling, say, \\(e^{-0.06 \\mathtt{time}}\\). newA &lt;- cbind(A, exp(-0.06*time)) qr.solve(newA, b) ## [1] 26.82297 53.27832 12.67486 gf_point(temp ~ time, data = CoolingWater, size=0) %&gt;% slice_plot(26.82 + 53.28*exp(-0.019*time) + 12.67*exp(-0.06*time) ~ time, color=&quot;green&quot;) 43.5 Exercises "],["statistical-modeling.html", "Chapter 44 Statistical modeling 44.1 R-squared 44.2 Interpreting the model 44.3 Improving the model 44.4 Exercises", " Chapter 44 Statistical modeling So far, we’ve been looking at linear combinations from a distinctively mathematical point of view: vectors, collections of vectors (matrices), projection, angles and orthogonality. We’ve show a few applications of the techniques for working with linear combinations, but have always expressed those techniques using mathematical terminology. In this Chapter, we will take a detour to get a sense of the perspective and terminology of another field: statistics. In the quantitative world, including fields such as biology and genetics, the social sciences, business decision making, etc. there are far more people working with linear combinations with a statistical eye than there are people working with the mathematical form of notation. Statistics is a far wider field than linear combination, so this chapter is not an attempt to replace the need to study statistics and data science. The purpose is merely to show you how a mathematical process can be used as part of a broader framework to provide useful information to decision-makers. Since a statistics and data-science courses are part of a complete quantitative education, we want to point out from the beginning what you are likely to experience a traditional introductory statistics course and why that will seem largely disconnected from what you see here. Statistics did not start out as a branch of mathematics, although people trained as mathematicians played a central role in it’s development. The story is complicated, but a simplification faithful to that history is to see statistics as an extension of biology. Statistics emerged in the last couple of decades of the 1800s. Possibly the key motivation was to understand genetics and Darwinian evolution. Today we know much about DNA sequences, RNA, amino acids, proteins, and so on. But quantitative genetics started in complete ignorance of any physical mechanism for genetics. Instead, mathematical models were the basis for the theory of genetics, beginning with with Gregor Mendel’s work on heritable traits published in 1865.1 Perhaps coincidentally—or perhaps not—Charles Darwin (1809-1882) was a half-cousin of Francis Galton (1822-1911). Traditional introductory statistics is based, more or less, on the work Galton and his contemporaries, for instance Karl Pearson (1857-1936) and William Sealy Gosset (1876-1937). The terms that you encounter in introductory statistics—correlation coefficient, regression, chi-squared test, t-test—where in place by 1910. Ronald Fisher (1890-1962), also a geneticist, extended this work based in part on the mathematical ideas of projection. Fisher invented analysis of variance (ANOVA) and maximum likelihood estimation and is perhaps the “great man” of statistics. (See Section 35.4 for an introduction to likelihood. We’ll briefly touch on ANOVA in this chapter.) To his historical discredit, Fisher was a leader in eugenics. He was also a major obstacle, to the statistical acceptance of the dangers of smoking. Most of the many techniques covered in traditional introductory statistics are, in fact, manifestations of the application of a general technique: the target problem. They are not taught this way, partly out of hide-bound tradition and partly because the target problem has been covered, if at all, in the fourth or fifth semester of a traditional calculus sequence. It often happens that a model is needed to help organize complex, multivariate data for purposes such as prediction. As a case in point, consider the data available in the Body_fat data frame, which consists of measurements of characteristics such as height, weight, chest circumference, and body fat on 252 men. Znotes::and_so_on(Body_fat) %&gt;% kableExtra::kable_minimal() bodyfat age weight height neck chest abdomen hip thigh knee ankle biceps forearm wrist 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 37.3 21.9 32.0 27.4 17.1 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 37.3 23.4 30.5 28.9 18.2 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 38.9 24.0 28.8 25.2 16.6 … and so on … 26.0 72 190.75 70.5 38.9 108.3 101.3 97.8 56.0 41.6 22.7 30.5 29.4 19.8 31.9 74 207.50 70.0 40.8 112.4 108.5 107.1 59.3 42.2 24.6 33.7 30.0 20.9 Body fat, the percentage of total body mass consisting of fat, is thought by some to be a good measure of general fitness. To what extent this theory is merely a reflection of general societal attitudes toward body shape is unknown. Whatever its actual utility, body fat is hard to measure directly; it involves submerging a person in water to measure total body volume, then calculating the persons mass density and converting this to a reading of body-mass percentage. For those who would like to see body fat used more broadly as a measure of health and fitness, this elaborate procedure stands in the way. And so they seek easier ways to estimate body fat along the lines of the Body Mass Index (BMI), which is a simple arithmetic combination of easily measured height and weight. (Note that BMI is also controversial as anything other than a rough description of body shape. In particular, the label “overweight,” officially \\(25 \\leq \\text{BMI}\\leq 30\\) has at best little connection to actual health.) How can we construct a model, based on the available data, of body fat as a function of the easy-to-measure characteristics such as height and weight? You can anticipate that this will be a matter of applying what we know about the target problem \\[\\text{Given}\\ \\mathit{A}\\ \\text{and}\\ \\vec{b}\\text{, solve } \\mathit{A} \\vec{x} = \\vec{b}\\ \\text{for}\\ \\vec{x}\\]where \\(\\vec{b}\\) is the column of body-mass measurements and \\(\\mathit{A}\\) is the matrix of all the other columns in the data frame. In statistics, the target \\(\\vec{b}\\) is called the response variable and \\(\\mathit{A}\\) is the set of explanatory variables. You can also think of the response variable as the output of the model we will build and the explanatory variables as the inputs to that model. Although application of the target problem is an essential part of constructing a statistical model, it is far from the only part. For instance, statisticians find is useful to think about “how much” of the response variable is explained by the explanatory variables. Measuring this requires a definition for “how much.” In defining “how much,” statisticians focus not on how much variation there is among the values in the response variable. The standard way to measure this is with the variance, which was introduce in Section 41.4 and can be thought of as the average of pair-wise differences among the elements in \\(\\vec{b}\\). In order to support this focus on the variance of \\(\\vec{b}\\), statisticians typically augment \\(\\vec{A}\\) with a column of ones, which they call the intercept. To move forward, we’re going to extract the response variable from the data and construct \\(\\vec{A}\\), adding in the vector of ones. We’ll show the vector/matrix commands for doing this, but you don’t have to remember them because statisticians have a more user-friendly interface to the calculations. b &lt;- with(Body_fat, cbind(bodyfat)) A &lt;- Body_fat[-1] %&gt;% as.matrix A &lt;- cbind(1, A) x &lt;- qr.solve(A, b) The -1 in the second command is a directive to get rid of column 1, which happens to the the response variable, in constructing \\(\\vec{A}\\). Having applied qr.solve(), \\(\\vec{x}\\) now contains the coefficients on the “best” linear combination of the columns in \\(\\vec{A}\\). One of the ways in which the R language is designed to support statistics, is that it keeps track of names of columns, so the elements of \\(\\vec{x}\\) are labelled with the name of the column the element applies to. x ## bodyfat ## -18.18848508 ## age 0.06207865 ## weight -0.08844468 ## height -0.06959043 ## neck -0.47060001 ## chest -0.02386415 ## abdomen 0.95477346 ## hip -0.20754112 ## thigh 0.23609984 ## knee 0.01528121 ## ankle 0.17399537 ## biceps 0.18160242 ## forearm 0.45202491 ## wrist -1.62063910 Based on the above, the model for body fat as a function of the explanatory variables is: \\[\\text{body fat} = -18.2 + 0.062\\, \\mathtt{age} - 0.0884\\, \\mathtt{weight} - 0.696\\, \\mathtt{height} + \\text{an so on}\\ .\\] Once we have the means to solve the target problem, as we do with qr.solve(), constructing such a model is straightforward. But some questions remain: - How good is the model? - Could we make a better model? To answer these questions, we’ll have to develop a measure of how good the model is. And this depends on the purpose for which we’ll be using the model. 44.1 R-squared One of the most basic indices of the quality of a model—but far from the only one!—is a statistic written R2 and pronounced “R-squared.” R2 is quite simple. Remember that a basic statistical question is how much variation there is in the response variable. That’s measured with the variance: var(b) ## bodyfat ## bodyfat 70.03582 We can also look at the variation in the model vector, \\(\\hat{b}\\). bhat &lt;- A %*% x var(bhat) ## bodyfat ## bodyfat 52.46033 R2 is simply the ratio of these two variances: var(bhat) / var(b) ## bodyfat ## bodyfat 0.74905 This result, 74.9%, is interpreted as the fraction of the variance in the response variable that is accounted for by the model. Near synonyms for “accounted for” is explained by or can be attributed to. In the same spirit, we can ask how much of the variance in the response variable is unexplained by, or unaccounted by the explanatory variables. To answer this, look at the size of the residual: var(b - bhat) / var(b) ## bodyfat ## bodyfat 0.25095 Notice that the amount of variance explained, 74.905%, plus the amount remaining unexplained, 25.095%, add up to 100%. This is no accident. It is the reason why statisticians use the variance as a measure of variability. Let’s re-do the calculations, but using the software interface designed for statistical modeling rather than the interface used for constructing matrices. In R, the core program is called lm(), which stands for “linear model.” The lm() interface combines the construction phase and the model fitting phase and is connected with other software for doing an analysis of the model. mod_big &lt;- lm(bodyfat ~ ., data = Body_fat) coef(mod_big) ## (Intercept) age weight height neck chest ## -18.18848508 0.06207865 -0.08844468 -0.06959043 -0.47060001 -0.02386415 ## abdomen hip thigh knee ankle biceps ## 0.95477346 -0.20754112 0.23609984 0.01528121 0.17399537 0.18160242 ## forearm wrist ## 0.45202491 -1.62063910 rsquared(mod_big) ## [1] 0.74905 The first argument to lm() is a tilde expression specifying the response variable (on the left of ~) and the explanatory variables. In the shorthand used in the tilde expression, . means “all the columns except the response variable.” The vector of ones, identified as the “intercept,” is automatically included by default. These features of the interface are convenient and likely lower the chance of error. The results from lm() are, as you can see, identical to those produced by the matrix manipulations in the previous section. 44.2 Interpreting the model One of the ways statistics differs from mathematics is that statistics is concerned with the interpretation of the model, the ways the model is or is not fit for purpose, and the ways the model can be improved. Since the purpose of the body-fat model is to estimate the actual body fat percentage from easy-to-measure variables, let’s examine how much knowing the output of the model tells us about the actual body fat. We’ve already seen that R2 is 75%, but there are other ways to look at things. Figure 44.1 shows the actual body fat in each of the 252 men whose data are in Body_fat as a function of the model output. Figure 44.1: Comparing the output of the model to the actual body-fat measurements made in the 252 men represented in the Body_fat data frame. Using the graph, consider what to make of a model output of 20. Looking at the dozen or so men whose measurements led to an output near 20, their actual body fat spans a range of values, from about 10% to 30%. The prediction error for any given man is the difference between the actual value of body fat and the model output. The gray band on the graph contains about 95% of all the men. The vertical width of the band—about -8% to +8%—gives a prediction error that encompasses 95% of the men. This is interpreted to mean that the model output reflects the actual body mass, plus-or-minus 8%. (There’s no guarantee that the prediction error will be \\(\\leq 8\\%\\), but in the large majority of cases (roughly 95%) the model output will have a prediction error that’s no larger than \\(\\pm 8\\%\\).) Another difference between statistics and mathematics is that good statistical work always requires some understanding of the system being studied, not just the manipulation of columns of numbers. Look back at the coefficients. Some make sense and some are questionable. For instance, the positive coefficient on abdomen makes intuitive sense since everyday experience is that body fat often appears there. But the negative coefficient on neck, what’s that about? 44.3 Improving the model The model that we built of body fat, which we called big_mod, might just as well have been called “kitchen sink.” It includes all possible explanatory variables. But it doesn’t have terms higher than first order. That is, neck enters into the model just as a linear function. But in Chapter 25 we introduced low-order polynomial terms, such as the quadratic neck^2 or the interaction neck*abdomen. Maybe we would do better by introducing such terms. Since there are 13 explanatory variables, this would add 13 quadratic terms and many interaction terms, one for each pair of variables. That’s 78 vectors for the interaction plus 13 linear terms plus another 13 quadratic terms. Let’s try adding these interaction terms a few at a time to see what happens. The modeling syntax for interaction terms uses the multiplication symbol *. We’ll compare the new models to big_mod which had an R2 of 74.9%. mod2 &lt;- lm(bodyfat ~ . * neck, data = Body_fat) rsquared(mod2) ## [1] 0.7730899 Including the interactions with neck has increased R2 to 77.3%. Let’s keep going: mod3 &lt;- lm(bodyfat ~ . * neck + . * weight + . * abdomen, data=Body_fat) rsquared(mod3) ## [1] 0.7932603 Again, R2 has gone up! Unfortunately, adding more terms is not a sure-fire way to improve a model. In model 3, we added 12 vectors from *.neck, 11 from .*weight, 10 from .*abdomen. (The reason for the decreasing counts is that .*neck already includes weight*neck, so the neck*weight that’s included in . * weight is redundant.) Adding altogether 33 new vectors on top of the 13 in the original model, has increased the R2 from 74.9% to 79.3%. That’s a gain of 4.4 percentage points from 33 new vectors. Is that gain worth it? To answer that question, we need to consider what is the cost induced by adding the 33 new vectors. The computational cost is not the issue, since even much bigger models are easily constructed on ordinary laptops. The issue has to do with the goal of predicting bodyfat. We don’t need to predict body fat for the men in the data; we already know their body fat. Instead, the point is to make predictions for men not in the data. These two kinds of prediction are called in-sample prediction (men in the data set) and out-of-sample prediction (men not in the data set). The problem is that in-sample predictions tend to have a larger R2 than out-of-sample predictions. And it’s the out-of-sample prediction that will matter in applications: predictions for people whose body fat is unknown and therefore will be estimated from the model. A major goal in statistics and machine learning is to estimate the R2 for out-of-sample prediction from just the data in the sample itself. Here’s one way to get at the out-of-sample prediction error: divide the data at random into two halves, fitting the model on the first half and estimating the out-of-sample prediction error using the other half, the half that is out-of-sample for fitting the model. This strategy, and many refinements to it, are called cross-validation and is a major technique in machine learning. An older statistical technique gets at the problem of in- and out-of-sample by using models constructed from random stand-ins for explanatory vectors. For instance, we can examine whether the 33 interaction terms we added to the model are contributing to prediction by replacing them with 33 random vectors and examining the gain in R2 from that. Here’s one way to do this, using rand(33) to generate the random variables: mod3R &lt;- lm(bodyfat ~ . + rand(33), data=Body_fat) rsquared(mod3) ## [1] 0.7932603 The random variables did every bit as well as the interaction terms! So there’s no particular reason to think that the interaction terms are contributing to the prediction. The example we just gave with . + rand(33) isn’t completely adequate to the statistician’s needs. Those 33 random vectors were just a particular random choice, a role of the vector dice. We need to establish whether the result we got, R2=79.3% was just good luck. The next code block shows one way to do this: we generate many trials of rand(33), calculating R2 for each trial. Then compare the “genuine vectors” result to the distribution of results from the random trials. You do not need to learn how to construct such code, but perhaps you will be able to gain some insight from it. We’re doing 100 random trials. trials &lt;- do(100) * {lm(bodyfat ~ . + rand(33), data=Body_fat) %&gt;% rsquared()} table(rsquared(mod3) &gt; trials) ## ## FALSE TRUE ## 12 88 The “genuine vectors” gave a result bigger than 90 out of 100 trials. This sort of “most of the time” doesn’t cut muster. A convention is to insist that the genuine vectors win at least 95% of the time. The 90-out-of-100 result would typically be reported as a p-value less than 10%. Another sort of standard statistical report carries out this same sort of comparison to random vectors, but building up the model one term at a time. This is called analysis of variance. A standard report looks like this: anova(mod_big) ## Analysis of Variance Table ## ## Response: bodyfat ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 1493.3 1493.3 80.5644 &lt; 2.2e-16 *** ## weight 1 6674.3 6674.3 360.0837 &lt; 2.2e-16 *** ## height 1 1043.0 1043.0 56.2712 1.250e-12 *** ## neck 1 152.4 152.4 8.2227 0.004508 ** ## chest 1 641.1 641.1 34.5856 1.373e-08 *** ## abdomen 1 2757.4 2757.4 148.7645 &lt; 2.2e-16 *** ## hip 1 22.5 22.5 1.2157 0.271327 ## thigh 1 110.6 110.6 5.9665 0.015309 * ## knee 1 0.0 0.0 0.0013 0.971409 ## ankle 1 0.1 0.1 0.0043 0.948064 ## biceps 1 45.2 45.2 2.4369 0.119842 ## forearm 1 57.5 57.5 3.1013 0.079514 . ## wrist 1 170.1 170.1 9.1781 0.002720 ** ## Residuals 238 4411.4 18.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first row of the report examines how well the model with just age does compared to random vectors. As you can see, the p-value (column Pr(&gt;F)) is much smaller than the standard cut-off of 0.05. The next row in the report gives the improvement in the model when weight is included along with age in the model. And so on down the line. The report highlights some stinkers among the explanatory variables: hip, knee, ankle, and biceps. Maybe this matches your intuition, for instance that ankle circumference is not the best way to look at body fat. This sort of analysis, which has many nuances not covered here, falls under the name statistical inference. Another, more recent term is statistical learning, often called machine learning. The general idea of statistical or machine learning is to search through many combinations of possible explanatory variables to construct a “best” model. This search often involves models that are not simply linear combinations. Some of these model architectures have catchy names: “deep learning,” “neural nets,” “support vector machines,” “lasso,” and so on. Very often, these machine learning models are built by gradient descent, and are elaborations of the basic \\(\\mathit{A}\\vec{x} = \\vec{b}\\) model architecture. But they often have capabilities beyond \\(\\mathit{A}\\vec{x} = \\vec{b}\\). For instance, deep learning and lasso are designed to handle situations where the number of explanatory variables—the number of columns in the data frame—is far larger than the number of rows. An example: learning to identify whether there is an animal in a photograph with 20,000 pixels. And returning to genetics: measuring 100,000 genetic expression products on a sample of 10 people with a disease and 10 healthy controls to determine which, if any, genes are related to the disease. 44.4 Exercises Exercise XX.XX: 97zwR6 A. What’s the largest possible value for R2? As you know, R2 is a ratio of variances. A good way to think of variance is as the square length of a vector. So think about R2 as if it were calculated from the square length of \\(\\vec{b}\\) and \\(\\hat{b}\\). (Hint: What does the projection problem tell you about the lengths of \\(\\vec{b}\\) and \\(\\hat{b}\\).) B. What’s the smallest possible value for R2? (Hint: What’s the smallest possible length for \\(\\hat{b}\\)?) Amazingly, this work attracted little attention until after 1900, when Mendel’s laws were rediscovered by the botanists de Vries, Correns, and von Tschermak.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
