[["vectors-draft.html", "Chapter 41 Vectors Draft 41.1 Length &amp; direction 41.2 The nth dimension 41.3 Geometry &amp; arithmetic 41.4 Angles 41.5 Orthogonality 41.6 Exercises 41.7 Outline", " Chapter 41 Vectors Draft Until now, our presentation of calculus has featured functions, sometimes expressed as formulas involving combinations of the basic modeling functions, sometimes generated directly from data by smoothing or splines. Now we turn to a new framework for expressing functions, the inputs on which they operate, and the kind of outputs they generate. This framework is central to technical work in a huge range of fields. The usual name given to it by mathematicians is linear algebra, although only the word “linear” conveys useful information about the subject. The physicists developing the first workable quantum theory called it matrix mechanics. The framework is fundamental to scientific computation and is often the approach of choice even to non-linear problems. Application of the framework to problems of information access was the spark the ignited the modern era of search engines. Although the words “algebra” and “quantum” may suggest that conceptual difficulties are in store, in fact human intuition is well suited to establishing a useful understanding. We will introduce the framework in two different but equivalent forms. The geometric form is readily grasped by many students, even those with modest mathematical background. The geometry would have been easily grasped by the mathematicians of antiquity, who might even have regarded it as so trivial to be beneath their notice. They also could not possibly have understood the many important applications of the form which became apparent only in the last half of the 20th century with the emergence of statistics, data science, and computing generally. It can fairly be said that some of the most important developments in computer technology stem from algorithms developed from geometric properties of the form. The other, equivalent way of describing the mathematics is via simple arithmetic and numbers. This form is lacks the concreteness and visibility of the geometric approach, but more than makes up for this by the ways it makes the mathematics amenable to exact computation and to extending the applications in ways that transcend the limitations of our human, geometrical intuition. 41.1 Length &amp; direction A vector is a mathematical idea that is deeply rooted in everyday physical experience. A vector is simply an object consisting only of length and direction. A pencil is a good physical metaphor for a vector, but a pencil has other, non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil always has position: the place it’s at. Figure 41.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are exactly the same vector. Pencils have position, but vectors don’t. The green pencil shares the same direction, but it has a different length, so it is a different vector from the blue/yellow vector. Of course, a pencil has a tip and a tail; the eraser is affixed at the tail. Figure 41.2: Two different vectors. They have the same length and are parallel, but they point in opposite directions. Vectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a table top: a two-dimensional space. Naturally, the pencil-vectors are also embedded in our everyday three-dimensional space. The table-top can be thought of as a representation of a two-dimensional subspace of three-dimensional space. Pencils are useful for visualizing vectors, but it’s helpful to conceptualize them as a step or displacement in the sense of “step to the left” or “step forward.” An individual vector is a step specific length in a particular direction. Much of the mathematics of vectors can be understood as constructing instructions for reaching a target: “take three and a half steps along the green vector, then turn and take two steps backwards along the yellow vector.” Vectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are properly represented not as simple numerical quantities but as vectors with magnitude (that is, length) and direction. A statement like, “the plane’s velocity is 450 miles per hour to the north-north-west” is perfectly intelligible to most people, describing as it magnitude and direction. Note that the vector velocity can be understood without having to know where the plane is located; vectors have only the two qualities of magnitude and direction. Position is irrelevant to describing velocity, or, for that matter, force or acceleration. The gradients that we studied with partial differentiation (Chapter 24) are vectors. A gradient’s direction points directly uphill from a given point; it’s magnitude tells how steep the hill is at that point. Vectors are a practical tool in many situations such as relative motion. Consider the problem of finding an aircraft heading and speed to intercept another plane that’s also moving. The US Navy training movie from the 1950s shows how such calculations used to be done with paper and pencil. Nowadays such relative motion calculations are computerized. You may well wonder how the computer is able to represent vectors, since pencils aren’t part of computer hardware. The answer is disappointingly simple: the properties of direction and magnitude can also be represented by a set of numbers. Two numbers will do for a vector embedded in two-dimensional space, three for a vector embedded in three-dimensions. Representing a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 41.3, the vector (that is, the green pencil) has been placed in a coordinate system. Usually you would expect there to be labels for each of the coordinate lines, but this labelling is not necessarily to show a vector even if it is needed to specify a position. The two coordinates to be assigned to the vector are the difference between the tip and the tail. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\). Figure 41.3: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper. By convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure 41.3, which we’ll call \\(\\overset{\\longrightarrow}{\\text{green}}\\), would be written: \\[\\overset{\\longrightarrow}{\\text{green}} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] Such notation is intended for a human reader, perhaps clearly to distinguish a vector from a coordinate description of a position such as \\((20,16)\\). It’s also useful to carry over the idea of “this is a column” in computer software, by including into the computer data structure connecting the numbers themselves—20 and 16 here—to a description of the “shape” of the vectors. By convention we would say that \\(\\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\) has a \\(2 \\times 1\\) shape, which will be printed out as 2 rows in one column. A vector like velocity will have three numerical components, that is \\[\\vec{v} = \\left[\\begin{array}{c}v_x\\\\v_y\\\\v_z\\end{array}\\right]\\ .\\] Each of these components might be a function of time, so \\[\\partial_t \\vec{v}(t) = \\left[\\begin{array}{c}\\partial_t v_x(t)\\\\\\partial_t v_y(t)\\\\\\partial_t v_z(t)\\end{array}\\right] = \\vec{a(t)}\\ ,\\] where \\(\\vec{a}(t)\\) is the vector acceleration. The numerical representation of vectors is convenient when we need to perform mathematical operations on vectors such as addition, subtraction, reversal, and scaling. For instance, if an object with velocity vector \\(\\vec{v}\\) is subjected to a force vector \\(\\vec{f}\\), the velocity vector will change over time: \\[\\partial_t \\vec{v} = \\frac{1}{m}\\vec{f}\\] (where \\(m\\) is the mass of the object). Concretely, the above equation can be seen as shorthand for the description of the derivative of three functions, one each for the \\(x\\), \\(y\\), and \\(z\\) components of \\(\\vec{v}\\), that is: \\[\\partial_t v_x(t) = f_x/m\\\\ \\partial_t v_y(t) = f_y/m\\\\ \\partial_t v_z(t) = f_z/m\\ .\\] We can apply numerical techniques such as integration to each of the components, for instance to find out what the velocity will be at some future time. The numerical representation of vectors is also the way that most people can understand a tremendously important generalization of vectors to something that will at first seem absurdly abstract: 4-dimensional space, 5-dimensional space, and on up. In general: \\(n\\)-dimensional space. 41.2 The nth dimension Living as we do in a palpably three-dimensional space, and being part of a species whose senses and brains developed in three dimensions, it’s hard and maybe even impossible to get a grasp on what higher-dimensional spaces would be like. A lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character, Square, receives a visitor, Sphere, from the three-dimensional world in which Flatland is embedded. Only with difficulty can Square assemble a conception of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace. But even if the spatial extent of higher dimensions is not accessible, the one-dimensional vector inhabitants of any such space can be readily perceived and constructed as alist of numbers. With this device, allow us to introduce vectors from 4, 5, and 6 dimensions, and even \\(n\\) dimensional space. \\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\] Sensible people may consider it mathematical tomfoolery to promote of an everyday column of numbers into a vector in high-dimensional space, but there is a good reason. It encourages us to think about the arithmetic we are about to do on vectors in terms of familiar geometrical concepts: lengths, angles, alignment, and so on. Perhaps unexpectedly, it also guides us to think about data—which consists of columns of numbers in a data frame—using our powerful geometrical intuition. 41.3 Geometry &amp; arithmetic In Chapter 44 we will apply a mathematical apparatus based on vectors to using data in models and to a deeper understanding of technological systems such as radio and spectroscopy based on sinusoidal oscillations. In applied work, there may be dozens or thousands of vectors involved. But three simple geometric properties will lay the foundation for even advanced techniques: The length of a vector. The scaling of a vector to make it longer or shorter or point in the opposite direction. The angle between two vectors. You can measure the length of a vector with a ruler. For instance, Figure 41.3 shows a green vector on ruled paper; the spacing between adjacent lines in the graph paper is 0.2 inches. If you place your index finger on the eraser and your little finger on the tip, and hold that distance, you can bring your fingers into alignment with the graph paper and see that the vector is roughly 5.2 inches long. Another way to find the length of the distance is with arithmetic. Recall that by counting squares between tip and tail, we found: \\[\\overset{\\longrightarrow}{\\text{green}} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\ .\\] Applying the Pythagorean theorem, we can calculate the vector length as \\[\\| \\overset{\\longrightarrow}{\\text{green}} \\| = 0.2\\, \\text {inches} \\sqrt{\\strut 20^2 + 16^2} = \\\\0.2\\, \\text{inches} \\times 25.6125 = \\\\\\strut5.12\\,\\text{inches}\\ .\\] The notation \\(\\| \\vec{v} \\|\\) should be read, “the length of \\(\\vec{v}\\).” For simplicity, we generally leave the length in the same units as the numbers specifying the vector, so \\[\\| \\overset{\\longrightarrow}{\\text{green}} \\| = \\sqrt{\\strut 20^2 + 16^2} = 25.6125\\ .\\] For a vector with \\(n\\) components, \\[\\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\end{array}\\right]\\ ,\\] the length is \\(\\|\\vec{w}\\| \\equiv \\sqrt{\\strut w_1^2 + w_2^2 + \\cdots + w_n^2}\\). This arithmetic formula for length allows us to find the length even of those vectors in 4- or higher-dimensional space, where we would be hard pressed to find a ruler. To scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. Perhaps a better way to think about this is to think about the vector as a step of length \\(\\|\\vec{w}\\|\\) in the vector’s direction. Scaling the vector by 2 means to take two steps, scaling it by 10 means to take 10 steps. Fractional steps and backward steps are also allowed, so scaling \\(\\vec{w}\\) by -2.5 means to take two and a half steps backward from the direction in which the vector points. Arithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. For instance, scaling \\(\\vec{w}\\) by -2.5 gives: \\[-2.5\\, \\vec{w} \\equiv \\left[\\begin{array}{c}-2.5\\,w_1\\\\-2.5\\,w_2\\\\\\vdots\\\\-2.5\\,w_n\\end{array}\\right]\\] The number doing the scaling is called a scalar, and the multiplication by a scalar is called scalar multiplication. 41.4 Angles Any two vectors of the same dimension have a distinct angle between them. This is easily seen for two-dimensional vectors. Draw two vectors on a sheet of paper. Since vectors have only two properties, length and direction, in your mind’s eye you can pick up one of the vectors and relocate its “tail” to meet the tail of the other vector. The letters L and V illustrate the connection between the two vectors as do the characters ^, &gt;, and &lt;. The angle for L is roughly 90 degrees, the other characters are made of vectors with acute angles (that is, less than 90 degrees). The two vectors and /, when brought together as / subtend an obtuse angle. In describing the angle between two vectors, we always measure the short way round. So angles between vectors are always between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260 degree angle. In 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the vectors so they are tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector. It’s also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) that are in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components: \\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\end{array}\\right]\\ ,\\] There is a straightforward arithmetic formula for the cosine of the angle \\(\\theta\\) between \\(\\vec{v}\\) and \\(\\vec{w}\\): \\[\\cos(\\theta) = \\frac{v_1\\, w_1 \\ + \\ v_2\\, w_2 \\ + \\ \\cdots\\ + \\ v_n\\, w_n}{\\sqrt{\\strut v_1^2 + v_2^2 + \\cdots + v_n^2}\\ \\sqrt{\\strut w_1^2 + w_2^2 + \\cdots + w_n^2}}\\] You might recognize the two quantities in the denominator of the ratio as the lengths \\(\\|\\vec{v}\\|\\) and \\(\\|\\vec{w}\\|\\) respectively. There’s also a special notation and name for the quantity in the numerator. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\(\\vec{v}\\cdot\\vec{w}\\) and the sum of pairwise products of the vectors components: \\[\\text{dot product:}\\ \\ \\ \\ \\ \\vec{v}\\cdot\\vec{w} \\equiv v_1\\, w_1 + v_2\\, w_2 + \\cdots + v_n\\, w_n\\ .\\] Using the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\] If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), there is a function that will do the conversion show in in Figure 41.4. Figure 41.4: The \\(\\arccos()\\) function converts \\(\\cos(\\theta)\\) to \\(\\theta\\). What does the angle \\(\\theta\\) between two vectors tell us? In geometrical terms, the angle tells us how strongly aligned the vectors are. An angle of 0 tells us the vectors point in exactly the same direction, and angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that by appropriate scalar multiplication, the two vectors could be made exactly equal to one another and, consequently, that the scaled vectors would be one and the same. Angles such as 5 or 175 degrees indicate that the two vectors are mostly aligned, but imperfectly. When the angle is 90 degrees of course—a right angle—the two vectors are perpendicular. The vector alignment has a particularly important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics there is an important quantity called the correlation coefficient, denoted \\(r\\). To say that two variables are correlated means that the variables are connected to one another in some way. For instance, among children, height and age are correlated. Since height tends to increase along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\). A negative correlation means that as one variable increases the other tends to decrease. Temperature and elevation are negatively correlated, as are the pressure and volume of a gas at a given temperature. The most negative possible correlation is \\(r=-1\\). A zero correlation indicates that there is no simple relationship between the two variables. In terms of vectors, that is, the columns in the data frame, the correlation coefficient \\(r\\) is exactly the same quantity as the cosine of the angle between the vectors. At the time the correlation coefficient was invented in the 1880s, it was not widely appreciated that \\(r\\) is simply the cosine of an angle. Perhaps the several generations of statistics students who have studied correlation would have had a better grasp on the subject if it had been called alignment and measured in degrees. 41.5 Orthogonality Two vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech we call a 90 degree angle a “right angle.” The word “orthogonal” is really just a literal translation of “right angle.” The syllable “gon” indicates an angle, as in the five-angled pentagon or six angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet). Two vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero. Example 41.1 Find a vector that’s orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\). The arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\). We can confirm the orthogonality by calculating the dot product: \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\). Example 41.2 Find a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\). We have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components. For instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\ __\\\\ __\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}1\\\\0\\\\3\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\). In addition to the two vectors we constructed with the arithmetic trick, any vector that is a linear combination of those two vectors will also be orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\). 41.6 Exercises Exercise XX.XX: Of7QlW Write (or draw) your answers on a sheet of paper. You can do the any calculations you need in the sandbox. Make sure to write column vectors in a correct column format. Use a SANDBOX for numerical calculations. What is \\(5.42 \\left(\\begin{array}{r}7.3\\\\8.9\\\\-2.4\\end{array}\\right)\\)? What is \\(-2.67 \\left(\\begin{array}{r}-19.34\\\\0.23\\\\14.82\\end{array}\\right)\\)? Draw a vector of your own choice. Next to it, draw the vector which is 2.5 times your vector. Take your original vector from (c) and draw the vector which is -3 times it. Write down the R code to create a vector named w that is \\[\\left(\\begin{array}{c}2\\\\5\\\\1\\\\5\\end{array}\\right)\\] Exercise XX.XX: dwALW7 Consider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) \\] Question A What is the length of the vector \\(\\vec{u}\\)? 2︎✘ There are two elements in \\(\\vec{u}\\), but that’s not what the “length” of a vector means. 5Nice!  25︎✘ This is the length-squared. Take the square root of it to find the length. None of the above︎✘ Question B What is the length of the vector \\(\\vec{w}\\)? 2Excellent! Right: \\(\\sqrt{1^2 + 1^2 + 1^2 + 1^2}\\) 4︎✘ There are four elements in \\(\\vec{w}\\)$, and the square-length of \\(\\vec{v}\\) happens to be 4 (\\(1^2 + 1^2 + 1^2 + 1^2\\)), but neither of these is the “length” of \\(\\vec{w}\\). 6︎✘ None of the above︎✘ Question C What is \\(w^T \\circ u\\)? 7︎✘ 14︎✘ It doesn’t exist.Nice! Two vectors have to have the same dimension for a dot product to make sense. Exercise XX.XX: xuENab Suppose \\[\\vec{\\mathbf u} \\equiv \\left(\\begin{array}{c}4\\\\-3\\end{array}\\right) \\ \\ \\mbox{and}\\ \\ \\vec{\\mathbf v} \\equiv \\left(\\begin{array}{c}2\\\\2\\end{array}\\right)\\ .\\] The following questions ask you to compute the length of the vector given. (If none of the available choices are exactly right, choose the closest.) Question A \\(3 \\vec{u}\\)     3︎✘        4︎✘        5︎✘        6︎✘        10︎✘        15\\(\\heartsuit\\ \\)       16︎✘ Question B 42) \\(-2 \\vec{u}\\)     3︎✘        4︎✘        5︎✘        6︎✘        10\\(\\heartsuit\\ \\)       15︎✘        16︎✘ Question C \\(-\\vec{v}\\)     3\\(\\heartsuit\\ \\)Right. It’s \\(\\sqrt{8} = \\sqrt{2^2 + 2^2}\\) to be precise.       4︎✘        5︎✘        6︎✘        10︎✘        15︎✘        16︎✘ Question D \\(\\vec{u}+ \\vec{v}\\)     3︎✘        4︎✘        5︎✘        6\\(\\heartsuit\\ \\)       10︎✘        15︎✘        16︎✘ Question E \\(2 \\vec{u} - 5 \\vec{v}\\)     3︎✘        4︎✘        5︎✘        6︎✘        10︎✘        15︎✘        16\\(\\heartsuit\\ \\) Exercise XX.XX: MBTGQt Collision course? Consider the diagram showing two straight-line tracks, a dot on each track, and a vector. Let’s imagine that dot 1 is an aircraft and that the black vector attached to it is the aircraft’s velocity. We’ll call this \\(\\vec{v}_1\\), Similarly for dot 2, where the velocity vector will be called \\(\\vec{v}_2\\). There’s a third vector drawn in red: the difference in position of the two aircraft at the exact moment depicted in the drawing. The question we want to address is whether the aircraft are on a collision course. Obviously, the two courses cross. So we know that the two aircraft will cross the same point. For a collision, the aircraft have to cross that point at the same time. Copy over the drawing to your own piece of paper. You don’t need to get the vectors and positions exactly right; any reasonable approximation will do. Now you are going to do visual vector addition and subtraction to answer the collision question. The relative velocity of the two planes is the difference between their velocities. Subtract \\(\\vec{v}_2\\) from \\(\\vec{v}_1\\) and draw the resulting vector. Pay attention to both the length and direction of the relative velocity. The displacement between the two planes is the red vector: the position of dot 2 subtracted from dot 1. Compare the directions of the relative velocity vector and the displacement vector. If they are aligned, then the planes are on a collision course. In the picture as drawn, the relative velocity vector and the displacement vector are not aligned. Figure out how much you would need to change the length of \\(\\vec{v}_2\\) so that the relative velocity does align with the displacement. (Keep the direction the same.) Draw this new vector and label it clearly “vector for intercept.” In (3) you changed the length of \\(\\vec{v}_2\\) keeping the direction the same. Now you are going to keep \\(\\vec{v}_2\\) at the original length, but change its direction so that the new relative velocity is aligned with the displacement vector. Items (3) and (4) are two different ways of designing an intercept of plane 1 by plane 2. Bonus) You can figure out how long it takes for each plane to reach the intersection point by finding out how many multiples of the velocity vector will cover the line segment between the plane’s position and the intersection point. For example, in the original drawing \\(4 \\vec{v}_1\\) will bring the plane to the intersection point, so it takes 4 “time units” for the plane to reach the point. (What is the time unit? If velocity is in miles/hour, then the time unit is hours. If the velocity is in feet/second, then the time unit is seconds.) Your task: Figure out where aircraft 2 will be in 4 time units. This will tell you the separation between aircraft 2 and aircraft 1 when 1 reaches the intersection point. Draw and clearly label this vector. Exercise XX.XX: Q2ars0 Copy over and label these vectors onto your paper. Any good approximation will do. Then, on paper, draw (and label with the Roman numeral) the following vector additions and subtractions. (You should “show your work” by putting the vectors being added in the diagram along with the result of the addition.) \\(\\vec{a} + \\vec{b}\\) \\(\\vec{c} + \\vec{d}\\) \\(\\vec{d} + \\vec{b} + \\vec{a}\\) \\(\\vec{b} - \\vec{a}\\) Exercise XX.XX: OoPatc Draw two vectors with different directions. Estimate by eye the angle between them. Give your angle in two different units: degrees and radians. (The conversion is degrees = \\(\\frac{180}{\\pi}\\) radians. Or, roughly, degrees = 57.3 radians.) Exercise XX.XX: Gw42pX Note: The radio-button multiple choice questions don’t allow us to display a column vector as such. Instead, we use the notation involving a superscript T. For instance, \\((2, -3)^T\\) is a column vector which we would ordinarily write \\(\\left(\\begin{array}{c}2\\\\-3\\end{array}\\right)\\) A vector written like \\((2, -3)\\), without the \\(^T\\), is a row vector. Suppose \\[\\vec{\\mathbf{u}} \\equiv \\left(\\begin{array}{c}2\\\\-3\\end{array}\\right) \\ \\ \\hbox{and}\\ \\ \\vec{\\mathbf{v}} \\equiv \\left(\\begin{array}{c}4\\\\1\\end{array}\\right)\\] Compute the following linear combinations (arithmetically) … Question A \\(- \\vec{\\mathbf{u}} -8 \\vec{\\mathbf{v}}\\) \\((-34, -5)^T\\)Correct.  \\((-34, -5)\\)︎✘ The linear combination of two column vectors will be a column vector. \\((-34, 15)\\)︎✘ This is a row vector, not a column vector. \\((34, 15)^T\\)︎✘ Watch your arithmetic. Invalid combination︎✘ Question B \\(2.1 \\vec{\\mathbf{u}} -1.3 \\vec{\\mathbf{v}}\\) \\((-1, -7.6)^T\\)Right!  \\((0.8)\\)︎✘ The linear combination of two column vectors will be a column vector. \\((-1, -7.6)\\)︎✘ This is a row vector, not a column vector. \\((2.1, -1.3)^T\\)︎✘ These are the scalar multipliers in the linear combination, not the value of the linear combination. Invalid combination︎✘ Question C \\(-0.5 \\vec{\\mathbf{u}} + 3.2 \\vec{\\mathbf{v}}\\) \\((11.8, 4.7)^T\\)Excellent!  \\((2.7)\\)︎✘ The linear combination of two column vectors will be a column vector. \\((13.8, 1.7)\\)︎✘ This is a row vector \\((34, 16)^T\\)︎✘ Watch your arithmetic. Invalid combination︎✘ Question D \\(7 \\vec{\\mathbf{u}} + 5 \\vec{\\mathbf{v}}\\) \\((34, -16)^T\\)Right!  \\((32, -14)^T\\)︎✘ Check your arithmetic! \\((34, -14)^T\\)︎✘ Check your arithmetic! \\((32, -16)^T\\)︎✘ Check your arithmetic! Invalid combination︎✘ Exercise XX.XX: 1Jxboc In this exercise, you are going to check proposed solutions to the target problem. Each question poses one target problem. One of the answers is correct. Use R in a SANDBOX to select the correct answer. The vectors you will be working with are: \\[\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\] Question A i. What linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) will reach target \\(T\\)? \\(3\\vec{a} - 7\\vec{b}\\)︎✘ \\(\\vec{a} + 2\\vec{b}\\)︎✘ \\(-4 \\vec{a} + 7 \\vec{b}\\)Good.  \\(- \\vec{a} + 4\\vec{b}\\)︎✘ No combination will reach \\(\\vec{T}\\) exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question B ii. What linear combination of \\(\\vec{b}\\) and \\(\\vec{c}\\) will reach target \\(T\\)? \\(\\frac{4}{3}\\vec{b} - \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{7}{3}\\vec{c}\\)︎✘ \\(-\\frac{7}{3}\\vec{b} + \\frac{5}{3}\\vec{c}\\)︎✘ \\(\\frac{5}{3} \\vec{b} + \\frac{4}{3} \\vec{c}\\)Good.  No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question C iii. What linear combination of \\(\\vec{b}\\) and \\(\\vec{d}\\) will reach target \\(T\\)? \\(0\\vec{b} - \\frac{3}{2}\\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ \\(0 \\vec{b} - \\frac{1}{2} \\vec{d}\\)Correct.  \\(\\frac{3}{2}\\vec{b} + \\frac{1}{2}\\vec{d}\\)︎✘ No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. None of the above︎✘ Question D iv. Which of these linear combinations of $ ec{a}$, \\(\\vec{b}\\) and \\(\\vec{c}\\) will reach target \\(T\\)? \\(\\vec{a} -\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{a} +\\vec{b} - 2\\vec{c}\\)︎✘ \\(2\\vec{a} -\\vec{b} + 2\\vec{c}\\)Nice!  \\(-2\\vec{a} +\\vec{b} + 2\\vec{c}\\)︎✘ No combination will reach $ exactly.︎✘ Two 2-dimensional vectors pointing in different directions can reach any point in the plane. Adding a third vector just increases the number of possibilities. None of the above︎✘ Exercise XX.XX: N98zli Referring to the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), and \\(\\vec{d}\\) in the figure, construct these linear combinations. Your diagram should show both the scaled vectors being added and the output of the linear combination. \\(2 \\vec{a} + 1 \\vec{b}\\) \\(1.5 \\vec{c} - 2 \\vec{d}\\) \\(- \\vec{b} + 2\\vec{c} + 3\\vec{d}\\) Exercise XX.XX: PCfYDo In physics and engineering, there is a very important operation on two vectors \\((a, b, c)^T\\) and \\((e, f, g)^T\\) called the “cross product.” (The operation is only defined for vectors in 3-dimensional space.) The output of the cross product is another 3-dimensional vector which can be calculated arithmetically as: \\[\\left(\\begin{array}{c}a\\\\b\\\\c\\end{array}\\right) \\times \\left(\\begin{array}{c}e\\\\f\\\\g\\end{array}\\right) \\equiv \\left(\\begin{array}{c}b g - c f\\\\c e - a g\\\\a f - b e\\end{array}\\right)\\] DD16 Exercise 6 on paper. Make up coordinates for two three-dimensional vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) that point in different directions. Then carry out the arithmetic to find the vector \\(\\vec{w}\\) that is the cross product \\(\\vec{u} \\times \\vec{v}\\). Find the angle between \\(\\vec{w}\\) and \\(\\vec{u}\\). Find the angle between \\(\\vec{w}\\) and \\(\\vec{v}\\). Other than this brief description, we will not use cross products at all in this course. But keep them in mind for your upcoming physics and engineering courses. Exercise XX.XX: 4QzeWP Place a dot on a piece of paper. Hold a pencil perpendicular to the paper with the eraser on the dot. Now, (using a pen!) draw a vector on the paper that is perpendicular to the pencil. Now draw 5 more vectors, each different from the other, that are all perpendicular to the pencil. Draw a vector \\(\\vec{c}\\) on the paper. Now draw another vector on the paper that is perpendicular to the first. Label it \\(\\vec{a}\\). Find another vector, to be labeled \\(\\vec{b}\\) that lies on the paper, is orthogonal to \\(\\vec{c}\\), and has a different direction from \\(\\vec{a}\\). It will turn out that \\(\\vec{a} = k \\vec{b}\\). Write down the numerical value of the scalar \\(k\\) for your \\(\\vec{a}\\) and \\(\\vec{b}\\). Consider the vector \\(\\vec{w} \\equiv (2, 5)^T\\). Using only arithmetic, find some other vector \\(\\vec{z}\\) that is perpendicular to \\(\\vec{w}\\). (Hint: To be perpendicular, \\(\\vec{z}^T \\cdot \\vec{w}\\) must be zero. So make up two numbers such \\(a\\) and \\(b\\) such that \\(2 a + 5 b = 0\\).) Write \\(\\vec{z}\\) on your paper. Repeat (c) and create another vector \\(\\vec{z}_2\\) that is different from \\(\\vec{z}\\). Write \\(\\vec{z}_2\\) on your paper. In addition, find the scalar \\(k\\) such that \\(\\vec{z}_2 = k \\vec{z}\\). In the following exercises, you are asked to find numerical vectors that are perpendicular (“orthogonal”) to the stated vector. This is equivalent to finding a new vector whose dot product with the stated vector is zero. (Except … the new vector can’t be all zeros! The all-zero vector is exceptional and has no direction.) One effective strategy is to write down a vector consisting of a 1 in any position you like and zeros elsewhere. Find the dot product, calling it \\(d\\). Then select one of the zeros to turn it non-zero. What value should it have? Just enough so that when included in the dot product it contributes \\(-d\\). Consider the vector \\(\\vec{w} \\equiv (1, 2, -3, 2)^T\\). Find a vector \\(\\vec{a}\\) that is perpendicular to \\(\\vec{w}\\). Find another vector \\(\\vec{b}\\) that is perpendicular both to \\(\\vec{w}\\) and to \\(\\vec{a}\\). (Hint: It can be done.) Create yet another vector \\(\\vec{c}\\) that is a linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\). (You can choose whatever combination you like, but both coefficients should be non-zero.) What is the angle between \\(\\vec{c}\\) and \\(\\vec{w}\\)? You have been sent on a mission to 5-dimensional space. Your task is to find a vector that is perpendicular to \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\). Do so. BONUS. Find as many vectors as you can that are perpendicular to \\(\\vec{w} \\equiv (2,1,3,3,-4)^T\\) and to each other. The arithmetic will not necessarily be easy. When you give up (with honor!), figure out how many mutually perpendicular vectors there can be in 5-dimensional space. How about 10-dimensional space? Exercise XX.XX: 5zmmBu Find the lengths of these vectors. Each square on the “graph paper” is 1 unit. Assume vectors begin and end exactly on the graph-paper intersections. Question A Length of vector \\(\\vec{A}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5\\(\\heartsuit\\ \\)       5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question B Length of vector \\(\\vec{B}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5\\(\\heartsuit\\ \\)       5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question C Length of vector \\(\\vec{C}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66\\(\\heartsuit\\ \\)       6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question D Length of vector \\(\\vec{D}\\).     2︎✘        2.82\\(\\heartsuit\\ \\)       4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question E Length of vector \\(\\vec{E}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32\\(\\heartsuit\\ \\)       8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question F Length of vector \\(\\vec{A} + 2 \\vec{B}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18\\(\\heartsuit\\ \\)       11.66︎✘        12︎✘        12.22︎✘ Question G Length of vector \\(\\vec{C} - \\vec{B}\\).     2︎✘        2.82︎✘        4︎✘        4.12\\(\\heartsuit\\ \\)       5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66︎✘        12︎✘        12.22︎✘ Question H Length of vector \\(\\vec{C} + \\vec{E}\\).     2︎✘        2.82︎✘        4︎✘        4.12︎✘        5︎✘        5.66︎✘        6.32︎✘        8︎✘        11.18︎✘        11.66\\(\\heartsuit\\ \\)       12︎✘        12.22︎✘ Exercise XX.XX: 7KJQQd Question A What is \\(\\vec{C}^T\\cdot\\vec{D}\\)?     -8︎✘        0\\(\\heartsuit\\ \\)       6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question B What is \\(\\vec{E}^T\\cdot\\vec{B}\\)?     -8︎✘        0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30\\(\\heartsuit\\ \\)       34︎✘        40︎✘        42︎✘ Question C What is \\(\\vec{A}^T\\cdot\\vec{E}\\)?     -8︎✘        0︎✘        6︎✘        10\\(\\heartsuit\\ \\)       14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question D What is \\(\\vec{D}^T\\cdot\\vec{E}\\)?     -8\\(\\heartsuit\\ \\)       0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40︎✘        42︎✘ Question E What is \\(\\left(\\vec{A}+\\vec{B}\\right)^T\\cdot\\vec{C}\\)?     -8︎✘        0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24︎✘        30︎✘        34︎✘        40\\(\\heartsuit\\ \\)       42︎✘ Question F What is \\(\\left(\\vec{C}+\\vec{D}\\right)^T\\cdot\\vec{E}\\)?     -8︎✘        0︎✘        6︎✘        10︎✘        14︎✘        16︎✘        24\\(\\heartsuit\\ \\)       30︎✘        34︎✘        40︎✘        42︎✘ Exercise XX.XX: k5u7hG Here are 12 vectors, labeled “a” through “m.” (Letter “i” has been left out.) There are several quick questions, each of which makes a claim about whether the sum of two vectors equals a third. Answer true or false to the claim. There are no tricks about exactitude, so if the claim is close to being true, answer true. Question A 1) \\(\\vec{a} + \\vec{b} = \\vec{L}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question B 2) \\(\\vec{b} + \\vec{J} = \\vec{a}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question C 3) \\(\\vec{b} + \\vec{m} = \\vec{J}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question D 4) \\(\\vec{c} + \\vec{f} = \\vec{d}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question E 5) \\(\\vec{k} + \\vec{L} = \\vec{e}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question F 6) \\(\\vec{e} + \\vec{b} = \\vec{m}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question G 7) \\(\\vec{m} + \\vec{g} = \\vec{b}\\)     True\\(\\heartsuit\\ \\)       False︎✘ 41.7 Outline Dual representations of vectors. arithmetic: a vector is a column of numbers. graphically: an arrow. direction and a magnitude. Position isn’t a feature. You can draw the vector whereever is convenient so long as you keep the direction and magnitude. A vector is embedded (“lives in”) a space. The dimension of that space is the count of rows in the column of numbers (arithmetic representation). Graphically, the dimension must be figured out in context, since all arrows look the same. Scalar multiplication and addition in both representations. Dot product takes two vectors, produces a number arithmetical operation: componentwise multiply then add interpreting graphically a. proportional to (cosine of) angle between vectors Length of a vector arithmetically: pythagorus arithmetically: sqruare root of dot product with itself graphically: ruler. gvec(from=c(-2,3), to=c(4,4), color=&quot;red&quot;) %&gt;% graph_paper(xticks=-5:5, yticks=-5:5) %&gt;% gvec(from=c(0,0), to=c(-2,3)) "],["linear-combinations-of-vectors.html", "Chapter 42 Linear combinations of vectors 42.1 Scaling 42.2 Sub-spaces 42.3 Functions as vectors 42.4 Matrices 42.5 Exercises", " Chapter 42 Linear combinations of vectors In the previous chapter, we suggested that you think of a vector as a “step” in a given direction and of a given magnitude: for example 1 foot to the northeast. This interpretation highlights the mathematical structure of vectors: just a direction and a length, nothing else. The “step”-interpretation is also faithful to an important reason why vectors are useful. We use steps to get from one place to another. Similarly, a central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We’ve used quotation marks around “place” because we are not necessarily referring to a physical destination. We’ll get to what else we might mean by “place” later in this chapter. For now, let’s focus on the word “steps.” Notice that it’s plural. Generally, you cannot get to the desired destination with just one step. For instance, a pirate’s treasure map might contain an instruction, “Dig 10 paces to the east of the tall tree.” East is the direction of the vector. “Pace” is the length of the vector (about 3 feet). And 10 is the number of steps to take. If pirates wrote in vector notation, those directions might have been given like this: \\[\\text{Let}\\ \\vec{p} \\equiv \\left[\\begin{array}\\,1\\,\\text{pace}\\\\0\\end{array}\\strut\\right]\\ .\\ \\ \\text{Then}, \\ \\text{treasure spot} \\equiv \\text{tall tree} \\ + 10 \\vec{p}\\ .\\] The count of steps is the scalar that multiplies the vector. It tells you the length of the journey in units of 1 vector. In a typical journey, there can be more than one vector involved. For instance, the vector pointing from the hidden cove to the tall tree might be identified as 1 nautical mile to the north-west. We’ll call this vector \\(\\overset{\\longrightarrow}{\\text{tree}}\\). The full directions to the treasure might then be, \"Start from the hidden cove. Then march \\(3.2\\, \\overset{\\longrightarrow}{\\text{tree}} + 10\\, \\vec{p}\\). This is a description of a journey as a linear combination of vectors. The vectors themselves are \\(\\vec{p}\\) and \\(\\overset{\\longrightarrow}{\\text{tree}}\\). The vector \\(3.2\\, \\overset{\\longrightarrow}{\\text{tree}}\\) is a scaled version of \\(\\overset{\\longrightarrow}{\\text{tree}}\\); it points in the same direction as \\(\\overset{\\longrightarrow}{\\text{tree}}\\), but it has length 3.2 nautical miles. Similarly, \\(10\\, \\vec{p}\\) is a vector of length 10 paces, pointing the the same direction of \\(\\vec{p}\\). When we scale a set of vectors and then add them, we have a linear combination, just as in Block 1 we created a linear combination of functions by scaling them and adding the results together. Pirates and other mariners use direction terms like “one point north of north-north-east.” Their maps are annotated with compass roses that translate the words into a direction. Mathematicians can replace a compass rose with just two vectors, say, \\(\\overset{\\longrightarrow}{\\text{North}}\\) and \\(\\overset{\\longrightarrow}{\\text{East}}\\). Other directions can be given as a linear combination. For instance, the compass rose’s “north-north-west” is the linear combination \\(0.9239\\,\\overset{\\longrightarrow}{\\text{North}} -0.3827\\,\\overset{\\longrightarrow}{\\text{East}}\\). Geometrically, that is, in terms of arrows or pencils, the scaling of a vector is stretching or shrinking, without changing the direction. (Scaling with a negative number means to reverse the tip and tail of the vector.) For a linear combination of two or more vectors, choose one of the vectors as a start, then move the tail of the second vector to the tip of the first, as in Figure 42.1. Figure 42.1: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. This resultant is equivalent to the blue vector. Subtraction is similar. For \\(\\vec{v} - \\vec{w}\\), \\(\\vec{v}\\) and \\(\\vec{w}\\) tail-to-tail. Read out the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 42.2, the yellow vector is \\(\\vec{v}\\), the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector. Figure 42.2: Subtracting blue from yellow gives green. 42.1 Scaling As you know, scaling a vector changes it’s length. If the scalar is negative, the vector tip and tail are swapped. Figure 42.3 shows a vector \\(\\vec{v}\\) and, as a dotted line, all of the possible results of scaling the vector. For instance, the tip of the vector \\(-\\frac{1}{2} \\vec{v}\\) is located at the point marked (a). The vector \\(\\frac{4}{5} \\vec{v}\\) has its tip at (b) and \\(2.5 \\vec{v}\\) has the tip at (c). For any point on the dotted line, there is some numerical scalar \\(\\alpha\\) such that the scaled vector \\(\\alpha \\vec{v}\\) will have its tip at that point. The dotted line, that is, all the points that can be “reached” by scaling \\(\\vec{v}\\) is called a subspace: the subspace spanned by \\(\\vec{v}\\). You can think of the subspace as all of the “destinations” that can be reached by stepping in the direction of \\(\\vec{v}\\). (Steps backward steps are also legitimate.) Figure 42.3: With a suitable scalar multiplying the vector \\(\\vec{v}\\), any point on the dotted line can be reached. The dotted line is the subspace spanned by the vector. Every vector is associated with a subspace that is one-dimensional; you can only reach the points on a line by stepping in the direction of a vector. 42.2 Sub-spaces Figure 42.4 includes a second vector \\(\\vec{w}\\) along with the vector \\(\\vec{v}\\) seen in Figure 42.3. Each vector has its own subspace, again shown as dotted lines. Figure 42.4: The subspaces spanned by each of two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\). Things get interesting when we consider not just the subspace spanned by the vectors individually, but the subspace spanned by them jointly. Recall that the subspace spanned by \\(\\vec{v}\\) is all the vectors that can be created by scalar multiplication of \\(\\vec{v}\\), that is, all the vectors \\(\\alpha \\vec{v}\\) for \\(-\\infty &lt; \\alpha &lt; \\infty\\). With two vectors, the subspace is all the vectors that can be created by a linear combination of the two, that is \\[\\alpha \\vec{v} + \\beta \\vec{w}\\] where \\(-\\infty &lt; \\alpha &lt; \\infty\\) and \\(-\\infty &lt; \\beta &lt; \\infty\\). The entire plane is spanned by such linear combination; any vector at all in the plane can be generated. Figure 42.4, lying as it does on the two-dimensional surface of your screen or paper, fails to indicate clearly that even in three- or higher-dimensional space, two (non-aligned) vectors will span a plane with a particular orientation. To see this better, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. You can point the whole rigid assembly in any direction you like. The angle between them will remain the same. Place a card on top of the pencils, slipping it between your pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determine the orientation of the surface. This simple fact will be extremely important later on. You could replace the pencils with line segments drawn on the card underneath each pencil. Now you have the angle readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors. Notice that you can also lay a card along a single vector. What’s different here is that you can roll the card around the pencil; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not determine uniquely the orientation of the planar surface in which the two vectors can reside. But with two fixed vectors, there is only one such surface. 42.3 Functions as vectors In Section 8.3 we looked at the use of the exponential function to describe the temperature of hot water cooling to room temperature. The exponential decreasing function is of course \\(e^{-kt}\\) and we can find \\(k\\) by estimating the half-life of exponential decay, which is about 36 minutes in the data we used. (Figure ??). In this section, we’re going to think about functions in terms of vectors. There are huge advantages to thinking in this way, but it will take some time for you to see them clearly. Recall the data on cooling water gave the temperature (in degrees C) versus time (in minutes). Economy on the page argues against showing all the rows of the CoolingWater data frame, but you have access to it in the SANDBOX. time temp 0 98.2 2 91.4 4 86.9 6 83.1 9 78.0 14 71.4 19 66.3 29 58.6 39 53.0 49 48.8 74 41.2 99 36.2 124 32.7 149 30.1 174 28.2 The temp column of the data frame is a set of numbers, hence interpretable as a vector. As a vector in 15 dimensions, there’s not much to be said about its direction, but the length is easy enough: take the square root of the sum of squares of the components. That comes to 251.3 degrees C. This is merely an arithmetic fact, the consequence of adding together 15 numbers. It has the dimension of temperature T, but has nothing to do with the hot-oven-like 251 C temperature. This temp vector is playing to role of the buried treasure; it’s the destination we want to reach. What have we got to reach it with? This is where our basic modeling function \\(e^{-kt}\\) comes in. By evaluating \\(e^{-kt}\\) at each of the values of \\(t\\) in the time column, we create another column, which we’ll call expkt. Knowing that the half-life is about 36 minutes, we’ll use \\(k=\\ln(2)/36 \\approx 0.02\\). At the same time, anticipating what is to come, we’ll add another column which we’ll call, following statistical practice, the intercept column and which represents the constant function (evaluated at each of the times). CW &lt;- CW %&gt;% mutate(expkt = exp(-0.02*time), intercept = 1) time temp expkt intercept 0 98.2 1.0000000 1 2 91.4 0.9607894 1 4 86.9 0.9231163 1 6 83.1 0.8869204 1 9 78.0 0.8352702 1 14 71.4 0.7557837 1 19 66.3 0.6838614 1 29 58.6 0.5598984 1 39 53.0 0.4584060 1 49 48.8 0.3753111 1 74 41.2 0.2276377 1 99 36.2 0.1380692 1 124 32.7 0.0837432 1 149 30.1 0.0507928 1 174 28.2 0.0308074 1 Confirm for yourself that the expkt column really does match an exponential decay with a half-life of about 36 minutes. You can see that at time zero the value of expkt, just as expected for an exponential. At time 36, somewhere between the rows for times 29 and 39, the value is about 0.5. At time 72—two half-lives after the start—the value should be 0.25, closely matching the recorded temperature at time 74. The two vectors expkt and intercept, like any two (non-aligned) vectors, span a planar subspace. Since expkt and intercept are embedded in a 15-dimensional space—this is just saying that there are 15 rows in the data frame—the plane is a subspace of the 15-dimensional space. This statement can seem hopelessly abstract, so let’s try to give a more concrete visualization. For the visualization, we’ll move onto the familiar ground of a graph of temperature versus time. In the domain of temperature vs time, each of the linear combinations \\(\\alpha\\)expkt\\(\\ + \\beta\\)intercept, appears as a set of 15 dots. Figure 42.5 shows three such sets of 15 does in three different colors, along with the 15 points of the actual temperature data. To show more than the three sets of dots would be visually confusing. Instead, we’ll add to the graph functions of the form \\(\\alpha\\, e^{-0.02 t} + \\beta\\, 1\\). Figure 42.5: Some of the possible linear combinations of the vectors expkt and intercept. Each combination is a set of 15 dots, but many of them are shown here as continuous functions that would connect the dots for that particular linear combination. We we to plot all the linear combinations of expkt and intercept, the graphics frame would be completely covered with ink. But each individual vector produced by a linear combination will look much of a kind with the ten shown here. The functions shown in Figure 42.5 all inhabit the two-dimensional subspace spanned by expkt and intercept. But there is a lot more to the 15 dimensional space. What do functions look like that inhabit the space outside of the two-dimensional expkt&amp;intercept subspace? Figure 42.6 shows a handful of them. Each is different in kind from the functions shown in Figure 42.5. Figure 42.6: A handful of the vectors in the 15-dimensional space outside of the 2-dimensional expkt&amp;intercept subspace. There are a lot of crazy-looking functions out there in 15-dimensional space! Look back at Figure 42.5 and focus your attention on the function drawn in black. That function is a reasonable match to the data (plotted as black dots). The question we face now is how to find such a function by searching through a 15-dimensional space. That’s the task we take on in the next chapter. It’s pretty easy to visualize the length of a vector and the arithmetic is straightforward even in n-dimensional space. For a vector \\(\\vec{v}\\) with components \\[\\vec{v} \\equiv \\left[\\strut \\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{array}\\right]\\] the length is \\(\\sqrt{\\strut v_1^2 + v_2^2 + \\cdots + v_n^2}\\). Similarly, the dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is \\(\\vec{v} \\cdot \\vec{w} \\equiv v_1 w_1 + v_2 w_2 + \\cdots + v_n w_n\\). In terms of the dot product, the vector length is \\(\\sqrt{\\strut\\vec{v} \\cdot \\vec{v}}\\). And by combining the dot products, we can calculate the angle between two vectors: \\[\\cos(\\theta_{v,w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{\\sqrt{\\strut (\\vec{v}\\cdot\\vec{v})\\ (\\vec{w}\\cdot\\vec{w}})}\\] As you know, a vector has two properties: length and direction. Both of these can be calculated using the dot product. This suggests a way to consider other mathematical objects, such as functions, as vectors. All we need is a reasonable definition for a dot product. Suppose we have two functions, \\(f(t)\\) and \\(g(t)\\) defined on some domain, say \\(0 &lt; t &lt; 2\\pi\\). A dot product multiplies the objects together and accumulates the products. When the vectors are sets of numbers, the accumulation is to add up component-by-component products. By analogy, to take the dot product of \\(f(t)\\) and \\(g(t)\\) over the domain, we can do an integral: \\[f() \\cdot g() \\equiv \\int_0^{2\\pi} f(t) g(t) dt\\ .\\] To illustrate, consider these two functions: \\(f(t) \\equiv \\sin(t)\\) and \\(g(t) \\equiv \\sin(2 t)\\), plotted below. How “long” are \\(f(t)\\) and \\(g(t)\\)? ` Integrate(sin(t) * sin(t) ~ t, domain(t = 0:(2 * pi))) %>% sqrt()        \\(1.7725\\) Integrate(sin(2 * t) * sin(2 * t) ~ t, domain(t = 0:(2 * pi))) %>% sqrt()        \\(1.7725\\) They are both the same “length”, which you might recognize as \\(\\sqrt{\\strut\\pi}\\). What’s the cosine of the angle between them? Integrate(sin(t) * sin(2 * t) ~ t, domain(t = 0:(2 * pi)))/pi        \\(0\\) The cosine of the angle is zero—1.3e-16 is just a round-off error. That means that the functions \\(\\sin(t)\\) and \\(\\sin(2t)\\) are orthogonal on the domain \\(0 &lt; t &lt; 2\\pi\\). 42.4 Matrices We have been writing linear combinations of vectors in this format, where \\(a\\) and \\(b\\)are scalars: \\[a \\vec{u} + b \\vec{v}\\ .\\] It’s helpful to have a standard notation that makes it evident which things are being combined. A matrix is a collection of vectors, all of which live in the same dimensional space. For instance, the matrix containing \\(\\vec{u}\\) and \\(\\vec{v}\\) looks like \\[\\left[\\begin{array}{cc}|&amp;|\\\\\\vec{u} &amp; \\vec{v} \\\\|&amp;|\\\\ \\end{array}\\right]\\] The vertical lines are meant to indicate that each of \\(\\vec{u}\\) and \\(\\vec{v}\\) are a column in the matrix. For instance, supposing \\[\\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ ,\\] the matrix \\(\\mathbf A\\) containing \\(\\vec{u}\\) and \\(\\vec{v}\\) consists of two columns: \\[ {\\mathbf A} \\equiv \\left[\\overset{\\strut}{\\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]}\\ \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\ \\right]\\] In this book, when we write the name of a matrix, we’ll use a CAPITAL bold-face letter, as M. Conventionally, we do without the interior square brackets, writing \\[ {\\mathbf A} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] In scientific computing languages, there is almost always a type of object called a “matrix” or an “array.” The one shown here, \\({\\mathbf A}\\) has 3 rows and 2 columns. (Columns always run vertically, as with the columns in a building.) Also, as a matter of convention, the linear combination of the vectors in a matrix is indicated by placing to the right of the matrix a column containing the coefficients of the combination \\(3 \\vec{u} + 1 \\vec{v}\\), as with \\[ {\\mathbf A} \\left[\\begin{array}{c}3\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right] \\left[\\begin{array}{c}3\\\\1\\end{array}\\right] = \\left[\\begin{array}{r}7\\\\11\\\\-9\\end{array}\\right]\\] In R, we make vectors with the rbind() command, that places its arguments in a row. We’ll make matrices with the cbind() command, which places its arguments in a column. Of course, the r in rbind() stands for “row” while the c in cbind() stands for “column.” u &lt;- rbind(2, 5, -3) v &lt;- rbind(1, -4, 0) A &lt;- cbind(u, v) A ## [,1] [,2] ## [1,] 2 1 ## [2,] 5 -4 ## [3,] -3 0 We take a linear combination of the vectors in a matrix by writing down the coefficients in the same style as a vector and multiplying using %*%: c &lt;- rbind(3, 1) A %*% c ## [,1] ## [1,] 7 ## [2,] 11 ## [3,] -9 Often, we’ll want to make several different linear combinations of the vectors in a matrix. Since each individual linear combination is specified with a column of coefficients, the convention is to place the several sets of coefficients as side-by-side columns, that is, as a matrix. For example, consider these three different linear combinations of the two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\): \\[\\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\ \\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right] \\left[\\begin{array}{rr}3 &amp; 0 &amp; -1\\\\1 &amp; 2 &amp; 0\\end{array}\\right] = \\left[\\begin{array}{r}7 &amp;2 &amp;-2\\\\11 &amp; -8 &amp; -5\\\\-9 &amp; 0 &amp; 3\\end{array}\\right]\\] In R, the left-hand side of the equation would involve constructing a matrix with the three pairs of coefficients: X &lt;- cbind( rbind(3, 1), rbind(0, 2), rbind(-1, 0) ) A %*% X ## [,1] [,2] [,3] ## [1,] 7 2 -2 ## [2,] 11 -8 -5 ## [3,] -9 0 3 A CATEGORICAL VARIABLE AND HOW IT TRANSLATES INTO A SET OF VECTORS. 42.5 Exercises Exercise XX.XX: 9bAVr2 Locating WW I aircraft The photograph shows part of an aircraft detection system from World War I. The concrete block is an “acoustic mirror.” Its purpose is to collect and reflect sounds from an aircraft, concentrating them at a point where they can be picked up by a microphone. Moving the microphone to a point where the concentrated sound is strongest allows the aircraft’s bearing to be identified, helping observers acquire the aircraft visually. Source With two or more such acoustic mirrors, the location of the aircraft can be identified. Question A Give the position of the aircraft as a multiple of \\(\\vec{a}\\) from sound mirror A and as a multiple of \\(\\vec{b}\\) from sound mirror B. (Choose the closest answer) \\(3 \\vec{a}\\) and \\(4.5 \\vec{b}\\)Right!  \\(4 \\vec{a}\\) and \\(6 \\vec{b}\\)︎✘ \\(3\\vec{a}\\) and \\(2 \\vec{b}\\)︎✘ \\(4\\vec{a}\\) and \\(4.5\\vec{b}\\)︎✘ Exercise XX.XX: dIobrt Each of the diagrams below consists of two vectors and a target point (denoted with a bull’s eye). Being a point, the target has a definite location relative to the coordinate axes. “Solving” this sort of system amounts to finding a linear combination of the two vectors whose result is a vector that can connect the origin to the target. (We call this a target problem.) Copy over each diagram to your paper. (Any reasonable approximation will do.) Then, for each diagram, find the linear combination of the two vectors that will reach from the origin to the target. Show your work, meaning that you should draw the scaled vectors in a proper position that demonstrates that they do indeed connect the origin to the target. Underneath the diagram, write down the numerical value of the scalars in the linear combination. (Again, any reasonable approximation will do.) Exercise XX.XX: DPY4Ue In this exercise, you are going to use R to find solutions to the target problem. Each question poses one target problem. You could solve these problems by eye, but we want to get you started on the computer solution so that you’ll be ready to solve harder problems that must be worked on the computer. The vectors you will be working with are: \\[\\vec{a} \\equiv \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{b} \\equiv \\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\ \\ \\ \\ \\vec{c} \\equiv \\left(\\begin{array}{c}1\\\\-2\\end{array}\\right)\\ \\ \\ \\ \\vec{d} \\equiv \\left(\\begin{array}{c}-6\\\\2\\end{array}\\right)\\ \\ \\ \\ \\vec{T} \\equiv \\left(\\begin{array}{c}3\\\\-1\\end{array}\\right)\\ \\ \\ \\ \\] To make your life easier, here are the commands for defining these vectors. One of the commands is wrong. You’ll have to correct it before moving on the the rest of the problem. NEED TO BREAK ONE OF THESE a &lt;- rbind(1, 2) b &lt;- rbind(1, 1) c &lt;- rbind(1, -1) d &lt;- rbind(-6, 2) T &lt;- rbind(3, -1) Question A What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{b}\\) to reach the target $ ec{T}? \\(-4 \\vec{a} + 7 \\vec{b}\\)Good.  \\(-2 \\vec{a} + 5 \\vec{b}\\)︎✘ \\(2 \\vec{a} -7 \\vec{b}\\)︎✘ \\(4 \\vec{a} - 5 \\vec{b}\\)︎✘ Question B What what is the correct linear combination of \\(\\vec{b}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{3}\\vec{b} + \\frac{4}{3} \\vec{c}\\)Correct.  \\(\\frac{4}{3}\\vec{b} + \\frac{7}{2} \\vec{c}\\)︎✘ \\(\\frac{5}{2}\\vec{b} + \\frac{4}{5} \\vec{c}\\)︎✘ \\(\\frac{2}{3}\\vec{b} + \\frac{9}{4} \\vec{c}\\)︎✘ Question C What what is the correct linear combination of \\(\\vec{c}\\) and \\(\\vec{d}\\) to reach the target $ ec{T}? \\(\\frac{0}{3}\\vec{c} + \\frac{4}{5} \\vec{d}\\)Right!  \\(\\frac{1}{3}\\vec{c} + \\frac{1}{3} \\vec{d}\\)︎✘ \\(\\frac{1}{2}\\vec{c} + \\frac{5}{7} \\vec{d}\\)︎✘ \\(\\frac{0}{3}\\vec{c} + \\frac{1}{2} \\vec{d}\\)︎✘ Question D What what is the correct linear combination of \\(\\vec{a}\\) and \\(\\vec{c}\\) to reach the target $ ec{T}? \\(\\frac{5}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)Good.  \\(\\frac{5}{4}\\vec{a} + \\frac{5}{4} \\vec{c}\\)︎✘ \\(\\frac{7}{4}\\vec{a} + \\frac{9}{4} \\vec{c}\\)︎✘ \\(\\frac{3}{4}\\vec{a} + \\frac{7}{4} \\vec{c}\\)︎✘ "],["projection-residual.html", "Chapter 43 Projection &amp; residual 43.1 Projecting a vector onto another vector 43.2 Projection onto a set of vectors 43.3 Exercises 43.4 MOVED FROM VECTORS 43.5 Matrix 43.6 Scalar multiplication 43.7 Dot product and %*% 43.8 Vector lengths 43.9 Outline", " Chapter 43 Projection &amp; residual In the previous chapter, we took linear combinations of vectors. Using the notation of matrices, we solved the problem \\[\\text{previous chapter:}\\ \\ {\\mathbf A}\\, \\vec{x} = \\vec{?}\\] where \\(\\vec{x}\\) is a column of coefficients, one coefficient for each of the columns in the matrix \\({\\mathbf A}\\). Finding \\(\\vec{?}\\) is a simple, if somewhat tedious, matter of multiplication. \\[\\text{this chapter:}\\ \\ {\\mathbf A}\\, \\vec{?} = \\vec{b}\\] In other words, the task of this chapter is to show how to solve \\({\\mathbf A} \\vec{x} = \\vec{b}\\) when we know the result of the linear combination \\(\\vec{b}\\), and the set of vectors being combined, \\(\\mathbf M\\), but we don’t yet know \\(\\vec{x}\\). As motivation for this “find \\(\\vec{x}\\)” problem, we refer you to Figure 42.5 which showed the temperature-vs-time data from the CoolingWater data frame. That figure shows several possible linear combinations of the vectors \\(u(t) \\equiv e^{-0.02 t}\\) (we called this expkt) and \\(v(t) \\equiv 1\\) (we called this intercept). Suppose we seek to find the particular linear combination of \\(u(t)\\) and \\(v(t)\\) that comes as close as possible to the black dots in the figure. That is, we know \\({\\mathbf A}\\): the two columns expkt and intercept from the data frame, and we know \\(\\vec{b}\\): the column temp from the data frame. This sort of problem is extremely common and important throughout quantitative fields of all sorts, from astronomy to zoology, and is one of the foundation techniques in statistics and data science. We’ll present the approach graphically, algorithmically, and computationally. Let’s highlight the three different modes in which we’re going to introduce the topic of projection and residual. Ultimately, in your professional work, you will rely on computation, getting the computer to do the work. For us in this chapter, the basic R functions you will use are rbind(), cbind(), qr.solve(), and “matrix multiplication” %*%, as well as the usual addition, subtraction, division, square root, and scalar multiplication.1 As we introduce them, you will want to make sure you know exactly what each of rbind(), cbind(), qr.solve(), and %*% are used for. In the world of linear combinations, these are fundamental operations. algorithmic describes a process of carrying out an operation. The R operators qr.solve() and %*% (and others!) are implemented as software. That software describes a series of steps to accomplish the operation. With linear combinations, most of those steps are arithmetic as with the dot product and vector length. We will show the arithmetic steps for the vector operations in the simple case of one, two, or three vectors. The generalization to more vectors requires the organizational principles provided by computer languages, and this is not a course in computer programming. By rendering the operations of linear combinations in arithmetic, we hope to provide a better understanding of what’s happening in the operations so you can be an aware consumer of the output of computer programmers. geometrically in two and three dimensions. We humans have powerful intuitive abilities to make sense of simple geometric constructions. Since many people are able to assimilate simple geometry more readily than arithmetic, the geometric presentation provides a good alternative route to understanding the mathematics of linear combinations, even though it’s restricted to the simple situations of two and three dimensional space. Again, ultimately professionals use the computational tools for working out tasks in linear combinations. You will use those tools better if you can think about them in terms of arithmetic and geometry. 43.1 Projecting a vector onto another vector We’ll start with one of the most fundamental operations: projecting one vector onto another. The geometric situation is shown in Figure 43.1. In our notation, we will always write the vector being projected as \\(\\vec{b}\\) or some variation of the letter “b.” The vectors onto which \\(\\vec{b}\\) is being projected will be written \\(\\vec{u}\\), \\(\\vec{v}\\), and so on. In this section, there is only a single vector being projected onto, and we’re calling it \\(\\vec{u}\\). Figure 43.1: Two vectors. Vector \\(\\vec{b}\\) is to be projected onto vector \\(\\vec{u}\\). A physical analogy for projection is the casting of shadows on a screen in the same manner as an old-fashioned slide projector or movie projector. The vector \\(\\vec{u}\\) is the screen. The light source is arranged to generate parallel rays which arrive perpendicularly to the screen. In this situation, the shadow cast by \\(\\vec{b}\\) on the screen \\(\\vec{u}\\) is the “projection of \\(\\vec{b}\\) onto \\(\\vec{u}\\).” In terms of vector-language, the projection of \\(\\vec{b}\\) onto \\(\\vec{u}\\) is the vector \\(\\alpha\\,\\vec{u}\\) where the scalar multiplier \\(\\alpha\\) is selected to place \\(\\alpha\\,\\vec{u}\\) as close as possible to \\(\\vec{b}\\). What do we mean exactly by “as close as possible?” We need to render this into vector-language. We’ll do this by a combination of length and vector subtraction. For any scalar value of \\(\\alpha\\), there is a vector \\(\\vec{b} - \\alpha\\, \\vec{u}\\). Figure 43.2 shows the situation for a handful of candidates for \\(\\alpha\\). As always, the vector \\(\\alpha\\,\\vec{u}\\) lies on the linear subspace defined by \\(\\vec{u}\\). Supposing that \\(\\alpha = 2\\), corresponding to the blue dot in Figure 43.2, the vector \\(\\vec{b} - \\alpha\\,\\vec{u}\\) is the vector extending from the blue dot to the tip of \\(\\vec{b}\\). Figure 43.2: The vector \\(\\alpha\\,\\vec{u}\\) is shown for several values of \\(\\alpha\\)$, each value being marked by a colored dot. (The values shown are roughly $lpha = $1.1 (orange), 1.3 (yellow), 1.6 (red), and 2.0 (blue).) The vector \\(\\vec{b} - \\alpha\\,\\vec{u}\\) for each of the colors is indicated by a dashed line connecting the tip of \\(\\alpha\\,\\vec{u}\\) to the tip of \\(\\vec{b}\\). In this context, “as close as possible” means the value of \\(\\alpha\\) that makes the length \\(\\| \\vec{b} - \\alpha\\,\\vec{u} \\|\\) as small as possible. It’s possible for many people to find the argmin \\(\\alpha^\\star\\) by eye. In the Figure, the orange and blue dots are clearly not the best. Yellow and red are about the same length, with yellow winning by a hair. The vector \\(\\vec{b} - \\alpha^\\star\\,\\vec{u}\\) will always be perpendicular to \\(\\vec{u}\\). In other words, the three vectors \\(\\vec{b}\\), \\(\\alpha^\\star\\,\\vec{u}\\), and \\(\\vec{b}-\\alpha^\\star\\,\\vec{u}\\) will constitute the sides of a right triangle, with \\(\\vec{b}\\) as the hypothenuse. This gives an important clue to finding \\(\\alpha^\\star\\) arithmetically. The length of \\(\\alpha^\\star\\, u\\) will be, using the trigonometry of right triangles, \\(\\|\\vec{b}\\| \\cos(\\theta)\\), where \\(\\theta\\) is the angle between \\(\\vec{u}\\) and \\(\\vec{b}\\). Recall that \\[\\cos(\\theta) = \\frac{\\vec{u}\\cdot \\vec{b}}{\\|\\vec{u}\\|\\ \\|\\vec{b}\\|}\\ .\\] Consequently, \\[\\|\\vec{b}\\| \\cos(\\theta) = \\frac{\\vec{u}\\cdot \\vec{b}}{\\|\\vec{u}\\|}\\] The above quantity is a length. But the projection of \\(\\vec{b}\\) onto \\(\\vec{u}\\) will be a vector. Since we know \\(\\vec{u}\\), we know the direction of the result of the projection. and the previous formula gives the length. We can create the unit-length vector pointing in the direction of \\(\\vec{u}\\) by appropriate scaling: \\(\\vec{u} / \\|\\vec{u} \\|\\). So the projection of \\(\\vec{b}\\) onto \\(\\vec{u}\\), which we’ll denote by \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) is \\[\\overset{\\longrightarrow}{{b}\\|_{u}} = \\underbrace{\\frac{\\vec{u}\\cdot\\vec{b}}{\\|\\vec{u}\\|}}_\\text{length}\\underbrace{\\frac{\\vec{u}}{\\|\\vec{u} \\|}}_\\text{direction} = \\frac{\\vec{u}\\cdot\\vec{b}}{\\vec{u}\\cdot\\vec{u}} \\vec{u} = \\alpha^\\star\\,\\vec{u}\\] Read \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) as “the component of \\(\\vec{b}\\) parallel to \\(\\vec{u}\\),” or as “\\(\\vec{b}\\) projected onto \\(\\vec{u}\\).” Remember that the vector \\(\\vec{b} - \\alpha^\\star\\, \\vec{u}\\) is perpendicular to \\(\\vec{u}\\) and connects \\(\\overset{\\longrightarrow}{b \\|_{u}}\\) to \\(\\vec{b}\\). This vector, called the residual vector in statistics, we will denote with \\[\\overset{\\longrightarrow}{b\\! \\perp_u}\\] which you can pronounce as “the component of \\(\\vec{b}\\) perpendicular to \\(\\vec{u}\\),” or as “the residual from projecting \\(\\vec{b}\\) onto \\(\\vec{u}\\).” Adding up the projection of \\(\\vec{b}\\) onto \\(\\vec{u}\\) and the residual vector from that projection reproduces the original \\(\\vec{b}\\): \\[\\overset{\\longrightarrow}{b\\|_u}\\ +\\ \\overset{\\longrightarrow}{b\\! \\perp_u} = \\vec{b}\\ .\\] MOVE THESE FUNCTIONS to {mosaicCalc}. We’re going to give the students some training wheels `%dot%` &lt;- function(u, v) { sum(u * v) } `%onto%` &lt;- function(b, A) { A %*% qr.solve(A, b) } `%perp%` &lt;- function(b, A) { b - (b %onto% A) } as_unit &lt;- function(A) { helper &lt;- function(v) { v / sqrt(sum(v^2)) } apply(A, 2, helper) } as_mag &lt;- function(A, method=c(&quot;2&quot;, &quot;O&quot;, &quot;I&quot;, &quot;F&quot;, &quot;M&quot;)) { method &lt;- match.arg(method) helper &lt;- function(v) {Matrix::norm(v, type=method)} apply(A, 2, helper) } Using the geometry of vectors and a little bit of trigonometry, we have now created the arithmetic formulas needed to: Project a vector \\(\\vec{b}\\) onto another vector \\(\\vec{u}\\), and Find the residual vector from the projection. To help you make the transition from arithmetic to computation, we provide you with two training-wheel functions, %onto% and %perp. Use them like this: b &lt;- rbind(3, 1, 7, 4) u &lt;- rbind(1,1,1,1) b %onto% u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}3.75\\\\ 3.75\\\\ 3.75\\\\ 3.75\\\\\\end{array}\\right]\\) b %perp% u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}-0.75\\\\ -2.75\\\\ 3.25\\\\ 0.25\\\\\\end{array}\\right]\\) Example 43.1 Using %dot%, %perp% and %onto% and two vectors \\(\\vec{b}\\) and \\(\\vec{u}\\) of your own choosing, demonstrate that \\[\\overset{\\longrightarrow}{b\\|_u}\\ +\\ \\overset{\\longrightarrow}{b\\! \\perp_u} = \\vec{b}\\|\\] and that \\(\\overset{\\longrightarrow}{b\\|_u}\\) is perpendicular to \\(\\overset{\\longrightarrow}{b\\! \\perp_u}\\). For instance, # create your own vectors! b &lt;- rbind(8, -3, 2, 1, 4) u &lt;- rbind(1, 2, 1, 2, 3) # demonstrate properties projected &lt;- b %onto% u residual &lt;- b %perp% u projected + residual        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}8\\\\ -3\\\\ 2\\\\ 1\\\\ 4\\\\\\end{array}\\right]\\) projected %dot% residual        \\(0\\) Example 43.2 Show that \\(\\overset{\\longrightarrow}{b\\|_u}\\) is parallel to \\(\\vec{u}\\). Two vectors are parallel if one of them is a scalar multiple of the other. For instance, suppose we have three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\), like this: u &lt;- rbind(1, 2, 3) v &lt;- rbind(2, 4, 6) b &lt;- rbind(3, 1, 2) Notice that \\(\\vec{v} = 2 \\vec{u}\\), that is, \\(\\vec{u}\\) and \\(\\vec{v}\\) are parallel. Here’s what we get when we divide \\(\\vec{v}\\) by \\(\\vec{u}\\) on a component-by-component basis: v/u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}2\\\\ 2\\\\ 2\\\\\\end{array}\\right]\\) Every component of \\(\\vec{v}\\) is the same multiple of the corresponding component of \\(\\vec{u}\\). On the other hand, \\(\\vec{b}\\) is not a scalar multiple of \\(\\vec{u}\\). Consequently: b/u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}3\\\\ 0.5\\\\ 0.6667\\\\\\end{array}\\right]\\) The component-by-component multipliers are not all the same. Now to show \\(\\overset{\\longrightarrow}{b\\|_u}\\) is parallel to \\(\\vec{u}\\). First, we’ll compute \\(\\overset{\\longrightarrow}{b\\|_u}\\) and then divide it, component-by-component, by \\(\\vec{u}\\) (b %onto% u)/u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.7857\\\\ 0.7857\\\\ 0.7857\\\\\\end{array}\\right]\\) In contrast, \\(\\overset{\\longrightarrow}{b\\!\\perp_u}\\) is, obviously, not parallel to \\(\\vec{u}\\): (b %perp% u)/u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}2.2143\\\\ -0.2857\\\\ -0.119\\\\\\end{array}\\right]\\) Example 43.3 Find the scalar multiplier \\(\\alpha^\\star\\) such that \\[\\overset{\\longrightarrow}{b\\!\\perp_u} = \\alpha^\\star\\,\\vec{u}\\ .\\] This is just a matter of dividing \\(\\overset{\\longrightarrow}{b\\!\\perp_u}\\) by \\(\\vec{u}\\) on a component-by-component basis: (b %onto% u)/u        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.7857\\\\ 0.7857\\\\ 0.7857\\\\\\end{array}\\right]\\) We see that \\(\\alpha^\\star = 0.7857143\\). You should commit the commands used in the previous demonstrations to memory, making sure you understand what each of the commands in the demonstration is doing. 43.2 Projection onto a set of vectors As we have just seen, projecting a target \\(\\vec{b}\\) onto a single vector is a matter of arithmetic: \\[\\overset{\\longrightarrow}{b\\|_u} = \\frac{\\vec{u}\\cdot\\vec{b}}{\\vec{u}\\cdot\\vec{u}} \\vec{u}\\] Now we take on the problem of projecting the target \\(\\vec{b}\\) onto two vectors simultaneously. Following our previous convention, The vectors to be projected onto will be \\(\\vec{u}\\) and \\(\\vec{v}\\). We’ll package those vectors into a matrix \\(\\mathbf A\\), that is, \\[{\\mathbf A} \\equiv \\left[\\vec{u}, \\vec{v}\\right]\\ .\\] We’ll refer to the space spanned by the vectors in \\({\\bf A}\\) as \\(span(\\mathbf A)\\). Since there are two vectors in \\({\\bf A}\\), that space is a plane. Finding the linear combination of the vectors in \\(\\mathbf A\\) that comes as close as possible to \\(\\vec{b}\\) involves two tasks. One task is to project \\(\\vec{b}\\) onto \\(span(\\mathbf A)\\). The resulting vector, which we will denote as \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\), lives in \\(span(\\mathbf A)\\). (Of course it does, since \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\) was created by projecting \\(\\vec{b}\\) onto \\(\\mathbf A\\).) The other task is two express \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\) as a linear combination of the vectors in \\(\\mathbf A\\). We know we can do this exactly because \\(span(\\mathbf A)\\) is defined to be those vectors that can be created by a linear combination of the vectors in \\(\\mathbf A\\). The vector \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\) was created specifically to live in \\(span(\\mathbf A)\\). We’ll start the description of the technique for approximating \\(\\vec{b}\\) as a linear combination of the two vectors in \\(\\mathbf A\\) with a reduced problem in which \\(\\vec{b}\\) happens to be already in \\(span(\\mathbf A)\\). You may already have encountered the technique in your childhood reading. The problem appears in Robert Louis Stevenson’s famous novel, Treasure Island. The story is about the discovery of a treasure map indicating the location of buried treasure on the eponymous Island. There is a red X on the map labelled “bulk of treasure here,” but that is hardly sufficient to guide the dig for treasure. After all, every buried treasure needs some secret to protect it. On the back of the map is written a cryptic clue to the precise location: Tall tree, Spy-glass shoulder, bearing a point to the N. of N.N.E. Skeleton Island E.S.E. and by E. Ten feet. Skeleton Island is clearly marked on the map, as is Spy-glass Hill. The plateau marked by the red X “was dotted thickly with pine-trees of varying height. Every here and there, one of a different species rose forty or fifty feed clear above its neighbours.” But which of these was the “tall tree” mentioned in the clue. Figure 43.3: The map of Treasure Island. The heading ‘E.S.E. and by E.’ is marked with a solid black line starting at Skeleton Island. The heading ‘N. of N.N.E.’ is marked by dotted lines, one of which is positioned to point at the shoulder of Spy-glass Hill. Where the bearing from Skeleton Island meets the bearing to Spy-glass Hill will be the Tall tree. Long John Silver, obviously an accomplished mathematician, starts near Skeleton Island, moving on along the vector that keeps Skeleton Island to the compass bearing one point east of east-south-east. While on the march, he keeps a telescope trained on the shoulder of Spy-glass Hill. The goal When that telescope points one point north of north-north-east, they are in the vicinity of a tall tree. That’s the tree matching the clue. The vectors in Treasure Island are embedded on the Earth’s surface, the set of positions spanned by latitude and longitude. Figure 43.4 shows a slightly more general problem in a two-dimensional space, where the vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) are not orthogonal. The task, still, is to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that will match \\(\\vec{b}\\). The diagram shows the \\(\\vec{u}\\) vector and the subspace aligned with \\(\\vec{u}\\), and similarly for \\(\\vec{v}\\) Figure 43.4: The telescope method of solving projection onto two vectors. The algorithm is based in Long John Silver’s technique. Pick either \\(\\vec{u}\\) or \\(\\vec{v}\\), it doesn’t matter which. In the diagram, we’ve picked \\(\\vec{v}\\). Align your telescope with that vector. Now march along the other vector, \\(\\vec{u}\\), carefully keeping the telescope on the bearing aligned with \\(\\vec{v}\\). From the diagram, you can see that when you’ve marched to \\(\\frac{1}{2} \\vec{u}\\), the telescope does not yet have \\(\\vec{b}\\) in view. Similarly, at \\(1 \\vec{u}\\), the target \\(\\vec{b}\\) isn’t yet visible. Marching a little further, to about \\(1.6 \\vec{u}\\) brings you to the point in the \\(\\vec{u}\\)-subspace where the target falls into view. This tells us that the coefficient on \\(\\vec{u}\\) will be 1.6. To find the coefficient on \\(\\vec{v}\\), you’ll need to march along the line of the telescope, taking steps of size \\(\\|\\vec{v}\\|\\). In the diagram, we’ve marked the march with copies of \\(\\vec{v}\\) to make the counting easier. We’ll need to march opposite the direction of \\(\\vec{v}\\), so the coefficient will be negative. Taking 2.8 steps of size \\(\\|\\vec{v}\\|\\) brings us to the target. Thus: \\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\] We were able to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\)—the vectors making up matrix \\(\\mathbf A\\)—because, as on Treasure Island, \\(\\vec{b}\\) happened to live in \\(span(\\mathbf A)\\). In a complete algorithm, we would have to project \\(\\vec{b}\\) onto \\(span(\\mathbf A)\\) before we started our march in \\(span(\\mathbf A)\\). Also, to make the algorithm general, we need to render it into a form that will work in any dimensional space. This means that telescopes and dotted lines are out, and we’ll have to work with arithmetic. Figure 43.5 the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\) in a three dimensional space. It turns out that the algorithm we will develop for this case is perfectly general, it will work in any-dimensional space. As before, we seek a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) which will match \\(\\vec{b}\\) as closely as possible. To acquaint yourself with the geometry, rotate the diagram and perform these experiments. \\(\\vec{u}\\) and \\(\\vec{v}\\) are fixed in length. However, their lengths will appear to change as you rotate the space. This might be called the “gun-barrel” effect; a tube looks very short when you look down it’s longitudinal axis, but looks longer when you look at it from the side. Rotate the space until both \\(\\vec{u}\\) and \\(\\vec{v}\\) reach their maximum apparent length. The viewpoint that accomplishes this is looking downward perpendicularly onto the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Vector \\(\\vec{b}\\) is not in that plane, but from the downward perpendicular viewpoint, you can see \\(\\widehat{b}\\), the point in the plane where the projection of \\(\\vec{b}\\) will fall. From this viewpoint, it’s very easy to find the linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that reaches Again rotate the space until the vector \\(\\vec{u}\\) is pointing straight toward you. You’ll see only the arrowhead of \\(\\vec{u}\\), but you’ll be able to figure out how many steps along \\(\\vec{v}\\) will be the projection of \\(\\vec{b}\\) onto \\(\\vec{v}\\). The plane you are looking downward onto is the plane spanned by \\(\\left[\\vec{v}, \\vec{b}\\right]\\). Again rotate the space to look straight down vector \\(\\vec{v}\\). From this perspective, you can calculate how many steps along \\(\\vec{v}\\) will be the projection of \\(\\vec{b}\\) onto \\(\\vec{v}\\). Figure 43.5: Showing the relative orientation of the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\). Drag the image to rotate it. Many people have difficulty with a bare vector diagram in 3-space. Much of our visual ability to see in three dimensions has to do with shape, shadow, and, to some extent, our stereo vision (which doesn’t apply to projection of an image onto the flat space of a screen or page of a book). Figure 43.6 shows the same diagram as the previous, but a translucent plane has been placed onto onto the \\(\\vec{u}\\) and \\(\\vec{v}\\) vectors to mark \\(span(\\mathbf A)\\). Try the rotations as before, but this time you will have the subspace to refer to visually. Figure 43.6: The same three vectors as in Figure 43.5, but now the space spanned by \\(\\left[\\vec{u}, \\vec{v}\\right]\\) marked with a translucent plane. Figure 43.4 shows the general case with vector \\(\\vec{b}\\) to be approached by a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\). With two vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) in two-dimensional space, there is a linear combination that will match any \\(\\vec{b}\\) in that space. A more general situation is an \\(n\\)-dimensional space with only \\(p &lt; n\\) vectors to form the linear combination. Typically, the target \\(\\vec{b}\\) will not be in the subspace spanned by the p vectors. We can illustrate using 3-dimensional space and two vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) to be linearly combined. b %onto% A        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.5\\\\ 2\\\\ 0\\\\\\end{array}\\right]\\) b %perp% A        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0\\\\ 0\\\\ 2.5\\\\\\end{array}\\right]\\) Figure 43.7: This diagram needs to show b projected and the residual vector. Of course, a set of vectors is simply a matrix, so we’ll cast the problem as one of projecting \\(\\vec{b}\\) onto a matrix \\(\\mathbf A\\). There is one case that is extremely simple: when the vectors in \\(\\mathbf A\\) are mutually orthogonal. Let’s make sure we understand this case well. The geometry is simple, as in Figure 43.8. Figure 43.8: Projecting \\(\\vec{b}\\) onto two orthogonal vectors. REPLACE THIS WITH THE ACTUAL IMAGE. To demonstrate the projection a bit more generally, let’s set up 3 orthogonal vectors in a four-dimensional space as \\(\\mathbf A\\): u &lt;- rbind(2, 3, 0, 6) v &lt;- rbind(0,-2,-2, 1) w &lt;- rbind(3,-2, 2, 0) b &lt;- rbind(1, 1, 1, 1) A &lt;- cbind(u, v, w) You should be able to confirm with simple arithmetic that \\(\\vec{u}\\) is orthogonal to \\(\\vec{v}\\), that \\(\\vec{u}\\) is orthogonal to \\(\\vec{w}\\), and that \\(\\vec{w}\\) is orthogonal to \\(\\vec{v}\\). (Hint: Use the dot product.) You can also see that \\(\\vec{b}\\) is not parallel to any one of the three columns in \\({\\mathbf A}\\). We’ll compute the correct answer and then see how we could do it with simple arithmetic. b %onto% A        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.9784\\\\ 0.9872\\\\ 1.0196\\\\ 1.0136\\\\\\end{array}\\right]\\) qr.solve(A, b)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.2245\\\\ -0.3333\\\\ 0.1765\\\\\\end{array}\\right]\\) This is telling us that \\[\\overset{\\longrightarrow}{b\\|_{\\mathbf A}} = \\left[\\begin{array}{r}0.9783914 \\\\0.9871949 \\\\1.0196078 \\\\1.0136054\\end{array}\\right] = 0.2244898 \\vec{u} - 0.3333333 \\vec{v} + 0.1764706 \\vec{w}\\] We can find the coefficients of the linear combination with simple, independent uses of the formula for projecting \\(\\vec{b}\\) onto each of the columns of \\({\\mathbf A}\\) one at a time: (b %dot% u)/(u %dot% u)        \\(0.2245\\) (b %dot% v)/(v %dot% v)        \\(-0.3333\\) (b %dot% w)/(w %dot% w)        \\(0.1765\\) This simple procedure of independent projections does not work if the columns of \\(\\mathbf A\\) are not mutually orthogonal*. For instance: u &lt;- rbind(1,2,3,4) v &lt;- rbind(4,3,2,5) A2 &lt;- cbind(u, v) The right coefficients qr.solve(A2, b)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.1111\\\\ 0.1852\\\\\\end{array}\\right]\\) Are different from the “one-projection-at-a-time” coefficients: (b %dot% u)/(u %dot% u)        \\(0.3333\\) (b %dot% v)/(v %dot% v)        \\(0.2593\\) Since independent projections won’t solve the target problem (when the columns of \\(\\mathbf A\\) are not mutually orthogonal), how do we solve it? The strategy is two simplify the problem by constructing from \\(\\mathbf A\\) another matrix that we’ll call \\(\\mathbf Q\\) which spans exactly the same subspace as \\(\\mathbf A\\) but which has mutually orthogonal columns. To start, well set the first column of \\(\\mathbf Q\\) to be any one of the vectors in \\(\\mathbf A\\). We’ll use \\(\\vec{u}\\) for the example. The second column of \\(\\mathbf Q\\) will be based on one of the remaining vectors, say \\(\\vec{v}\\). But \\(\\vec{v}\\) is not orthogonal to \\(\\vec{u}\\). For the second column of \\(\\mathbf Q\\) we’ll insert not \\(\\vec{v}\\) itself, but the component of \\(\\vec{v}\\) that is orthogonal to \\(\\vec{u}\\), that is: \\[\\overset{\\longrightarrow}{v\\!\\perp_u} = \\vec{v} - \\frac{\\vec{u}\\cdot\\vec{v}}{\\vec{u}\\cdot\\vec{u}} \\vec{u}\\] In computer notation, we’ll refer to \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) with the name v_perp_u. coef &lt;- ((u %dot% v) / (u %dot% u)) v_perp_u &lt;- v - coef * u coef        \\(1.2\\) Now the \\({\\mathbf Q}\\) matrix is \\[{\\mathbf Q} \\equiv \\left[\\begin{array}{cc}|&amp;|\\\\ \\vec{u}&amp;\\overset{\\longrightarrow}{v\\!\\perp_u}\\\\|&amp;|\\end{array}\\right]\\] Caution: The next few paragraphs are rough going. It suffices to follow the flow of the argument and to note that the only operations used are scalar multiplication, addition, subtraction, and the simple dot-product form for the coefficient produced by projecting one vector onto another vector. Since \\(\\vec{u}\\) and \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) are orthogonal, we can easily calculate the coefficients on the two vectors for projecting \\(\\vec{b}\\) onto the subspace spanned by \\(\\mathbf Q\\). alpha1 &lt;- (u %dot% b) / (u %dot% u) alpha2 &lt;- (v_perp_u %dot% b) / (v_perp_u %dot% v_perp_u) alpha1        \\(0.3333\\) alpha2        \\(0.1852\\) These coefficients—\\(\\alpha_1 =\\ \\) 0.3333333 and \\(\\alpha_2 =\\ \\) 0.1851852 respectively—when multiplied by \\(\\vec{u}\\) and \\(\\overset{\\longrightarrow}{v\\!\\perp_u}\\) will give us the projection of \\(\\vec{b}\\) onto the subspace spanned by \\({\\mathbf Q}\\). Since the subspace spanned by \\({\\mathbf Q}\\) is exactly the same as the subspace spanned by \\({\\mathbf A}\\), we have the answer for \\(\\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\) and consequently for \\[\\overset{\\longrightarrow}{b\\!\\perp_{\\mathbf A}} = \\vec{b} - \\overset{\\longrightarrow}{b\\|_{\\mathbf A}}\\ .\\] In terms of the coefficients, the projection of \\(\\vec{b}\\) onto \\(\\mathbf A\\) is \\[\\overset{\\longrightarrow}{b\\|_{\\mathbf A}} = 0.3333333 \\vec{u} + 0.1851852 \\overset{\\longrightarrow}{v\\!\\perp_u}\\] These are not the coefficients on \\(\\vec{u}\\) and \\(\\vec{v}\\) that we originally sought. But recognizing that \\(\\overset{\\longrightarrow}{v\\!\\perp_u} = v - 1.2 \\vec{u}\\), we have \\[\\overset{\\longrightarrow}{b\\|_{\\mathbf A}} = 0.3333333 \\vec{u} + 0.1851852 \\left[\\strut \\vec{v} - 1.2 \\vec{u}\\right] \\\\= [0.3333333 - 1.2\\times0.1851852] \\vec{u} + 0.1851852 \\vec{v}\\\\= 0.1111111 \\vec{u} + 0.1851852 \\vec{v}\\] These coefficients on \\(\\vec{u}\\) and \\(\\vec{v}\\) are the ones we sought and the ones produced by the professional software. qr.solve(A2, b)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.1111\\\\ 0.1852\\\\\\end{array}\\right]\\) Of course, nobody would want to undertake the process described above in the step-by-step fashion we’ve followed. In addition to being hard to follow, it’s hard to avoid making mistakes along the way. Fortunately, expert programmers have done the work for us and encapsulated the process in a software function. For us using R, that function is qr.solve(). The result is that we now have a way to solve the target problem, finding the coefficients on for the linear combination of a set of vectors that will bring us as close as possible to a target \\(\\vec{b}\\). In the next chapter, we’ll use this capability to solve real-world modeling problems. 43.3 Exercises Exercise XX.XX: wniqMc Refer to the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), \\(\\vec{c}\\), and \\(\\vec{d}\\) in the figure. Carry out the following projections graphically. You should show not only the result of the projection, but also the original vector being projected and the original vector being projected onto. Project \\(\\vec{a}\\) onto \\(\\vec{b}\\) Project \\(\\vec{c}\\) onto \\(\\vec{a}\\) Project \\(\\vec{b}\\) onto itself. Go back to your diagrams and add on to each diagram the residual vector from the projection. Make sure to use a different color ink or some other device to distinguish the residuals from the vectors you had already drawn. Exercise XX.XX: yeY17y Here is a direction, \\(\\vec{\\mathbf D}\\): \\[\\vec{\\mathbf D} \\equiv \\left(\\begin{array}{c}2\\\\5\\end{array}\\right)\\] Question A Find the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.59 \\vec{\\mathbf D}\\)Nice!  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question B Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (1, 3)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(0.83, 2.07\\right)\\)Right!  \\(\\vec{\\mathbf R} = \\left(0.31, 1.28\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(1.28, 0.31\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.83, 0.31\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Question C Find the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(0.379 \\vec{\\mathbf D}\\)Correct.  \\(\\left(5, 6\\right)^T\\)︎✘ The projection of \\(\\vec{\\mathbf V}\\) onto the direction of \\(\\vec{\\mathbf D}\\) will always be a scalar multiple of \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf V} - \\left(5, 6\\right)^T\\)︎✘ If \\(\\left(5,6\\right)^T\\) were the residual vector, this would be true. Question D Find the residual vector \\(\\vec{\\mathbf R}\\) from the projection of \\(\\vec{\\mathbf V} \\equiv (3, 1)^T\\) onto \\(\\vec{\\mathbf D}\\). \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)^T\\)Nice!  \\(\\vec{\\mathbf R} = \\left(-0.17, 0.07\\right)\\)︎✘ The residual from projecting a column vector onto another column vector will be a column vector \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)^T\\)︎✘ \\(\\vec{\\mathbf R} = \\left(0.07, -0.17\\right)\\)︎✘ Try adding \\(\\vec{\\mathbf R} + \\vec{\\mathbf V}\\) and see if you get $ Exercises confirming that qr.solve() produces results that are as they should be: residual orthogonal to every vector in A, projected + residual = \\(\\vec{b}\\). Exercises confirming that adding more columns to A produces a smaller residual. Demonstration that even random vectors can be combined to exactly equal \\(\\vec{b}\\), so long as we have enough of them. Exercise XX.XX: p209w8 Here are twelve labeled vectors, A through M. There is a thirteenth vector, labeled “Null vector.” That’s a vector of length zero, so it can’t be drawn as an arrow. Note that the direction of the null vector doesn’t matter, since the vector length is zero. Each of the following statements is of the form, \"\\(\\vec{v}\\) projected onto \\(\\vec{u}\\) gives \\(\\vec{w}\\). Say whether the statement is true or false. Question A \\(\\vec{A}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{K}\\) TrueCorrect.  False︎✘ A projected onto D will be in the direction of D. K is in the direction of D, points in the right direction (downwards), and equals the vertical component of A. Question B \\(\\vec{D}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{L}\\) True︎✘ D projected onto E will be in the direction of E. L is in the direction of E but L does not have the right length. FalseCorrect.  Question C \\(\\vec{J}\\) projected onto \\(\\vec{E}\\) gives the null vector. True︎✘ J and E are not orthogonal. So the projection of one onto the other cannot be the null vector. FalseExcellent!  Question D \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question E \\(\\vec{J}\\) projected onto \\(\\vec{K}\\) gives \\(\\vec{D}\\) True︎✘ J and K are parallel, so projecting J onto K will produce the vector J. But J is much shorter than D. FalseExcellent!  Question F \\(\\vec{C}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{L}\\) True︎✘ C projected onto D will be in the direction of D. L is not in the direction of D. FalseCorrect.  Question G \\(\\vec{L}\\) projected onto \\(\\vec{B}\\) gives the null vector True︎✘ It’s only when vectors are orthogonal that the projection of one onto the other produces the null vector. L and B are not orthogonal. FalseRight!  Question H \\(\\vec{E}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{E}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question I \\(\\vec{G}\\) projected onto \\(\\vec{C}\\) gives the null vector.     True\\(\\heartsuit\\ \\)       False︎✘ Question J \\(\\vec{E}\\) projected onto \\(\\vec{D}\\) gives \\(\\vec{J}\\)     True\\(\\heartsuit\\ \\)       False︎✘ Question K \\(\\vec{A}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{K}\\)     True︎✘ A onto B will be in the direction of B. But K is orthogonal to B.       False\\(\\heartsuit\\ \\) Question L \\(\\vec{H}\\) projected onto \\(\\vec{A}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question M \\(\\vec{F}\\) projected onto \\(\\vec{C}\\) gives \\(\\vec{H}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question N \\(\\vec{C}\\) projected onto \\(\\vec{E}\\) gives \\(\\vec{E}\\)     True︎✘        False\\(\\heartsuit\\ \\) Question O \\(\\vec{E}\\) projected onto \\(\\vec{G}\\) gives the null vector     True\\(\\heartsuit\\ \\)       False︎✘ Question P \\(\\vec{G}\\) projected onto \\(\\vec{B}\\) gives \\(\\vec{C}\\)     True︎✘        False\\(\\heartsuit\\ \\) Exercise XX.XX: dvWdE7 Vectors \\(\\vec{A}, \\vec{B}, \\cdots, \\vec{H}\\) have been defined as A, B, …, H so that you can use them in the sandbox. (To see one of these vectors, give a command in the sandbox consisting of just the vector name.) A &lt;- rbind(2.1, -3.4) B &lt;- rbind(0.3, 4.2) C &lt;- rbind(-2, 3.8) D &lt;- rbind(1.1, -3, 2.2, -1) E &lt;- rbind(6, 4, -2, -1) F &lt;- rbind(0, 3, 2, 1) G &lt;- rbind(1, -2, 1, 1) H &lt;- rbind(-2, 0, 3, 4) Here is the sequence of commands to project vector C onto A and B. M &lt;- cbind(A, B) # Problem setup: Matrix packaging up vectors b &lt;- C # Problem setup: Target vector x &lt;- qr.solve(M, b) # Solve the target problem M %*% x = b for x bhat &lt;- M %*% x # Result: projected vector resid &lt;- b - bhat # Result: residual vector t(A) %*% A # square length of a vector Question A A) Solve \\((\\vec{A}, \\vec{C}) \\cdot \\vec{x} = \\vec{B}\\). What is the length of \\(\\vec{x}\\)?     Exactly 0︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43︎✘        About 12\\(\\heartsuit\\ \\)       Can’t be solved︎✘ Question B B) Solve \\((\\vec{A}) \\cdot \\vec{x} = \\vec{B}\\). What’s the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6︎✘        About 2.5\\(\\heartsuit\\ \\)       About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Question C D) Solve \\((\\vec{D}, \\vec{E}) \\cdot \\vec{x} = \\vec{F}\\). What is the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43\\(\\heartsuit\\ \\)       About 12︎✘        Can’t be solved︎✘ Question D E) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}) \\cdot \\vec{x} = \\vec{G}\\). What is the length of the residual?     Exactly 0︎✘        About 0.3︎✘        About 1.6\\(\\heartsuit\\ \\)       About 2.5︎✘        About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Question E F) Solve \\((\\vec{D}, \\vec{E}, \\vec{F}, \\vec{G}) \\cdot \\vec{x} = \\vec{H}\\). What is the length of the residual?     Exactly 0\\(\\heartsuit\\ \\)       About 0.3︎✘        About 1.6︎✘        About 2.5︎✘        About 3.43︎✘        About 12︎✘        Can’t be solved︎✘ Calculate the angle between two vectors arithmetically, using dot product graphically, using protractor Find a nonzero vector orthogonal to a given vector Calculate the projection of a vector onto another vector arithmetically with dot product graphically Decompose a vector into a residual and a component directed along a second vector graphically arithmetically Our goal is to scale the expkt vector so that the scaled numbers will be as close as possible to our destination, namely, temp. Comparing the two columns of numbers, you might anticipate that the scalar will be about 100. We’ll see how to calculate it exactly in the next chapter. The result turns out to be 99.23. The resulting model will be \\[T(t) = 99.23\\, e^{-0.02 t}\\ .\\] How are we to judge whether this is a good model or not? Common sense suggests plotting out the model function along with the data, as in Figure 43.9. Figure 43.9: Comparing the model \\(99.23\\, e^{-0.02 t}\\) to the recorded data in `CoolingWater. Judge for yourself whether this is a good model. The obvious deficiency is that the model falls, as decaying exponentials will do, toward a temperature of 0, whereas the water is cooling to a room temperature of about 25 degrees. Let’s return to the model seen in terms of vectors. The advantage of doing this is to develop a general procedure we can use for interpreting models of all sorts, rather than just the particular situation of the cooling-water data. What are the geometric facts? We know that the temp vector has length 251.3 deg C. Similarly we can calculate the length of the expkt vector: 2.46 deg C. It might seem that the “direction” of the vector is meaningless, because it’s a direction in an abstract, hard-to-envision 15-dimensional space. (There are 15 components to each of temp and expkt.) Even so, we can calculate the angle between the two vectors, using the formula \\(\\cos(\\theta) = \\frac{\\vec{v}\\cdot \\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\). Doing the arithmetic gives \\[\\cos{\\theta} = \\frac{599.8}{251.3 \\times 2.46} = 0.9708\\ \\ \\implies \\ \\ \\ \\theta = 13.88^\\circ\\] Figure 43.10: The vectors temp and expkt have an angle of 13.88 deg between them. Here, expkt has been drawn 10x it’s actual size. With these geometrical facts, we can draw a picture. Figure 43.10 shows temp in black and expkt in magenta. (We’ve drawn it 10 times as long as it really is so that you can see it well.) For the expkt vector to be a good model of temp, we need to scale it so that the result, which must be on the dotted line in the picture, is as close as possible to the tip of temp. You can count off yourself how many expkt steps will bring you close to temp. (Remember to multiply your result by 10, since in the picture we drew expkt ten times longer than its arithmetic length.) One reasonable way to quantify how good a model of temp can be made by a properly scaled version of vector expkt is the angle between them: 13.88 degrees. Likewise, we can scale the vector intercept to make it match temp as well as possible. The angle between intercept and temp works out to be 75.7 degrees; the vectors are not very well aligned. Scaling by 58.2 will bring intercept as close as it is ever going to get to temp, which is not very close at all. The idea of a linear combination is to scale and add multiple vectors. As a very rough start, let’s look at the combination 58.2 intercept + 99.23 expkt, the combination of the two individual models we constructed by vector analogy. CAUTION: The model will be poor. That’s not because the vector analogy is poor but because we still have to work out, as we will in the next two chapters, how properly to work with vectors. CW &lt;- CW %&gt;% mutate(model = 99.23*expkt + 58.2*intercept) time temp expkt intercept one two three model 0 98.2 1.0000000 1 29.00000 84.00000 115.00000 157.43000 2 91.4 0.9607894 1 30.99974 85.41158 111.07894 153.53914 4 86.9 0.9231163 1 32.92107 86.76781 107.31163 149.80084 6 83.1 0.8869204 1 34.76706 88.07086 103.69204 146.20911 9 78.0 0.8352702 1 37.40122 89.93027 98.52702 141.08386 14 71.4 0.7557837 1 41.45503 92.79179 90.57837 133.19642 19 66.3 0.6838614 1 45.12307 95.38099 83.38614 126.05957 29 58.6 0.5598984 1 51.44518 99.84366 70.98984 113.75871 39 53.0 0.4584060 1 56.62129 103.49738 60.84060 103.68763 49 48.8 0.3753111 1 60.85913 106.48880 52.53111 95.44212 74 41.2 0.2276377 1 68.39048 111.80504 37.76377 80.78849 99 36.2 0.1380692 1 72.95847 115.02951 28.80692 71.90061 124 32.7 0.0837432 1 75.72910 116.98524 23.37432 66.50984 149 30.1 0.0507928 1 77.40957 118.17146 20.07928 63.24017 174 28.2 0.0308074 1 78.42882 118.89093 18.08074 61.25702 The resulting model is … well, terrible! Figure ?? shows the linear combination EXERCISE: Repeat the calculations for the entire CoolingData data frame. Adding vectors. The result is a vector Scaling and adding vectors: a linear combination of vectors. 43.4 MOVED FROM VECTORS A pencil is a physical object that does a good job representing a vector in three dimensional space. Three-dimensional space is all around us and it’s easy—indeed, inevitable—to situate a pencil in it. We have no such physical access to 4-dimensional space or higher-dimensional spaces. Instead of a physical representation, we need to rely on a mathematical one: a column of numbers. And we can’t use a protractor to measure the angle between two vectors in 4- or higher-dimensional space. Instead, we calculate the angle using arithmetic. But to define the numerical process for calculating an angle, we need to make sure that the result follows the familiar conventions for angles, specifically that they be between 0 and 180 degrees, that the angle between two vectors with the same orientation is 0, and that the angle between two opposite-pointing vectors is 180 degrees. The arithmetic formula for computing the angle between two vectors is simple. Or, rather, it’s simple if we allow ourselves to calculate the cosine of the angle rather than the angle \\(\\theta\\) itself. The underlying quantity can be calculated using dot products: \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}^T \\cdot \\vec{w}}{\\sqrt{\\strut (\\vec{v}^T\\cdot \\vec{v})(\\vec{w}^T\\cdot \\vec{w})}}\\] A starting point is creating a vector on the computer. In some sense this a vector is just a collection of numbers, but it’s helpful to be disciplined and remember that, for our purposes, a vector is a column of numbers. R knows about such columns and will handle them appropriately. One way to create a column vector is with the rbind() function. use the rbind() function applied to individual arguments. Here, for instance, is a command that makes a three-dimensional vector we are calling b. b &lt;- rbind(4, -2, 6) b ## [,1] ## [1,] 4 ## [2,] -2 ## [3,] 6 Notice that in printing out a vector, R includes a series of indices (e.g. [1,] or [3,]) to help the reader identify the location of any element in the vector. It also prints a header ([,1]) which is helpful later when we work with collections of vectors. 43.5 Matrix You are going to hear the word “matrix” a lot. Later in this tutorial we will use the term “matrix multiplication.” A matrix is a collection of vectors, all of the same dimension. We’ll get to them in good time. 43.6 Scalar multiplication You can multiply a vector times a number. The result is a new vector with exactly the same direction as the original, but with a different length. The arithmetic is very simple: do ordinary multiplication of the number by each of the elements of the vector. Examples: \\[2 \\left(\\begin{array}{c}4\\\\7\\end{array}\\right) = \\left(\\begin{array}{c}8\\\\14\\end{array}\\right)\\] This simple multiplication is called “scalar multiplication” for two reasons: The result is to “scale” the vector, in the sense of a “scale model”, that is, to make the vector bigger or smaller. There is another important form of vector arithmetic called “matrix multiplication.” By saying “scalar multiplication,” we avoid the confusion that might arise if we used “multiplication” alone. In R, scalar multiplication of a vector is done with *, just like ordinary multiplication with numbers: b &lt;- rbind(4, -2, 6) 2.3 * b ## [,1] ## [1,] 9.2 ## [2,] -4.6 ## [3,] 13.8 Example 43.4 In a SANDBOX, Write the R code to create a vector named w with components 4, -1, and -3.5. Then scalar multiply w by 6.3. w &lt;- _______ 6.3 _______ w Solution w &lt;- rbind(4, -1, -3.5) 6.3 * w ## [,1] ## [1,] 25.20 ## [2,] -6.30 ## [3,] -22.05 43.7 Dot product and %*% Now to introduce a new R arithmetic function, written %*%. This symbol is pronounced “matrix multiply.” In traditional mathematical notation, matrix multiplication is indicated by putting the two quantities next to one another, like this: \\(\\vec{\\mathbf m}^T \\vec{\\mathbf x}\\), or sometimes with a dot \\(\\vec{\\mathbf m}T\\cdot \\vec{\\mathbf x}\\). The superscript \\(^T\\) means “transpose.” For us, this is merely a book-keeping convention. The operation %*% will do several different types of arithmetic with vectors. The one we will work with here is called a dot product. (There are also “matrix products” and “outer products”.) The R notation for a dot product very much echoes the traditional matrix notation, at least with respect to \\(^T\\). We’ll illustrate by creating two vectors u and v and then calculating their dot product. u &lt;- rbind(6, -3, 7) v &lt;- rbind(2, 1, 3) t(u) %*% v ## [,1] ## [1,] 30 Notice that the output of a dot product is a single number: a scalar. (R prints the output as if the scalar were a vector in one-dimension.) Arithmetically, the dot product is calculated by multiplying the corresponding components in the two vectors (e.g. \\(6 \\times 2\\) and \\(-3 \\times 1\\) and \\(7 \\times 3\\)) and adding up the result. You can see why the dot product always involves two vectors with the same number of elements. The R t() function corresponds to the mathematical notation for the transpose: \\(^T\\). So t(u) would be written, mathematically, as \\(\\vec{u}^T\\). The purpose of t() is to turn columns (like our vector u) into rows, and vice versa. If you like, try the command t(u) in the sandbox to see how it is printed. For us, the purpose of writing t(u) is to signal to the %*% matrix multiplication operation that we want a particular operation: the dot product. The dot product always involves the transpose of a column vector on the left side of %*% and a column vector on the right side. You can also write a command u %*% t(v), but this is not a dot product. It is called an “outer product” and we will not need it in this course. Try it out in the sandbox. The dot product and the outer product are written in very similar ways but produce completely different results. We won’t have much use for outer products in this course, but you should be aware what they look like so that you can diagnose the problem if your attempt at a dot product goes wrong. Dot products t(u) %*% v ## [,1] ## [1,] 30 t(v) %*% u ## [,1] ## [1,] 30 Outer product u %*% t(v) ## [,1] [,2] [,3] ## [1,] 12 6 18 ## [2,] -6 -3 -9 ## [3,] 14 7 21 v %*% t(u) ## [,1] [,2] [,3] ## [1,] 12 -6 14 ## [2,] 6 -3 7 ## [3,] 18 -9 21 43.8 Vector lengths For a vector \\(\\vec{v}\\), the length is denoted \\(|| \\vec{v} ||\\). Vector length can be measured with a ruler … so long as you have physical access to the vector. But often, all we have is the numerical representation. So, we use arithmetic—the dot product—to calculate vector length: \\[|| \\vec{v} || \\equiv \\sqrt{\\ \\vec{v}^T \\cdot \\vec{v}}\\] ::: {.example data-latex=\"\"} Consider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\ \\ \\ \\mbox{and} \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right) \\] The length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\). The length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\). Using a SANDBOX, use R commands to create the vectors \\(\\vec{u}\\) and \\(\\vec{w}\\) and find their lengths using the dot-product operator%*%` operator. u &lt;- rbind( ____ ) # length of u sqrt( ____ %*% ____ ) v &lt;- rbind( ______ ) # length of v sqrt( ____ %*% ____ ) Solution u &lt;- rbind(3, 4) sqrt(t(u) %*% u) ## [,1] ## [1,] 5 w &lt;- rbind(1, 1, 1, 1) sqrt(t(w) %*% w) ## [,1] ## [1,] 2 43.9 Outline Write a system of linear equations as a “target problem” involving a linear combination of vectors. Definition of a linear combination of vectors. Matrix is a collection of vectors Linear combination is written as matrix multiplied by vector Solve target problem algebraically or graphically in two dimensions. Subtracting vectors algebraically and graphically Scalar multiplication is just the regular sort of multiplication that you grew up with, as in \\(3 \\times 4 \\rightarrow 12.\\) We call it “scalar multiplication” to make it clear that we are not talking about “matrix multiplication.” Terms such as “scalar multiplication are called retronyms, a modification to a long-existing word that has become ambiguous because of the introduction of a new equivalent. Example: the existing”mail\" was at first supplemented by “e-mail”. Now that e-mail is more common than the old-fashioned postal kind, the retronym “snail mail” is used to indicate the kind delivered by postal workers.↩︎ "],["target-problem.html", "Chapter 44 The target problem 44.1 Properties of the solution 44.2 The geometry of qr.solve() 44.3 Finding the target with QR", " Chapter 44 The target problem In Chapter 43, we solve the problem of finding the scalar multiple \\(\\alpha^\\star\\) such that \\(\\alpha^\\star\\, \\vec{u}\\) is as close as possible to another vector \\(\\vec{b}\\). In this chapter, we’ll generalize that method to let us finding the particular linear combination of any set of vectors \\(\\vec{u}\\), \\(\\vec{v}\\), \\(\\vec{w}, \\ldots\\) that is a close as possible to \\(\\vec{b}\\). We call this the target problem. For clarity, we’ll always write the target as \\(\\vec{b}\\): the place we want to get to. 44.1 Properties of the solution As you might expect, there is a known solution to the target problem. We’ll start by using a computer implementation of this solution to demonstrate some simple properties of the solution. As an example, we’ll use three vectors in a 5-dimensional space as the “screen” to be projected onto, and another vector \\(\\vec{b}\\) as the object being projected. The notation \\[\\overset{\\longrightarrow}{{b}\\|_{u,v,w}}\\] is expressive of the solution we seek. But it’s more concise to place \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) into a matrix \\({\\mathbf A}\\): \\[{\\mathbf A} \\equiv \\left[\\strut \\begin{array}{ccc}|&amp;|&amp;|\\\\\\vec{u} &amp; \\vec{v} &amp; \\vec{w}\\\\|&amp;|&amp;|\\end{array}\\right]\\] so that we can write the solution as \\[\\overset{\\longrightarrow}{{b}\\|_{\\mathbf A}}\\ .\\] Such notation, lovely and expressive though it may be, is hardly suited for a computer expression, which has to be constructed from typewriter characters. So, we’ll adopt another convention by writing b_hat on the computer to stand for \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf A}}\\). In the same spirit, we’ll write b_resid to stand for \\(\\overset{\\longrightarrow}{b\\!\\perp_{\\mathbf A}}\\). # the three vectors u &lt;- rbind(6, 4, 9, 3, 1) v &lt;- rbind(1, 5,-2, 0, 7) w &lt;- rbind(3,-5, 2, 8, 4) A &lt;- cbind(u, v, w) # the target b &lt;- rbind(8, 2,-5, 7, 0) # the solution and residual b_hat &lt;- b %onto% A b_resid &lt;- b %perp% A b_hat        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}2.0304\\\\ -0.6152\\\\ 0.6526\\\\ 4.0231\\\\ 4.3358\\\\\\end{array}\\right]\\) b_resid        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}5.9696\\\\ 2.6152\\\\ -5.6526\\\\ 2.9769\\\\ -4.3358\\\\\\end{array}\\right]\\) How can we confirm that this really is the solution to the target problem for this set of vectors? One check is that b_hat + b_resid should be identical to the original b: b - (b_hat + b_resid)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\\\end{array}\\right]\\) So far, so good. Another check is that b_hat be perpendicular to b_resid. If so, the dot product of b_hat and b_resid should be 0. b_hat %dot% b_resid        \\(0\\) Close enough! (Round-off error in computer arithmetic has created this slight deviation from 0.) An interesting consequence of the residual being perpendicular to b_hat is that the residual will also be perpendicular to each and every one of the vectors being projected onto, and to any linear combination of those vectors. b_resid %dot% u        \\(0\\) b_resid %dot% v        \\(0\\) b_resid %dot% w        \\(0\\) Examples of linear combinations b_resid %dot% (2 * u - 4.7 * v + pi * w)        \\(0\\) b_resid %dot% (-5.4 * u - 0.7 * v + 8.3 * w)        \\(0\\) Check! Although we know b_hat and b_resid, we still don’t know what particular linear combination of the columns in A produces b_hat. Here’s how to find out: x        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0.0384\\\\ 0.3348\\\\ 0.4885\\\\\\end{array}\\right]\\) x contains the coefficients for the linear combination. To demonstrate, let’s compare b_hat to the linear combination: b_hat - (0.03835171 * u + 0.33478133 * v + 0.48849968 * w)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\\\end{array}\\right]\\) b_hat/(0.03835171 * u + 0.33478133 * v + 0.48849968 * w)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\\\\\end{array}\\right]\\) Ideally, the result should be all ones. It’s not because of round-off error and because we copied only the first several digits of the coefficients into the calculation. It would have been better not to copy. Instead, we can calculate the linear combination by matrix multiplication: b_hat - (A %*% x)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\\\end{array}\\right]\\) b_hat/(A %*% x)        \\(\\left[\\strut\\begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrr}1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\\\\\end{array}\\right]\\) You should add qr.solve() to your computational toolbox of R functions. It carries out the projection calculations, but instead of returning b_hat or \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf A}}\\) or whatever we want to call the shadow of \\(\\vec{b}\\) onto \\({\\mathbf A}\\), it returns the coefficients for the linear combination of the vectors in \\({\\mathbf A}\\) to produce \\(\\overset{\\longrightarrow}{{b}\\|_{\\mathbf A}}\\). 44.2 The geometry of qr.solve() 44.3 Finding the target with QR Start with the very simple example of the identity matrix and reaching a target with that. Show that the vectors in the identity matrix are mutually orthogonal. Show that we can find a mutually orthogonal basis set for any matrix. Write a target problem in the form A x = b Relationship between \\(\\vec{b}\\), \\(\\hat{b}\\), and the residual. Compose linear combination problems and solve using R/qr.solve() Applications of projection Relationship between projection and method of least squares Computations on results from qr.solve() b-hat residual orthogonality between b-hat and residual "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
