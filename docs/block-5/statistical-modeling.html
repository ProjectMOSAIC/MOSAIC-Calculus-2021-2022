<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 45 Statistical modeling | MOSAIC-Calculus.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 45 Statistical modeling | MOSAIC-Calculus.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 45 Statistical modeling | MOSAIC-Calculus.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="target-problem.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<span class="math inline">
\(\newcommand{\line}{\text{line}}
\newcommand{\hump}{\text{hump}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\recip}{\text{recip}}
\newcommand{\diff}[1]{{\cal D}_#1}
\newcommand{\pnorm}{\text{pnorm}}
\newcommand{\dnorm}{\text{dnorm}}
\let\origvec\vec
\let\origmathit\mathit
\let\orighat\hat
\newcommand{\vec}[1]{\overset{{\rule[-1pt]{0mm}{1mm}}\rightharpoonup}{\mathbf{#1}}}
\newcommand{\mathit}[1]{\underset{\leftharpoondown}{\overset{{\rightharpoonup}}{\large\mathbf #1}}}
\newcommand{\hat}[1]{\widehat{\ \mathbf#1\ }}
\newcommand{\len}[1]{\|{\mathbf #1}\|}
\newcommand{\tvec}[1]{\overset{\uparrow}{\mathbf #1}}
\newcommand{\tmat}[1]{\overset{\leftrightarrows}{\mathbf #1}}
\newcommand{\perpendicularto}[2]{#1\!\perp\!#2}
\newcommand{\modeledby}[2]{#1\!\sim\!#2}
\newcommand{\CC}[1]{\color{#648fff}{#1}}
\newcommand{\CE}[1]{\color{#785ef0}{#1}}
\newcommand{\CA}[1]{\color{#dc267f}{#1}}
\newcommand{\CB}[1]{\color{#fe6100}{#1}}
\newcommand{\CD}[1]{\color{#ffb000}{#1}}\)
</span>



<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="mosaic-calc-style-copy.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="../docs/welcome-to-calculus.html">Welcome to calculus</li>
<li><a href="../block-1/change.html">Block 1: Functions & quantities</a></li>
<li><a href="../block-2/change-relatonships.html">Block 2: Differentiation</a></li>
<li><a href="../block-3/change-accumulation.html">Block 3: Accumulation</a></li>
<li><a href="../block-4/block4-intro.html">Block 4: Manifestations</a></li>
<hr>
<li><strong><a href="./">Block 5: Linear combinations</strong></a></li>

<li class="divider"></li>
<li class="chapter" data-level="41" data-path="vectors.html"><a href="vectors.html"><i class="fa fa-check"></i><b>41</b> Vectors</a>
<ul>
<li class="chapter" data-level="41.1" data-path="vectors.html"><a href="vectors.html#length-direction"><i class="fa fa-check"></i><b>41.1</b> Length &amp; direction</a></li>
<li class="chapter" data-level="41.2" data-path="vectors.html"><a href="vectors.html#the-nth-dimension"><i class="fa fa-check"></i><b>41.2</b> The n<sup>th</sup> dimension</a></li>
<li class="chapter" data-level="41.3" data-path="vectors.html"><a href="vectors.html#geometry-arithmetic"><i class="fa fa-check"></i><b>41.3</b> Geometry &amp; arithmetic</a></li>
<li class="chapter" data-level="41.4" data-path="vectors.html"><a href="vectors.html#vector-length"><i class="fa fa-check"></i><b>41.4</b> Vector lengths</a></li>
<li class="chapter" data-level="41.5" data-path="vectors.html"><a href="vectors.html#angles"><i class="fa fa-check"></i><b>41.5</b> Angles</a></li>
<li class="chapter" data-level="41.6" data-path="vectors.html"><a href="vectors.html#orthogonality"><i class="fa fa-check"></i><b>41.6</b> Orthogonality</a></li>
<li class="chapter" data-level="41.7" data-path="vectors.html"><a href="vectors.html#exercises"><i class="fa fa-check"></i><b>41.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="42" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html"><i class="fa fa-check"></i><b>42</b> Linear combinations of vectors</a>
<ul>
<li class="chapter" data-level="42.1" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#scaling-vectors"><i class="fa fa-check"></i><b>42.1</b> Scaling vectors</a></li>
<li class="chapter" data-level="42.2" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#adding-vectors"><i class="fa fa-check"></i><b>42.2</b> Adding vectors</a></li>
<li class="chapter" data-level="42.3" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#linear-combinations"><i class="fa fa-check"></i><b>42.3</b> Linear combinations</a></li>
<li class="chapter" data-level="42.4" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#matrices-and-linear-combinations"><i class="fa fa-check"></i><b>42.4</b> Matrices and linear combinations</a></li>
<li class="chapter" data-level="42.5" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#sub-spaces"><i class="fa fa-check"></i><b>42.5</b> Sub-spaces</a></li>
<li class="chapter" data-level="42.6" data-path="linear-combs-vectors.html"><a href="linear-combs-vectors.html#exercises-1"><i class="fa fa-check"></i><b>42.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="43" data-path="projection-residual.html"><a href="projection-residual.html"><i class="fa fa-check"></i><b>43</b> Projection &amp; residual</a>
<ul>
<li class="chapter" data-level="43.1" data-path="projection-residual.html"><a href="projection-residual.html#projection-terminology"><i class="fa fa-check"></i><b>43.1</b> Projection terminology</a></li>
<li class="chapter" data-level="43.2" data-path="projection-residual.html"><a href="projection-residual.html#projection-onto-a-single-vector"><i class="fa fa-check"></i><b>43.2</b> Projection onto a single vector</a></li>
<li class="chapter" data-level="43.3" data-path="projection-residual.html"><a href="projection-residual.html#projection-onto-a-set-of-vectors"><i class="fa fa-check"></i><b>43.3</b> Projection onto a set of vectors</a></li>
<li class="chapter" data-level="43.4" data-path="projection-residual.html"><a href="projection-residual.html#exercises-2"><i class="fa fa-check"></i><b>43.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="44" data-path="target-problem.html"><a href="target-problem.html"><i class="fa fa-check"></i><b>44</b> The target problem</a>
<ul>
<li class="chapter" data-level="44.1" data-path="target-problem.html"><a href="target-problem.html#linear-equations"><i class="fa fa-check"></i><b>44.1</b> Linear equations</a></li>
<li class="chapter" data-level="44.2" data-path="target-problem.html"><a href="target-problem.html#visualization-in-a-two-dimensional-subspace"><i class="fa fa-check"></i><b>44.2</b> Visualization in a two-dimensional subspace</a></li>
<li class="chapter" data-level="44.3" data-path="target-problem.html"><a href="target-problem.html#properties-of-the-solution"><i class="fa fa-check"></i><b>44.3</b> Properties of the solution</a></li>
<li class="chapter" data-level="44.4" data-path="target-problem.html"><a href="target-problem.html#application-of-the-target-problem"><i class="fa fa-check"></i><b>44.4</b> Application of the target problem</a></li>
<li class="chapter" data-level="44.5" data-path="target-problem.html"><a href="target-problem.html#exercises-3"><i class="fa fa-check"></i><b>44.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="45" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>45</b> Statistical modeling</a>
<ul>
<li class="chapter" data-level="45.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#r-squared"><i class="fa fa-check"></i><b>45.1</b> R-squared</a></li>
<li class="chapter" data-level="45.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#interpreting-the-model"><i class="fa fa-check"></i><b>45.2</b> Interpreting the model</a></li>
<li class="chapter" data-level="45.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#improving-the-model"><i class="fa fa-check"></i><b>45.3</b> Improving the model</a></li>
<li class="chapter" data-level="45.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#exercises-4"><i class="fa fa-check"></i><b>45.4</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="../block-6/diff-eq-intro.html">Block 6: Dynamics</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-modeling" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 45</span> Statistical modeling</h1>
<div style="float:right;">
<a href="https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/Block-5/B5-stat-modeling.Rmd"><img src="www/icons/edit.png" /></a>
</div>
<p>So far, we’ve been looking at linear combinations from a distinctively mathematical point of view: vectors, collections of vectors (matrices), projection, angles and orthogonality. We’ve show a few applications of the techniques for working with linear combinations, but have always expressed those techniques using mathematical terminology. In this Chapter, we will take a detour to get a sense of the perspective and terminology of another field: statistics.</p>
<p>In the quantitative world, including fields such as biology and genetics, the social sciences, business decision making, etc. there are far more people working with linear combinations with a statistical eye than there are people working with the mathematical form of notation. Statistics is a far wider field than linear combination, so this chapter is <strong>not</strong> an attempt to replace the need to study statistics and data science. The purpose is merely to show you how a mathematical process can be used as part of a broader framework to provide useful information to decision-makers.</p>
<div class="takenote">
<p>Since a statistics and data-science courses are part of a complete quantitative education, we want to point out from the beginning what you are likely to experience a traditional introductory statistics course and why that will seem largely disconnected from what you see here.</p>
<p>Statistics did not start out as a branch of mathematics, although people trained as mathematicians played a central role in it’s development. The story is complicated, but a simplification faithful to that history is to see statistics as an extension of biology. Statistics emerged in the last couple of decades of the 1800s. Possibly the key motivation was to understand <strong><em>genetics</em></strong> and Darwinian evolution. Today we know much about DNA sequences, RNA, amino acids, proteins, and so on. But quantitative genetics started in complete ignorance of any physical mechanism for genetics. Instead, mathematical models were the basis for the theory of genetics, beginning with with Gregor Mendel’s work on heritable traits published in 1865.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Perhaps coincidentally—or perhaps not—Charles Darwin (1809-1882) was a half-cousin of Francis Galton (1822-1911). Traditional introductory statistics is based, more or less, on the work Galton and his contemporaries, for instance Karl Pearson (1857-1936) and William Sealy Gosset (1876-1937). The terms that you encounter in introductory statistics—correlation coefficient, regression, chi-squared test, t-test—where in place by 1910. Ronald Fisher (1890-1962), also a geneticist, extended this work based in part on the mathematical ideas of projection. Fisher invented <strong><em>analysis of variance</em></strong> (ANOVA) and <strong><em>maximum likelihood estimation</em></strong> and is perhaps the “great man” of statistics. (See Section 35.4 for an introduction to likelihood. We’ll briefly touch on ANOVA in this chapter.) To his historical discredit, Fisher was a leader in eugenics. He was also a major obstacle, to the statistical acceptance of the dangers of smoking.</p>
<p>Most of the many techniques covered in traditional introductory statistics are, in fact, manifestations of the application of a general technique: the target problem. They are not taught this way, partly out of hide-bound tradition and partly because the target problem has been covered, if at all, in the fourth or fifth semester of a traditional calculus sequence.</p>
</div>
<p>It often happens that a model is needed to help organize complex, multivariate data for purposes such as prediction. As a case in point, consider the data available in the <code>Body_fat</code> data frame, which consists of measurements of characteristics such as height, weight, chest circumference, and body fat on 252 men.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="statistical-modeling.html#cb108-1" aria-hidden="true" tabindex="-1"></a>Znotes<span class="sc">::</span><span class="fu">and_so_on</span>(Body_fat) <span class="sc">%&gt;%</span> kableExtra<span class="sc">::</span><span class="fu">kable_minimal</span>()</span></code></pre></div>
<table class=" lightable-minimal" style="font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
bodyfat
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:right;">
weight
</th>
<th style="text-align:right;">
height
</th>
<th style="text-align:right;">
neck
</th>
<th style="text-align:right;">
chest
</th>
<th style="text-align:right;">
abdomen
</th>
<th style="text-align:right;">
hip
</th>
<th style="text-align:right;">
thigh
</th>
<th style="text-align:right;">
knee
</th>
<th style="text-align:right;">
ankle
</th>
<th style="text-align:right;">
biceps
</th>
<th style="text-align:right;">
forearm
</th>
<th style="text-align:right;">
wrist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
12.3
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
154.25
</td>
<td style="text-align:right;">
67.75
</td>
<td style="text-align:right;">
36.2
</td>
<td style="text-align:right;">
93.1
</td>
<td style="text-align:right;">
85.2
</td>
<td style="text-align:right;">
94.5
</td>
<td style="text-align:right;">
59.0
</td>
<td style="text-align:right;">
37.3
</td>
<td style="text-align:right;">
21.9
</td>
<td style="text-align:right;">
32.0
</td>
<td style="text-align:right;">
27.4
</td>
<td style="text-align:right;">
17.1
</td>
</tr>
<tr>
<td style="text-align:right;">
6.1
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
173.25
</td>
<td style="text-align:right;">
72.25
</td>
<td style="text-align:right;">
38.5
</td>
<td style="text-align:right;">
93.6
</td>
<td style="text-align:right;">
83.0
</td>
<td style="text-align:right;">
98.7
</td>
<td style="text-align:right;">
58.7
</td>
<td style="text-align:right;">
37.3
</td>
<td style="text-align:right;">
23.4
</td>
<td style="text-align:right;">
30.5
</td>
<td style="text-align:right;">
28.9
</td>
<td style="text-align:right;">
18.2
</td>
</tr>
<tr>
<td style="text-align:right;">
25.3
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
154.00
</td>
<td style="text-align:right;">
66.25
</td>
<td style="text-align:right;">
34.0
</td>
<td style="text-align:right;">
95.8
</td>
<td style="text-align:right;">
87.9
</td>
<td style="text-align:right;">
99.2
</td>
<td style="text-align:right;">
59.6
</td>
<td style="text-align:right;">
38.9
</td>
<td style="text-align:right;">
24.0
</td>
<td style="text-align:right;">
28.8
</td>
<td style="text-align:right;">
25.2
</td>
<td style="text-align:right;">
16.6
</td>
</tr>
<tr>
<td colspan="14" align="center">
… and so on …
</td>
</tr>
<tr>
</tr>
<tr>
<td style="text-align:right;">
26.0
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
190.75
</td>
<td style="text-align:right;">
70.5
</td>
<td style="text-align:right;">
38.9
</td>
<td style="text-align:right;">
108.3
</td>
<td style="text-align:right;">
101.3
</td>
<td style="text-align:right;">
97.8
</td>
<td style="text-align:right;">
56.0
</td>
<td style="text-align:right;">
41.6
</td>
<td style="text-align:right;">
22.7
</td>
<td style="text-align:right;">
30.5
</td>
<td style="text-align:right;">
29.4
</td>
<td style="text-align:right;">
19.8
</td>
</tr>
<tr>
<td style="text-align:right;">
31.9
</td>
<td style="text-align:right;">
74
</td>
<td style="text-align:right;">
207.50
</td>
<td style="text-align:right;">
70.0
</td>
<td style="text-align:right;">
40.8
</td>
<td style="text-align:right;">
112.4
</td>
<td style="text-align:right;">
108.5
</td>
<td style="text-align:right;">
107.1
</td>
<td style="text-align:right;">
59.3
</td>
<td style="text-align:right;">
42.2
</td>
<td style="text-align:right;">
24.6
</td>
<td style="text-align:right;">
33.7
</td>
<td style="text-align:right;">
30.0
</td>
<td style="text-align:right;">
20.9
</td>
</tr>
</tbody>
</table>
<p>Body fat, the percentage of total body mass consisting of fat, is thought by some to be a good measure of general fitness. To what extent this theory is merely a reflection of general societal attitudes toward body shape is unknown.</p>
<p>Whatever its actual utility, body fat is hard to measure directly; it involves submerging a person in water to measure total body volume, then calculating the persons mass density and converting this to a reading of body-mass percentage. For those who would like to see body fat used more broadly as a measure of health and fitness, this elaborate procedure stands in the way. And so they seek easier ways to estimate body fat along the lines of the <strong><em>Body Mass Index</em></strong> (BMI), which is a simple arithmetic combination of easily measured height and weight. (Note that BMI is also controversial as anything other than a rough description of body shape. In particular, the label “overweight,” officially <span class="math inline">\(25 \leq \text{BMI}\leq 30\)</span> has at best little connection to actual health.)</p>
<p>How can we construct a model, based on the available data, of body fat as a function of the easy-to-measure characteristics such as height and weight? You can anticipate that this will be a matter of applying what we know about the target problem
<span class="math display">\[\text{Given}\ \mathit{A}\  \text{and}\ \vec{b}\text{, solve } \mathit{A} \vec{x} = \vec{b}\ \text{for}\ \vec{x}\]</span>where <span class="math inline">\(\vec{b}\)</span> is the column of body-mass measurements and <span class="math inline">\(\mathit{A}\)</span> is the matrix of all the other columns in the data frame.</p>
<p>In statistics, the target <span class="math inline">\(\vec{b}\)</span> is called the <strong><em>response variable</em></strong> and <span class="math inline">\(\mathit{A}\)</span> is the set of <strong><em>explanatory variables</em></strong>. You can also think of the response variable as the output of the model we will build and the explanatory variables as the inputs to that model.</p>
<p>Although application of the target problem is an essential part of constructing a statistical model, it is far from the only part. For instance, statisticians find is useful to think about “how much” of the response variable is explained by the explanatory variables. Measuring this requires a definition for “how much.” In defining “how much,” statisticians focus not on how much <strong><em>variation</em></strong> there is among the values in the response variable. The standard way to measure this is with the <strong><em>variance</em></strong>, which was introduce in Section <a href="vectors.html#vector-length">41.4</a> and can be thought of as the average of pair-wise differences among the elements in <span class="math inline">\(\vec{b}\)</span>.</p>
<p>In order to support this focus on the variance of <span class="math inline">\(\vec{b}\)</span>, statisticians typically augment <span class="math inline">\(\vec{A}\)</span> with a column of ones, which they call the <strong><em>intercept</em></strong>.</p>
<p>To move forward, we’re going to extract the response variable from the data and construct <span class="math inline">\(\vec{A}\)</span>, adding in the vector of ones. We’ll show the vector/matrix commands for doing this, but you don’t have to remember them because statisticians have a more user-friendly interface to the calculations.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="statistical-modeling.html#cb109-1" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">with</span>(Body_fat, <span class="fu">cbind</span>(bodyfat))</span>
<span id="cb109-2"><a href="statistical-modeling.html#cb109-2" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> Body_fat[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">%&gt;%</span> as.matrix</span>
<span id="cb109-3"><a href="statistical-modeling.html#cb109-3" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, A)</span>
<span id="cb109-4"><a href="statistical-modeling.html#cb109-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">qr.solve</span>(A, b)</span></code></pre></div>
<p>The -1 in the second command is a directive to get rid of column 1, which happens to the the response variable, in constructing <span class="math inline">\(\vec{A}\)</span>.</p>
<p>Having applied <code>qr.solve()</code>, <span class="math inline">\(\vec{x}\)</span> now contains the coefficients on the “best” linear combination of the columns in <span class="math inline">\(\vec{A}\)</span>. One of the ways in which the R language is designed to support statistics, is that it keeps track of names of columns, so the elements of <span class="math inline">\(\vec{x}\)</span> are labelled with the name of the column the element applies to.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="statistical-modeling.html#cb110-1" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<pre><code>##              bodyfat
##         -18.18848508
## age       0.06207865
## weight   -0.08844468
## height   -0.06959043
## neck     -0.47060001
## chest    -0.02386415
## abdomen   0.95477346
## hip      -0.20754112
## thigh     0.23609984
## knee      0.01528121
## ankle     0.17399537
## biceps    0.18160242
## forearm   0.45202491
## wrist    -1.62063910</code></pre>
<p>Based on the above, the model for body fat as a function of the explanatory variables is:
<span class="math display">\[\text{body fat} = -18.2 + 0.062\, \mathtt{age} - 0.0884\, \mathtt{weight} - 0.696\, \mathtt{height} + \text{an so on}\ .\]</span>
Once we have the means to solve the target problem, as we do with <code>qr.solve()</code>, constructing such a model is straightforward.</p>
<p>But some questions remain:
- <em>How good is the model?</em>
- <em>Could we make a better model?</em></p>
<p>To answer these questions, we’ll have to develop a measure of how good the model is. And this depends on the purpose for which we’ll be using the model.</p>
<div id="r-squared" class="section level2" number="5.1">
<h2><span class="header-section-number">45.1</span> R-squared</h2>
<p>One of the most basic indices of the quality of a model—but far from the only one!—is a statistic written R<sup>2</sup> and pronounced “R-squared.”</p>
<p>R<sup>2</sup> is quite simple. Remember that a basic statistical question is how much variation there is in the response variable. That’s measured with the variance:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="statistical-modeling.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(b)</span></code></pre></div>
<pre><code>##          bodyfat
## bodyfat 70.03582</code></pre>
<p>We can also look at the variation in the model vector, <span class="math inline">\(\hat{b}\)</span>.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="statistical-modeling.html#cb114-1" aria-hidden="true" tabindex="-1"></a>bhat <span class="ot">&lt;-</span> A <span class="sc">%*%</span> x</span>
<span id="cb114-2"><a href="statistical-modeling.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(bhat)</span></code></pre></div>
<pre><code>##          bodyfat
## bodyfat 52.46033</code></pre>
<p>R<sup>2</sup> is simply the ratio of these two variances:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="statistical-modeling.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(bhat) <span class="sc">/</span> <span class="fu">var</span>(b)</span></code></pre></div>
<pre><code>##         bodyfat
## bodyfat 0.74905</code></pre>
<p>This result, 74.9%, is interpreted as the fraction of the variance in the response variable that is <strong><em>accounted for</em></strong> by the model. Near synonyms for “accounted for” is <strong><em>explained by</em></strong> or <strong><em>can be attributed to</em></strong>.</p>
<p>In the same spirit, we can ask how much of the variance in the response variable is <strong><em>unexplained by</em></strong>, or <strong><em>unaccounted by</em></strong> the explanatory variables. To answer this, look at the size of the residual:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="statistical-modeling.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(b <span class="sc">-</span> bhat) <span class="sc">/</span> <span class="fu">var</span>(b)</span></code></pre></div>
<pre><code>##         bodyfat
## bodyfat 0.25095</code></pre>
<p>Notice that the amount of variance explained, 74.905%, plus the amount remaining unexplained, 25.095%, add up to 100%. This is no accident. It is the reason why statisticians use the variance as a measure of variability.</p>
<div class="rmosaic">
<p>Let’s re-do the calculations, but using the software interface designed for statistical modeling rather than the interface used for constructing matrices. In R, the core program is called <code>lm()</code>, which stands for “linear model.” The <code>lm()</code> interface combines the construction phase and the model fitting phase and is connected with other software for doing an analysis of the model.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="statistical-modeling.html#cb120-1" aria-hidden="true" tabindex="-1"></a>mod_big <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> ., <span class="at">data =</span> Body_fat)</span>
<span id="cb120-2"><a href="statistical-modeling.html#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod_big)</span></code></pre></div>
<pre><code>##  (Intercept)          age       weight       height         neck        chest 
## -18.18848508   0.06207865  -0.08844468  -0.06959043  -0.47060001  -0.02386415 
##      abdomen          hip        thigh         knee        ankle       biceps 
##   0.95477346  -0.20754112   0.23609984   0.01528121   0.17399537   0.18160242 
##      forearm        wrist 
##   0.45202491  -1.62063910</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="statistical-modeling.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rsquared</span>(mod_big)</span></code></pre></div>
<pre><code>## [1] 0.74905</code></pre>
<p>The first argument to <code>lm()</code> is a tilde expression specifying the response variable (on the left of <code>~</code>) and the explanatory variables. In the shorthand used in the tilde expression, <code>.</code> means “all the columns except the response variable.” The vector of ones, identified as the “intercept,” is automatically included by default. These features of the interface are convenient and likely lower the chance of error.</p>
<p>The results from <code>lm()</code> are, as you can see, identical to those produced by the matrix manipulations in the previous section.</p>
</div>
</div>
<div id="interpreting-the-model" class="section level2" number="5.2">
<h2><span class="header-section-number">45.2</span> Interpreting the model</h2>
<p>One of the ways statistics differs from mathematics is that statistics is concerned with the interpretation of the model, the ways the model is or is not fit for purpose, and the ways the model can be improved.</p>
<p>Since the purpose of the body-fat model is to estimate the actual body fat percentage from easy-to-measure variables, let’s examine how much knowing the output of the model tells us about the actual body fat. We’ve already seen that R<sup>2</sup> is 75%, but there are other ways to look at things. Figure <a href="statistical-modeling.html#fig:body-fat-pred">45.1</a> shows the actual body fat in each of the 252 men whose data are in <code>Body_fat</code> as a function of the model output.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:body-fat-pred"></span>
<img src="MOSAIC-Calculus_files/figure-html/body-fat-pred-1.png" alt="Comparing the output of the model to the actual body-fat measurements made in the 252 men represented in the `Body_fat` data frame." width="90%" />
<p class="caption">
Figure 45.1: Comparing the output of the model to the actual body-fat measurements made in the 252 men represented in the <code>Body_fat</code> data frame.
</p>
</div>
<p>Using the graph, consider what to make of a model output of 20. Looking at the dozen or so men whose measurements led to an output near 20, their actual body fat spans a range of values, from about 10% to 30%. The prediction error for any given man is the difference between the actual value of body fat and the model output. The gray band on the graph contains about 95% of all the men. The vertical width of the band—about -8% to +8%—gives a prediction error that encompasses 95% of the men. This is interpreted to mean that the model output reflects the actual body mass, plus-or-minus 8%. (There’s no guarantee that the prediction error will be <span class="math inline">\(\leq 8\%\)</span>, but in the large majority of cases (roughly 95%) the model output will have a prediction error that’s no larger than <span class="math inline">\(\pm 8\%\)</span>.)</p>
<p>Another difference between statistics and mathematics is that good statistical work always requires some understanding of the system being studied, not just the manipulation of columns of numbers. Look back at the coefficients. Some make sense and some are questionable. For instance, the positive coefficient on <code>abdomen</code> makes intuitive sense since everyday experience is that body fat often appears there. But the negative coefficient on <code>neck</code>, what’s that about?</p>
</div>
<div id="improving-the-model" class="section level2" number="5.3">
<h2><span class="header-section-number">45.3</span> Improving the model</h2>
<p>The model that we built of body fat, which we called <code>big_mod</code>, might just as well have been called “kitchen sink.” It includes all possible explanatory variables. But it doesn’t have terms higher than first order. That is, <code>neck</code> enters into the model just as a linear function. But in Chapter 25 we introduced low-order polynomial terms, such as the quadratic <code>neck^2</code> or the interaction <code>neck*abdomen</code>. Maybe we would do better by introducing such terms. Since there are 13 explanatory variables, this would add 13 quadratic terms and many interaction terms, one for each pair of variables. That’s 78 vectors for the interaction plus 13 linear terms plus another 13 quadratic terms.</p>
<p>Let’s try adding these interaction terms a few at a time to see what happens. The modeling syntax for interaction terms uses the multiplication symbol <code>*</code>. We’ll compare the new models to <code>big_mod</code> which had an R<sup>2</sup> of 74.9%.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="statistical-modeling.html#cb124-1" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> . <span class="sc">*</span> neck, <span class="at">data =</span> Body_fat)</span>
<span id="cb124-2"><a href="statistical-modeling.html#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rsquared</span>(mod2)</span></code></pre></div>
<pre><code>## [1] 0.7730899</code></pre>
<p>Including the interactions with <code>neck</code> has increased R<sup>2</sup> to 77.3%. Let’s keep going:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="statistical-modeling.html#cb126-1" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> . <span class="sc">*</span> neck <span class="sc">+</span> . <span class="sc">*</span> weight <span class="sc">+</span> . <span class="sc">*</span> abdomen, <span class="at">data=</span>Body_fat)</span>
<span id="cb126-2"><a href="statistical-modeling.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rsquared</span>(mod3)</span></code></pre></div>
<pre><code>## [1] 0.7932603</code></pre>
<p>Again, R<sup>2</sup> has gone up! Unfortunately, adding more terms is not a sure-fire way to improve a model. In model 3, we added 12 vectors from <code>*.neck</code>, 11 from <code>.*weight</code>, 10 from <code>.*abdomen</code>. (The reason for the decreasing counts is that <code>.*neck</code> already includes <code>weight*neck</code>, so the <code>neck*weight</code> that’s included in <code>. * weight</code> is redundant.) Adding altogether 33 new vectors on top of the 13 in the original model, has increased the R<sup>2</sup> from 74.9% to 79.3%. That’s a gain of 4.4 percentage points from 33 new vectors.</p>
<p>Is that gain worth it? To answer that question, we need to consider what is the cost induced by adding the 33 new vectors. The computational cost is not the issue, since even much bigger models are easily constructed on ordinary laptops. The issue has to do with the goal of predicting <code>bodyfat</code>. We don’t need to predict body fat for the men in the data; we already know their body fat. Instead, the point is to make predictions for men <em>not in the data</em>. These two kinds of prediction are called <strong><em>in-sample prediction</em></strong> (men in the data set) and <strong><em>out-of-sample prediction</em></strong> (men not in the data set).</p>
<p>The problem is that in-sample predictions tend to have a larger R<sup>2</sup> than out-of-sample predictions. And it’s the out-of-sample prediction that will matter in applications: predictions for people whose body fat is unknown and therefore will be estimated from the model.</p>
<p>A major goal in statistics and machine learning is to estimate the R<sup>2</sup> for out-of-sample prediction from just the data in the sample itself. Here’s one way to get at the out-of-sample prediction error: divide the data at random into two halves, fitting the model on the first half and estimating the out-of-sample prediction error using the other half, the half that is out-of-sample for fitting the model. This strategy, and many refinements to it, are called <strong><em>cross-validation</em></strong> and is a major technique in machine learning.</p>
<p>An older statistical technique gets at the problem of in- and out-of-sample by using models constructed from <strong><em>random stand-ins</em></strong> for explanatory vectors. For instance, we can examine whether the 33 interaction terms we added to the model are contributing to prediction by replacing them with 33 random vectors and examining the gain in R<sup>2</sup> from that. Here’s one way to do this, using <code>rand(33)</code> to generate the random variables:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="statistical-modeling.html#cb128-1" aria-hidden="true" tabindex="-1"></a>mod3R <span class="ot">&lt;-</span> <span class="fu">lm</span>(bodyfat <span class="sc">~</span> . <span class="sc">+</span> <span class="fu">rand</span>(<span class="dv">33</span>), <span class="at">data=</span>Body_fat)</span>
<span id="cb128-2"><a href="statistical-modeling.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rsquared</span>(mod3)</span></code></pre></div>
<pre><code>## [1] 0.7932603</code></pre>
<p>The random variables did every bit as well as the interaction terms! So there’s no particular reason to think that the interaction terms are contributing to the prediction.</p>
<div class="intheworld">
<p>The example we just gave with <code>. + rand(33)</code> isn’t completely adequate to the statistician’s needs. Those 33 random vectors were just a particular random choice, a role of the vector dice. We need to establish whether the result we got, R<sup>2</sup>=79.3% was just good luck. The next code block shows one way to do this: we generate many trials of <code>rand(33)</code>, calculating R<sup>2</sup> for each trial. Then compare the “genuine vectors” result to the distribution of results from the random trials.</p>
<p>You do not need to learn how to construct such code, but perhaps you will be able to gain some insight from it. We’re doing 100 random trials.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="statistical-modeling.html#cb130-1" aria-hidden="true" tabindex="-1"></a>trials <span class="ot">&lt;-</span> <span class="fu">do</span>(<span class="dv">100</span>) <span class="sc">*</span> {<span class="fu">lm</span>(bodyfat <span class="sc">~</span> . <span class="sc">+</span> <span class="fu">rand</span>(<span class="dv">33</span>), <span class="at">data=</span>Body_fat) <span class="sc">%&gt;%</span> <span class="fu">rsquared</span>()}</span>
<span id="cb130-2"><a href="statistical-modeling.html#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">rsquared</span>(mod3) <span class="sc">&gt;</span> trials)</span></code></pre></div>
<pre><code>## 
## FALSE  TRUE 
##    15    85</code></pre>
<p>The “genuine vectors” gave a result bigger than 90 out of 100 trials. This sort of “most of the time” doesn’t cut muster. A convention is to insist that the genuine vectors win at least 95% of the time. The 90-out-of-100 result would typically be reported as a <strong><em>p-value</em></strong> less than 10%.</p>
<p>Another sort of standard statistical report carries out this same sort of comparison to random vectors, but building up the model one term at a time. This is called <strong><em>analysis of variance</em></strong>. A standard report looks like this:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="statistical-modeling.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod_big)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: bodyfat
##            Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
## age         1 1493.3  1493.3  80.5644 &lt; 2.2e-16 ***
## weight      1 6674.3  6674.3 360.0837 &lt; 2.2e-16 ***
## height      1 1043.0  1043.0  56.2712 1.250e-12 ***
## neck        1  152.4   152.4   8.2227  0.004508 ** 
## chest       1  641.1   641.1  34.5856 1.373e-08 ***
## abdomen     1 2757.4  2757.4 148.7645 &lt; 2.2e-16 ***
## hip         1   22.5    22.5   1.2157  0.271327    
## thigh       1  110.6   110.6   5.9665  0.015309 *  
## knee        1    0.0     0.0   0.0013  0.971409    
## ankle       1    0.1     0.1   0.0043  0.948064    
## biceps      1   45.2    45.2   2.4369  0.119842    
## forearm     1   57.5    57.5   3.1013  0.079514 .  
## wrist       1  170.1   170.1   9.1781  0.002720 ** 
## Residuals 238 4411.4    18.5                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The first row of the report examines how well the model with just <code>age</code> does compared to random vectors. As you can see, the p-value (column <code>Pr(&gt;F)</code>) is much smaller than the standard cut-off of 0.05. The next row in the report gives the improvement in the model when <code>weight</code> is included along with <code>age</code> in the model. And so on down the line.</p>
<p>The report highlights some stinkers among the explanatory variables: <code>hip</code>, <code>knee</code>, <code>ankle</code>, and <code>biceps</code>. Maybe this matches your intuition, for instance that ankle circumference is not the best way to look at body fat.</p>
<p>This sort of analysis, which has many nuances not covered here, falls under the name <strong><em>statistical inference</em></strong>. Another, more recent term is <strong><em>statistical learning</em></strong>, often called <strong><em>machine learning</em></strong>. The general idea of statistical or machine learning is to search through many combinations of possible explanatory variables to construct a “best” model. This search often involves models that are not simply linear combinations. Some of these model architectures have catchy names: “deep learning,” “neural nets,” “support vector machines,” “lasso,” and so on. Very often, these machine learning models are built by <strong><em>gradient descent</em></strong>, and are elaborations of the basic <span class="math inline">\(\mathit{A}\vec{x} = \vec{b}\)</span> model architecture. But they often have capabilities beyond <span class="math inline">\(\mathit{A}\vec{x} = \vec{b}\)</span>. For instance, deep learning and lasso are designed to handle situations where the number of explanatory variables—the number of columns in the data frame—is far larger than the number of rows. An example: learning to identify whether there is an animal in a photograph with 20,000 pixels. And returning to genetics: measuring 100,000 genetic expression products on a sample of 10 people with a disease and 10 healthy controls to determine which, if any, genes are related to the disease.</p>
</div>
</div>
<div id="exercises-4" class="section level2" number="5.4">
<h2><span class="header-section-number">45.4</span> Exercises</h2>
<details>
<summary>
<strong>Exercise XX.XX</strong>: <span><a name="File: Exercises/girl-have-rug.Rmd" href="#97zwR6"><img src="www/icons8-signpost.png" title="Location: Exercises/girl-have-rug.Rmd" width="12px"/></a><span style="color: red; font-size: 9pt;">97zwR6</red></span>
</summary>
<p>A. What’s the largest possible value for R<sup>2</sup>?</p>
<p>As you know, R<sup>2</sup> is a ratio of variances. A good way to think of variance is as the square length of a vector. So think about R<sup>2</sup> as if it were calculated from the square length of <span class="math inline">\(\vec{b}\)</span> and <span class="math inline">\(\hat{b}\)</span>.</p>
<p>(Hint: What does the projection problem tell you about the lengths of <span class="math inline">\(\vec{b}\)</span> and <span class="math inline">\(\hat{b}\)</span>.)</p>
B. What’s the smallest possible value for R<sup>2</sup>? (Hint: What’s the smallest possible length for <span class="math inline">\(\hat{b}\)</span>?)
</details>

</div>
</div>






<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Amazingly, this work attracted little attention until after 1900, when Mendel’s laws were rediscovered by the botanists de Vries, Correns, and von Tschermak.<a href="statistical-modeling.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="target-problem.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "B5-stat-modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "B5-stat-modeling.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"edit": {
"link": "https://github.com/ProjectMOSAIC/MOSAIC-Calculus/blob/main/Block-5/%s",
"text": "Edit me!"
},
"sharing": false
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
